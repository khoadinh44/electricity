{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "real_running.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV8nVSdymR3j"
      },
      "source": [
        "## Clone github "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyp_5Fi9lSH4",
        "outputId": "50f0973a-8089-4ade-9128-d07795bdbe1b"
      },
      "source": [
        "!git clone https://github.com/khoadinh44/electricity.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'electricity'...\n",
            "remote: Enumerating objects: 210, done.\u001b[K\n",
            "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
            "remote: Compressing objects: 100% (188/188), done.\u001b[K\n",
            "remote: Total 210 (delta 93), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (210/210), 154.26 KiB | 4.28 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P2Ti356mZTX"
      },
      "source": [
        "# Go into the main brand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "didCze6MmXv3",
        "outputId": "6c676935-58e2-40fd-ed77-34a24d971d65"
      },
      "source": [
        "%cd /content/electricity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/electricity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oIxjdmpm-UJ"
      },
      "source": [
        "# Install library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqMKJgllm3Fh",
        "outputId": "0a1ea554-d4cf-44df-d41e-3f05b16f792d"
      },
      "source": [
        "%pip install -qr requirements.txt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKWN0KYLmkpc"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrMUXNvCmjH1",
        "outputId": "8f88f199-5486-43af-caa7-89d057b10e98"
      },
      "source": [
        "!python real_main.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Adam/gradients/p_re_lu/Relu_1_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781298: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/Relu_1_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/Neg_1_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781324: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/Neg_1_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/AddN_1: (AddN): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781347: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/AddN_1: (AddN): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781374: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781404: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781437: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781462: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_1: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781485: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_1: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_2: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781507: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_2: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_3: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781544: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_3: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_4: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781566: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_4: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_5: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781588: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_5: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781609: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/stack: (Pack): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781636: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/stack: (Pack): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781666: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781708: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/global_norm: (Sqrt): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781747: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/global_norm: (Sqrt): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781777: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/truediv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781825: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/truediv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781867: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781922: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781952: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.781996: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782019: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_0: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782043: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_0: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_2: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782066: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_2: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782089: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_3: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782111: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_3: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782134: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_4: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782156: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_4: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_3: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782179: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_3: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_5: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782202: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_5: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_4: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782220: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_4: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_6: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782236: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_6: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_5: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782254: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_5: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_7: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782272: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_7: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_6: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782288: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_6: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta1_power: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782307: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta1_power: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta1_power/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782324: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta1_power/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta1_power/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782352: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta1_power/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta2_power: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782375: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta2_power: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta2_power/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782397: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta2_power/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta2_power/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782417: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta2_power/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782445: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782467: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782486: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782505: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782526: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782768: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782811: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782833: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782856: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782879: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782914: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782938: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782956: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782974: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.782996: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783017: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783038: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783060: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783081: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783102: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783123: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783159: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783179: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783200: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783220: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783241: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783261: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783293: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783314: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783334: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783353: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783374: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783395: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783415: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783442: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783462: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783484: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783504: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783524: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783543: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783717: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783747: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783768: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783805: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783826: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783848: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783871: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783904: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_p_re_lu/alpha/ResourceApplyAdam: (ResourceApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783925: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_p_re_lu/alpha/ResourceApplyAdam: (ResourceApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected_1/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783948: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected_1/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected_1/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783970: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected_1/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected_2/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.783991: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected_2/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected_2/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784012: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected_2/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784033: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784053: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784091: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784127: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784171: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0: (AssignAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784187: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0: (AssignAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/Merge/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.784202: I tensorflow/core/common_runtime/placer.cc:114] Adam/Merge/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Adam/train_op_0: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784228: I tensorflow/core/common_runtime/placer.cc:114] Adam/train_op_0: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784253: I tensorflow/core/common_runtime/placer.cc:114] save/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784282: I tensorflow/core/common_runtime/placer.cc:114] save/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.784314: I tensorflow/core/common_runtime/placer.cc:114] save/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784343: I tensorflow/core/common_runtime/placer.cc:114] save/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.784373: I tensorflow/core/common_runtime/placer.cc:114] save/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784398: I tensorflow/core/common_runtime/placer.cc:114] save/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784430: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784469: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784486: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784503: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784522: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784540: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784557: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784572: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784582: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784592: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784601: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784619: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784640: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784659: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784676: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784694: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784707: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784717: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784736: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784759: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784782: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784827: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.784857: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785220: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785265: I tensorflow/core/common_runtime/placer.cc:114] save/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785301: I tensorflow/core/common_runtime/placer.cc:114] save/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785330: I tensorflow/core/common_runtime/placer.cc:114] save/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785351: I tensorflow/core/common_runtime/placer.cc:114] save/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785375: I tensorflow/core/common_runtime/placer.cc:114] save/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785388: I tensorflow/core/common_runtime/placer.cc:114] save/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785398: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785408: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785440: I tensorflow/core/common_runtime/placer.cc:114] save/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785471: I tensorflow/core/common_runtime/placer.cc:114] save_1/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785512: I tensorflow/core/common_runtime/placer.cc:114] save_1/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.785527: I tensorflow/core/common_runtime/placer.cc:114] save_1/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785538: I tensorflow/core/common_runtime/placer.cc:114] save_1/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.785558: I tensorflow/core/common_runtime/placer.cc:114] save_1/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785584: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785615: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785639: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785658: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785683: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785709: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785733: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785772: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785796: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785824: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785850: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785874: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785907: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785930: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785950: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785969: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.785986: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786006: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786028: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786048: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786066: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786084: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786098: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786109: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786130: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786165: I tensorflow/core/common_runtime/placer.cc:114] save_1/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786193: I tensorflow/core/common_runtime/placer.cc:114] save_1/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786233: I tensorflow/core/common_runtime/placer.cc:114] save_1/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786258: I tensorflow/core/common_runtime/placer.cc:114] save_1/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786289: I tensorflow/core/common_runtime/placer.cc:114] save_1/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786314: I tensorflow/core/common_runtime/placer.cc:114] save_1/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786332: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786350: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786376: I tensorflow/core/common_runtime/placer.cc:114] save_1/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786416: I tensorflow/core/common_runtime/placer.cc:114] save_2/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786455: I tensorflow/core/common_runtime/placer.cc:114] save_2/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.786488: I tensorflow/core/common_runtime/placer.cc:114] save_2/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786514: I tensorflow/core/common_runtime/placer.cc:114] save_2/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.786538: I tensorflow/core/common_runtime/placer.cc:114] save_2/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786560: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786581: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786602: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786620: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786641: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786661: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786688: I tensorflow/core/common_runtime/placer.cc:114] save_2/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786709: I tensorflow/core/common_runtime/placer.cc:114] save_2/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786736: I tensorflow/core/common_runtime/placer.cc:114] save_2/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786762: I tensorflow/core/common_runtime/placer.cc:114] init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "init_1: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786788: I tensorflow/core/common_runtime/placer.cc:114] init_1: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786812: I tensorflow/core/common_runtime/placer.cc:114] group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "init_2: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786838: I tensorflow/core/common_runtime/placer.cc:114] init_2: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "is_training/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786862: I tensorflow/core/common_runtime/placer.cc:114] is_training/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Assign/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786889: I tensorflow/core/common_runtime/placer.cc:114] Assign/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Assign_1/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786934: I tensorflow/core/common_runtime/placer.cc:114] Assign_1/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "InputData/X: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786963: I tensorflow/core/common_runtime/placer.cc:114] InputData/X: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.786986: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787095: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787121: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787143: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787164: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787185: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787207: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787253: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787274: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787297: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787318: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787339: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787360: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787381: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "TargetsData/Y: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787409: I tensorflow/core/common_runtime/placer.cc:114] TargetsData/Y: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/ArgMax/dimension: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787443: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/ArgMax/dimension: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/ArgMax_1/dimension: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787599: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/ArgMax_1/dimension: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787629: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Sum/reduction_indices: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787658: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Sum/reduction_indices: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Cast/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787685: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Cast/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Cast_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787712: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Cast_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Sum_1/reduction_indices: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787740: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Sum_1/reduction_indices: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787767: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Training_step/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787795: I tensorflow/core/common_runtime/placer.cc:114] Training_step/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Global_Step/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787821: I tensorflow/core/common_runtime/placer.cc:114] Global_Step/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787848: I tensorflow/core/common_runtime/placer.cc:114] Add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_loss/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787905: I tensorflow/core/common_runtime/placer.cc:114] val_loss/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_acc/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787937: I tensorflow/core/common_runtime/placer.cc:114] val_acc/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "placeholder/val_loss: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787967: I tensorflow/core/common_runtime/placer.cc:114] placeholder/val_loss: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "placeholder/val_acc: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.787995: I tensorflow/core/common_runtime/placer.cc:114] placeholder/val_acc: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Mean/moving_avg/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788018: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Mean/moving_avg/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/decay: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788061: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/decay: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/add/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788089: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/add/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/add_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788113: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/add_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/AssignMovingAvg/sub/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788140: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/AssignMovingAvg/sub/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Mean/moving_avg/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788163: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Mean/moving_avg/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/decay: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788214: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/decay: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/add/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788249: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/add/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/add_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788276: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/add_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/AssignMovingAvg/sub/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788304: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/AssignMovingAvg/sub/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Loss/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.788328: I tensorflow/core/common_runtime/placer.cc:114] Loss/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Adam/Loss/raw/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.788352: I tensorflow/core/common_runtime/placer.cc:114] Adam/Loss/raw/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Adam/gradients/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788379: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/grad_ys_0/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788407: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/grad_ys_0/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788439: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Shape_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788468: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Shape_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788495: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788522: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Maximum/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788565: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Maximum/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/Size: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788587: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/Size: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788607: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788629: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788664: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/ones/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788685: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/ones/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788712: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788740: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788768: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788795: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/truediv/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788822: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/truediv/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788850: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/truediv_1/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788877: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/truediv_1/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788915: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta1_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788939: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta1_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta2_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788962: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta2_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.788984: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789005: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789026: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789047: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789068: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789090: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789111: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789132: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789153: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789173: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789194: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789215: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789236: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789256: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789277: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789298: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789499: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789524: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789545: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789566: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/learning_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789594: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/learning_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/beta1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789622: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/beta1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/beta2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789649: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/beta2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/epsilon: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789676: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/epsilon: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.789698: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.789725: I tensorflow/core/common_runtime/placer.cc:114] save/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.789750: I tensorflow/core/common_runtime/placer.cc:114] save/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.789789: I tensorflow/core/common_runtime/placer.cc:114] save/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.789817: I tensorflow/core/common_runtime/placer.cc:114] save/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.789841: I tensorflow/core/common_runtime/placer.cc:114] save/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790483: I tensorflow/core/common_runtime/placer.cc:114] save_1/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790514: I tensorflow/core/common_runtime/placer.cc:114] save_1/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790539: I tensorflow/core/common_runtime/placer.cc:114] save_1/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790565: I tensorflow/core/common_runtime/placer.cc:114] save_1/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790589: I tensorflow/core/common_runtime/placer.cc:114] save_1/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790614: I tensorflow/core/common_runtime/placer.cc:114] save_2/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790637: I tensorflow/core/common_runtime/placer.cc:114] save_2/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790660: I tensorflow/core/common_runtime/placer.cc:114] save_2/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790684: I tensorflow/core/common_runtime/placer.cc:114] save_2/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.790708: I tensorflow/core/common_runtime/placer.cc:114] save_2/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.834649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:14:01.835843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:14:01.836856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:14:01.837853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:14:01.838812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:14:01.839584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-10-13 13:14:01.839659: I tensorflow/core/common_runtime/direct_session.cc:361] Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "\n",
            "---------------------------------\n",
            "Run id: 1BTUMK\n",
            "Log directory: /content/electricity/save/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 497\n",
            "Validation samples: 10\n",
            "--\n",
            "is_training: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938117: I tensorflow/core/common_runtime/placer.cc:114] is_training: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "is_training/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938187: I tensorflow/core/common_runtime/placer.cc:114] is_training/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "is_training/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938217: I tensorflow/core/common_runtime/placer.cc:114] is_training/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938242: I tensorflow/core/common_runtime/placer.cc:114] Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938268: I tensorflow/core/common_runtime/placer.cc:114] Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938296: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938324: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938347: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938367: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938386: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938408: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938431: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938469: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938489: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938531: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938572: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938598: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938620: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938641: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938661: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938686: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938709: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938734: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/Neg_1: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938768: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/Neg_1: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/Relu_1: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938810: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/Relu_1: (Relu): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938837: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938878: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938927: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938968: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.938990: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939012: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939032: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939057: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939112: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939149: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939176: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939196: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939212: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939247: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939257: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939271: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939283: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939294: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939308: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939323: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939337: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939351: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939367: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939384: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/BiasAdd: (BiasAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/ArgMax: (ArgMax): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939401: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/ArgMax: (ArgMax): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/ArgMax_1: (ArgMax): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939416: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/ArgMax_1: (ArgMax): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Equal: (Equal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939433: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Equal: (Equal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939449: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Mean: (Mean): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939466: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Mean: (Mean): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939482: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939499: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/clip_by_value/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939516: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/clip_by_value/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/clip_by_value: (Maximum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939532: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/clip_by_value: (Maximum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Log: (Log): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939549: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Log: (Log): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939566: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939583: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939599: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Mean: (Mean): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939615: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Mean: (Mean): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Training_step: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939630: I tensorflow/core/common_runtime/placer.cc:114] Training_step: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Training_step/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939644: I tensorflow/core/common_runtime/placer.cc:114] Training_step/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Training_step/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939659: I tensorflow/core/common_runtime/placer.cc:114] Training_step/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Global_Step: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939673: I tensorflow/core/common_runtime/placer.cc:114] Global_Step: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Global_Step/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939687: I tensorflow/core/common_runtime/placer.cc:114] Global_Step/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Global_Step/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939701: I tensorflow/core/common_runtime/placer.cc:114] Global_Step/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939717: I tensorflow/core/common_runtime/placer.cc:114] Add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939731: I tensorflow/core/common_runtime/placer.cc:114] Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_loss: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939751: I tensorflow/core/common_runtime/placer.cc:114] val_loss: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_loss/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939766: I tensorflow/core/common_runtime/placer.cc:114] val_loss/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_loss/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939779: I tensorflow/core/common_runtime/placer.cc:114] val_loss/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_acc: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.939809: I tensorflow/core/common_runtime/placer.cc:114] val_acc: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_acc/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.993828: I tensorflow/core/common_runtime/placer.cc:114] val_acc/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_acc/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.993930: I tensorflow/core/common_runtime/placer.cc:114] val_acc/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "assign/val_loss: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.993991: I tensorflow/core/common_runtime/placer.cc:114] assign/val_loss: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "assign/val_acc: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994011: I tensorflow/core/common_runtime/placer.cc:114] assign/val_acc: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Mean/moving_avg: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994056: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Mean/moving_avg: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Mean/moving_avg/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994086: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Mean/moving_avg/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Mean/moving_avg/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994131: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Mean/moving_avg/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994182: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/add_1: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994262: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/add_1: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994318: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994358: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/AssignMovingAvg/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994383: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/AssignMovingAvg/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/AssignMovingAvg/sub_1: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994406: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/AssignMovingAvg/sub_1: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/AssignMovingAvg/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994426: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/AssignMovingAvg/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/AssignMovingAvg: (AssignSub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994458: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/AssignMovingAvg: (AssignSub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994482: I tensorflow/core/common_runtime/placer.cc:114] moving_avg: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/Total_Loss: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994518: I tensorflow/core/common_runtime/placer.cc:114] Adam/Total_Loss: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Mean/moving_avg: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994553: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Mean/moving_avg: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Mean/moving_avg/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994571: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Mean/moving_avg/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Mean/moving_avg/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994587: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Mean/moving_avg/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994609: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/add_1: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994631: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/add_1: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994668: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994691: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/AssignMovingAvg/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994714: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/AssignMovingAvg/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/AssignMovingAvg/sub_1: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994737: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/AssignMovingAvg/sub_1: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/AssignMovingAvg/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994764: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/AssignMovingAvg/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/AssignMovingAvg: (AssignSub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994781: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/AssignMovingAvg: (AssignSub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994804: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Loss: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.994835: I tensorflow/core/common_runtime/placer.cc:114] Loss: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Adam/Loss/raw: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:01.994886: I tensorflow/core/common_runtime/placer.cc:114] Adam/Loss/raw: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Adam/gradients/grad_ys_0: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994933: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/grad_ys_0: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.994980: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995013: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Tile: (Tile): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995042: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Tile: (Tile): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995221: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Prod: (Prod): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995255: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Prod: (Prod): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Prod_1: (Prod): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995279: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Prod_1: (Prod): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Maximum: (Maximum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995302: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Maximum: (Maximum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/floordiv: (FloorDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995324: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/floordiv: (FloorDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995346: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Cast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995367: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Neg_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995390: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Neg_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995411: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995431: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/mod: (FloorMod): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995450: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/mod: (FloorMod): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995471: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/range: (Range): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/ones: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995495: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/ones: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/DynamicStitch: (DynamicStitch): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995596: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/DynamicStitch: (DynamicStitch): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995627: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/BroadcastTo: (BroadcastTo): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995651: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/BroadcastTo: (BroadcastTo): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995674: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995696: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995717: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995738: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995761: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995797: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/Mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995818: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/Mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995843: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995865: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Log_grad/Reciprocal: (Reciprocal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995889: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Log_grad/Reciprocal: (Reciprocal): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Log_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995934: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Log_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995961: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.995985: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/GreaterEqual: (GreaterEqual): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996006: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/GreaterEqual: (GreaterEqual): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996029: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/SelectV2: (SelectV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996053: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/SelectV2: (SelectV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996076: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996117: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/SelectV2_1: (SelectV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996141: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/SelectV2_1: (SelectV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996164: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996185: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996223: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996253: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/LessEqual: (LessEqual): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996278: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/LessEqual: (LessEqual): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996301: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/SelectV2: (SelectV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996322: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/SelectV2: (SelectV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996343: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996363: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/SelectV2_1: (SelectV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996383: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/SelectV2_1: (SelectV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996404: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996424: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996448: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996485: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996522: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/RealDiv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996558: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/RealDiv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996594: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996617: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996847: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/RealDiv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996888: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/RealDiv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/RealDiv_2: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996944: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/RealDiv_2: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.996982: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997025: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/truediv_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997049: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/truediv_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997086: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_grad/BroadcastTo: (BroadcastTo): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997129: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_grad/BroadcastTo: (BroadcastTo): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/AddN: (AddN): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997150: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/AddN: (AddN): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected_2/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997174: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected_2/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected_2/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997199: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected_2/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected_2/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997265: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected_2/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected_1/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997288: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected_1/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected_1/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997327: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected_1/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected_1/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997388: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected_1/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/add_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997730: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/add_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/add_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997775: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/add_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997801: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/add_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997822: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/add_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/add_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997843: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/add_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/add_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997861: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/add_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/add_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997881: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/add_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/Relu_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997916: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/Relu_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997954: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997973: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.997997: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998047: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/Mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998067: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998087: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/Mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998191: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/Mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998220: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998249: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/Neg_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998274: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/Neg_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/Relu_1_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998296: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/Relu_1_grad/ReluGrad: (ReluGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/p_re_lu/Neg_1_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998316: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/p_re_lu/Neg_1_grad/Neg: (Neg): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/AddN_1: (AddN): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998350: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/AddN_1: (AddN): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998374: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998395: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/FullyConnected/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998416: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/FullyConnected/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998454: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_1: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998486: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_1: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_2: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998506: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_2: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_3: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998525: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_3: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_4: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998547: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_4: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_5: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998565: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_5: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998584: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/stack: (Pack): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998606: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/stack: (Pack): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998631: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/Sum: (Sum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998652: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/global_norm: (Sqrt): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998675: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/global_norm: (Sqrt): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998699: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/truediv: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/truediv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998722: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/truediv_1: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998742: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Minimum: (Minimum): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998769: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998790: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998814: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998833: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_0: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998850: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_0: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_2: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998881: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_2: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998911: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_3: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998949: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_3: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998967: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_4: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.998985: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_4: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_3: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999001: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_3: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_5: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999051: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_5: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_4: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999067: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_4: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_6: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999090: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_6: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_5: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999108: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_5: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul_7: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999125: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul_7: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Adam/clip_by_global_norm/_6: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999139: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Adam/clip_by_global_norm/_6: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta1_power: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999156: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta1_power: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta1_power/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999173: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta1_power/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta1_power/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999189: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta1_power/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta2_power: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999208: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta2_power: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta2_power/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999224: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta2_power/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta2_power/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999249: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta2_power/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999269: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999286: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999302: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999318: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999370: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999414: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999434: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999451: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999465: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999481: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999500: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999518: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999533: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999547: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999566: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999581: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999597: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999612: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999628: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999646: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999662: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999795: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999824: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999840: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999861: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999878: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999908: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/Initializer/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999927: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999942: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999957: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999977: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:01.999994: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000011: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000030: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000049: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000067: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000087: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000105: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000124: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000143: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000160: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000177: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000195: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000211: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000227: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000266: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam_1: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000291: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000309: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam_1/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000323: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000339: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_p_re_lu/alpha/ResourceApplyAdam: (ResourceApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000356: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_p_re_lu/alpha/ResourceApplyAdam: (ResourceApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected_1/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000375: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected_1/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected_1/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000391: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected_1/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected_2/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000408: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected_2/W/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update_FullyConnected_2/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000424: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update_FullyConnected_2/b/ApplyAdam: (ApplyAdam): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000440: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000459: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000475: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000491: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/update: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000515: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/update: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0: (AssignAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000536: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0: (AssignAdd): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/Merge/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.000596: I tensorflow/core/common_runtime/placer.cc:114] Adam/Merge/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Adam/train_op_0: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000624: I tensorflow/core/common_runtime/placer.cc:114] Adam/train_op_0: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000645: I tensorflow/core/common_runtime/placer.cc:114] save/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000666: I tensorflow/core/common_runtime/placer.cc:114] save/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.000687: I tensorflow/core/common_runtime/placer.cc:114] save/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000707: I tensorflow/core/common_runtime/placer.cc:114] save/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.000730: I tensorflow/core/common_runtime/placer.cc:114] save/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000749: I tensorflow/core/common_runtime/placer.cc:114] save/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.000767: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001102: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001146: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001169: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001190: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001212: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001234: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001257: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001278: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001298: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001317: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001339: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001358: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001379: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001393: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001413: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001436: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001460: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001485: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001511: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001538: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001562: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001585: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001612: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001659: I tensorflow/core/common_runtime/placer.cc:114] save/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001685: I tensorflow/core/common_runtime/placer.cc:114] save/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001709: I tensorflow/core/common_runtime/placer.cc:114] save/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001733: I tensorflow/core/common_runtime/placer.cc:114] save/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001761: I tensorflow/core/common_runtime/placer.cc:114] save/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001784: I tensorflow/core/common_runtime/placer.cc:114] save/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001807: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001845: I tensorflow/core/common_runtime/placer.cc:114] save/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001952: I tensorflow/core/common_runtime/placer.cc:114] save/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.001991: I tensorflow/core/common_runtime/placer.cc:114] save_1/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002026: I tensorflow/core/common_runtime/placer.cc:114] save_1/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.002059: I tensorflow/core/common_runtime/placer.cc:114] save_1/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002084: I tensorflow/core/common_runtime/placer.cc:114] save_1/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.002145: I tensorflow/core/common_runtime/placer.cc:114] save_1/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002173: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002216: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002274: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002305: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002329: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002370: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002395: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002420: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002443: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002465: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002481: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002497: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002512: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002528: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002540: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002553: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002569: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002584: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002616: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002631: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002645: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002657: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002681: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002707: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002722: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002755: I tensorflow/core/common_runtime/placer.cc:114] save_1/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002776: I tensorflow/core/common_runtime/placer.cc:114] save_1/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002805: I tensorflow/core/common_runtime/placer.cc:114] save_1/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002828: I tensorflow/core/common_runtime/placer.cc:114] save_1/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002865: I tensorflow/core/common_runtime/placer.cc:114] save_1/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002928: I tensorflow/core/common_runtime/placer.cc:114] save_1/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002959: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.002986: I tensorflow/core/common_runtime/placer.cc:114] save_1/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_1/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003020: I tensorflow/core/common_runtime/placer.cc:114] save_1/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003055: I tensorflow/core/common_runtime/placer.cc:114] save_2/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003090: I tensorflow/core/common_runtime/placer.cc:114] save_2/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.003117: I tensorflow/core/common_runtime/placer.cc:114] save_2/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003137: I tensorflow/core/common_runtime/placer.cc:114] save_2/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.003151: I tensorflow/core/common_runtime/placer.cc:114] save_2/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003177: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003244: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003269: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003308: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003325: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003350: I tensorflow/core/common_runtime/placer.cc:114] save_2/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003382: I tensorflow/core/common_runtime/placer.cc:114] save_2/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003414: I tensorflow/core/common_runtime/placer.cc:114] save_2/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_2/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003460: I tensorflow/core/common_runtime/placer.cc:114] save_2/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003505: I tensorflow/core/common_runtime/placer.cc:114] init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "init_1: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003554: I tensorflow/core/common_runtime/placer.cc:114] init_1: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003587: I tensorflow/core/common_runtime/placer.cc:114] group_deps: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "init_2: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003621: I tensorflow/core/common_runtime/placer.cc:114] init_2: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003651: I tensorflow/core/common_runtime/placer.cc:114] save_3/filename: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003679: I tensorflow/core/common_runtime/placer.cc:114] save_3/Const: (PlaceholderWithDefault): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.003703: I tensorflow/core/common_runtime/placer.cc:114] save_3/SaveV2: (SaveV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_3/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003726: I tensorflow/core/common_runtime/placer.cc:114] save_3/control_dependency: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.003757: I tensorflow/core/common_runtime/placer.cc:114] save_3/RestoreV2: (RestoreV2): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_3/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003813: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003870: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_1: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003908: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_2: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003945: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_3: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.003971: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_4: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004004: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_5: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004035: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_6: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004065: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_7: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004094: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_8: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004120: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_9: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004145: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_10: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004166: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_11: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004179: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_12: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004203: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_13: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004242: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_14: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004259: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_15: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004278: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_16: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004295: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_17: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004311: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_18: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004330: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_19: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004346: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_20: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004365: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_21: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004385: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_22: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004422: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_23: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004448: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_24: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004481: I tensorflow/core/common_runtime/placer.cc:114] save_3/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004511: I tensorflow/core/common_runtime/placer.cc:114] save_3/AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004560: I tensorflow/core/common_runtime/placer.cc:114] save_3/Identity_1: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004584: I tensorflow/core/common_runtime/placer.cc:114] save_3/AssignVariableOp_1: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004616: I tensorflow/core/common_runtime/placer.cc:114] save_3/Identity_2: (Identity): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004689: I tensorflow/core/common_runtime/placer.cc:114] save_3/AssignVariableOp_2: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004734: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_25: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004754: I tensorflow/core/common_runtime/placer.cc:114] save_3/Assign_26: (Assign): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save_3/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.004774: I tensorflow/core/common_runtime/placer.cc:114] save_3/restore_all: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/__raw_: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.004792: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/__raw_: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Merge/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.004814: I tensorflow/core/common_runtime/placer.cc:114] Merge/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Accuracy: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.004845: I tensorflow/core/common_runtime/placer.cc:114] Accuracy: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Merge_1/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.004872: I tensorflow/core/common_runtime/placer.cc:114] Merge_1/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Loss/Validation: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.004935: I tensorflow/core/common_runtime/placer.cc:114] Loss/Validation: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Merge_2/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.004982: I tensorflow/core/common_runtime/placer.cc:114] Merge_2/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Accuracy/Validation: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.005060: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Validation: (ScalarSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Merge_3/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.005091: I tensorflow/core/common_runtime/placer.cc:114] Merge_3/MergeSummary: (MergeSummary): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "is_training/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005122: I tensorflow/core/common_runtime/placer.cc:114] is_training/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Assign/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005170: I tensorflow/core/common_runtime/placer.cc:114] Assign/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Assign_1/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005204: I tensorflow/core/common_runtime/placer.cc:114] Assign_1/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "InputData/X: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005234: I tensorflow/core/common_runtime/placer.cc:114] InputData/X: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005257: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005273: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005285: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005300: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005315: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005330: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005359: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005374: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005384: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005407: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005448: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005465: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005488: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "TargetsData/Y: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005520: I tensorflow/core/common_runtime/placer.cc:114] TargetsData/Y: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/ArgMax/dimension: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005553: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/ArgMax/dimension: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/ArgMax_1/dimension: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005585: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/ArgMax_1/dimension: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005617: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Sum/reduction_indices: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005648: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Sum/reduction_indices: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Cast/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005799: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Cast/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Cast_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005833: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Cast_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Sum_1/reduction_indices: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005864: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Sum_1/reduction_indices: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005909: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Training_step/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.005952: I tensorflow/core/common_runtime/placer.cc:114] Training_step/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Global_Step/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006001: I tensorflow/core/common_runtime/placer.cc:114] Global_Step/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006034: I tensorflow/core/common_runtime/placer.cc:114] Add/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_loss/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006066: I tensorflow/core/common_runtime/placer.cc:114] val_loss/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "val_acc/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006114: I tensorflow/core/common_runtime/placer.cc:114] val_acc/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "placeholder/val_loss: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006148: I tensorflow/core/common_runtime/placer.cc:114] placeholder/val_loss: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "placeholder/val_acc: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006183: I tensorflow/core/common_runtime/placer.cc:114] placeholder/val_acc: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Accuracy/Mean/moving_avg/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006207: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Mean/moving_avg/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/decay: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006234: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/decay: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/add/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006262: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/add/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/add_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006288: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/add_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "moving_avg/AssignMovingAvg/sub/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006328: I tensorflow/core/common_runtime/placer.cc:114] moving_avg/AssignMovingAvg/sub/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Crossentropy/Mean/moving_avg/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006352: I tensorflow/core/common_runtime/placer.cc:114] Crossentropy/Mean/moving_avg/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/decay: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006378: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/decay: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/add/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006416: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/add/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/add_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006456: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/add_1/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/moving_avg/AssignMovingAvg/sub/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006486: I tensorflow/core/common_runtime/placer.cc:114] Adam/moving_avg/AssignMovingAvg/sub/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Loss/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.006518: I tensorflow/core/common_runtime/placer.cc:114] Loss/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Adam/Loss/raw/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.006549: I tensorflow/core/common_runtime/placer.cc:114] Adam/Loss/raw/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Adam/gradients/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006584: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/grad_ys_0/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006611: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/grad_ys_0/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006637: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Reshape/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Shape_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006662: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Shape_2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006706: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006733: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Mean_grad/Maximum/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006790: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Mean_grad/Maximum/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/Size: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006836: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/Size: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006856: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006880: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/range/start: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006920: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/range/delta: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/Sum_1_grad/ones/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006961: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/Sum_1_grad/ones/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.006999: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007037: I tensorflow/core/common_runtime/placer.cc:114] Adam/gradients/Crossentropy/clip_by_value/Minimum_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007074: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/global_norm/Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007104: I tensorflow/core/common_runtime/placer.cc:114] Adam/global_norm/Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/truediv/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007148: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/truediv/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007189: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/truediv_1/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007215: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/truediv_1/y: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/clip_by_global_norm/mul/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007240: I tensorflow/core/common_runtime/placer.cc:114] Adam/clip_by_global_norm/mul/x: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta1_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007260: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta1_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/beta2_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007280: I tensorflow/core/common_runtime/placer.cc:114] Adam/beta2_power/initial_value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007299: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007318: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007337: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/W/Adam_1/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007357: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/W/Adam_1/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.007376: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008077: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008105: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "p_re_lu/alpha/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008134: I tensorflow/core/common_runtime/placer.cc:114] p_re_lu/alpha/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008154: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008176: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008204: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/Initializer/zeros/shape_as_tensor: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/W/Adam_1/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008227: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/W/Adam_1/Initializer/zeros/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008253: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_1/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008271: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_1/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008295: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/W/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008327: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/W/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008368: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "FullyConnected_2/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008409: I tensorflow/core/common_runtime/placer.cc:114] FullyConnected_2/b/Adam_1/Initializer/zeros: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/learning_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008531: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/learning_rate: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/beta1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008569: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/beta1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/beta2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008595: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/beta2: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/epsilon: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008640: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/epsilon: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Adam/apply_grad_op_0/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "2021-10-13 13:14:02.008682: I tensorflow/core/common_runtime/placer.cc:114] Adam/apply_grad_op_0/value: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
            "save/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008724: I tensorflow/core/common_runtime/placer.cc:114] save/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008769: I tensorflow/core/common_runtime/placer.cc:114] save/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008794: I tensorflow/core/common_runtime/placer.cc:114] save/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008831: I tensorflow/core/common_runtime/placer.cc:114] save/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008856: I tensorflow/core/common_runtime/placer.cc:114] save/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008881: I tensorflow/core/common_runtime/placer.cc:114] save_1/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008915: I tensorflow/core/common_runtime/placer.cc:114] save_1/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008936: I tensorflow/core/common_runtime/placer.cc:114] save_1/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008961: I tensorflow/core/common_runtime/placer.cc:114] save_1/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_1/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.008988: I tensorflow/core/common_runtime/placer.cc:114] save_1/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009014: I tensorflow/core/common_runtime/placer.cc:114] save_2/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009039: I tensorflow/core/common_runtime/placer.cc:114] save_2/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009064: I tensorflow/core/common_runtime/placer.cc:114] save_2/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009096: I tensorflow/core/common_runtime/placer.cc:114] save_2/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_2/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009143: I tensorflow/core/common_runtime/placer.cc:114] save_2/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_3/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009176: I tensorflow/core/common_runtime/placer.cc:114] save_3/filename/input: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_3/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009208: I tensorflow/core/common_runtime/placer.cc:114] save_3/SaveV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_3/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009241: I tensorflow/core/common_runtime/placer.cc:114] save_3/SaveV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_3/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009273: I tensorflow/core/common_runtime/placer.cc:114] save_3/RestoreV2/tensor_names: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "save_3/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009324: I tensorflow/core/common_runtime/placer.cc:114] save_3/RestoreV2/shape_and_slices: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Accuracy/__raw_/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009356: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/__raw_/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Accuracy/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009387: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Loss/Validation/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009427: I tensorflow/core/common_runtime/placer.cc:114] Loss/Validation/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Accuracy/Validation/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "2021-10-13 13:14:02.009460: I tensorflow/core/common_runtime/placer.cc:114] Accuracy/Validation/tags: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
            "Training Step: 1  | time: 0.471s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 016/497\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m82593.18750\u001b[0m\u001b[0m | time: 0.475s\n",
            "| Adam | epoch: 001 | loss: 82593.18750 - acc: 0.4500 -- iter: 032/497\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m78512.37500\u001b[0m\u001b[0m | time: 0.480s\n",
            "| Adam | epoch: 001 | loss: 78512.37500 - acc: 0.4909 -- iter: 048/497\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m127541.26562\u001b[0m\u001b[0m | time: 0.485s\n",
            "| Adam | epoch: 001 | loss: 127541.26562 - acc: 0.2634 -- iter: 064/497\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m66369.31250\u001b[0m\u001b[0m | time: 0.490s\n",
            "| Adam | epoch: 001 | loss: 66369.31250 - acc: 0.4272 -- iter: 080/497\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m61341.42188\u001b[0m\u001b[0m | time: 0.494s\n",
            "| Adam | epoch: 001 | loss: 61341.42188 - acc: 0.4740 -- iter: 096/497\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m59402.35156\u001b[0m\u001b[0m | time: 0.499s\n",
            "| Adam | epoch: 001 | loss: 59402.35156 - acc: 0.4521 -- iter: 112/497\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m60027.84375\u001b[0m\u001b[0m | time: 0.503s\n",
            "| Adam | epoch: 001 | loss: 60027.84375 - acc: 0.2681 -- iter: 128/497\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m61118.47656\u001b[0m\u001b[0m | time: 0.508s\n",
            "| Adam | epoch: 001 | loss: 61118.47656 - acc: 0.2254 -- iter: 144/497\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m63079.18750\u001b[0m\u001b[0m | time: 0.513s\n",
            "| Adam | epoch: 001 | loss: 63079.18750 - acc: 0.2065 -- iter: 160/497\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m59683.49609\u001b[0m\u001b[0m | time: 0.518s\n",
            "| Adam | epoch: 001 | loss: 59683.49609 - acc: 0.1679 -- iter: 176/497\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m65675.46875\u001b[0m\u001b[0m | time: 0.521s\n",
            "| Adam | epoch: 001 | loss: 65675.46875 - acc: 0.2330 -- iter: 192/497\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m100332.66406\u001b[0m\u001b[0m | time: 0.525s\n",
            "| Adam | epoch: 001 | loss: 100332.66406 - acc: 0.2403 -- iter: 208/497\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m143801.90625\u001b[0m\u001b[0m | time: 0.529s\n",
            "| Adam | epoch: 001 | loss: 143801.90625 - acc: 0.2187 -- iter: 224/497\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m150119.90625\u001b[0m\u001b[0m | time: 0.533s\n",
            "| Adam | epoch: 001 | loss: 150119.90625 - acc: 0.1820 -- iter: 240/497\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m157853.09375\u001b[0m\u001b[0m | time: 0.537s\n",
            "| Adam | epoch: 001 | loss: 157853.09375 - acc: 0.1841 -- iter: 256/497\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m154810.98438\u001b[0m\u001b[0m | time: 0.541s\n",
            "| Adam | epoch: 001 | loss: 154810.98438 - acc: 0.2303 -- iter: 272/497\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m149160.06250\u001b[0m\u001b[0m | time: 0.545s\n",
            "| Adam | epoch: 001 | loss: 149160.06250 - acc: 0.2804 -- iter: 288/497\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m156279.59375\u001b[0m\u001b[0m | time: 0.549s\n",
            "| Adam | epoch: 001 | loss: 156279.59375 - acc: 0.2911 -- iter: 304/497\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m161791.57812\u001b[0m\u001b[0m | time: 0.552s\n",
            "| Adam | epoch: 001 | loss: 161791.57812 - acc: 0.2779 -- iter: 320/497\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m154341.07812\u001b[0m\u001b[0m | time: 0.556s\n",
            "| Adam | epoch: 001 | loss: 154341.07812 - acc: 0.2304 -- iter: 336/497\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m165542.70312\u001b[0m\u001b[0m | time: 0.560s\n",
            "| Adam | epoch: 001 | loss: 165542.70312 - acc: 0.1988 -- iter: 352/497\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m155337.65625\u001b[0m\u001b[0m | time: 0.563s\n",
            "| Adam | epoch: 001 | loss: 155337.65625 - acc: 0.2137 -- iter: 368/497\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m138981.76562\u001b[0m\u001b[0m | time: 0.567s\n",
            "| Adam | epoch: 001 | loss: 138981.76562 - acc: 0.2590 -- iter: 384/497\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m133570.39062\u001b[0m\u001b[0m | time: 0.571s\n",
            "| Adam | epoch: 001 | loss: 133570.39062 - acc: 0.2907 -- iter: 400/497\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m128859.54688\u001b[0m\u001b[0m | time: 0.575s\n",
            "| Adam | epoch: 001 | loss: 128859.54688 - acc: 0.3130 -- iter: 416/497\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m122500.86719\u001b[0m\u001b[0m | time: 0.579s\n",
            "| Adam | epoch: 001 | loss: 122500.86719 - acc: 0.3129 -- iter: 432/497\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m116672.00781\u001b[0m\u001b[0m | time: 0.583s\n",
            "| Adam | epoch: 001 | loss: 116672.00781 - acc: 0.2503 -- iter: 448/497\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m114658.36719\u001b[0m\u001b[0m | time: 0.587s\n",
            "| Adam | epoch: 001 | loss: 114658.36719 - acc: 0.2350 -- iter: 464/497\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m101784.18750\u001b[0m\u001b[0m | time: 0.591s\n",
            "| Adam | epoch: 001 | loss: 101784.18750 - acc: 0.2386 -- iter: 480/497\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m97904.81250\u001b[0m\u001b[0m | time: 0.595s\n",
            "| Adam | epoch: 001 | loss: 97904.81250 - acc: 0.2556 -- iter: 496/497\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m105040.35156\u001b[0m\u001b[0m | time: 1.619s\n",
            "| Adam | epoch: 001 | loss: 105040.35156 - acc: 0.2825 | val_loss: 116493.76562 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m93410.15625\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 002 | loss: 93410.15625 - acc: 0.2205 -- iter: 016/497\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m84507.51562\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 002 | loss: 84507.51562 - acc: 0.1732 -- iter: 032/497\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m96701.52344\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 002 | loss: 96701.52344 - acc: 0.2416 -- iter: 048/497\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m95632.09375\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 002 | loss: 95632.09375 - acc: 0.2817 -- iter: 064/497\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m99382.25781\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 002 | loss: 99382.25781 - acc: 0.3379 -- iter: 080/497\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m104317.21875\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 002 | loss: 104317.21875 - acc: 0.3818 -- iter: 096/497\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m105788.47656\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 002 | loss: 105788.47656 - acc: 0.3566 -- iter: 112/497\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m104583.94531\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 002 | loss: 104583.94531 - acc: 0.4069 -- iter: 128/497\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m101941.65625\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 002 | loss: 101941.65625 - acc: 0.4125 -- iter: 144/497\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m99493.07031\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 002 | loss: 99493.07031 - acc: 0.4058 -- iter: 160/497\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m100008.28906\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 002 | loss: 100008.28906 - acc: 0.4334 -- iter: 176/497\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m100805.36719\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 002 | loss: 100805.36719 - acc: 0.4233 -- iter: 192/497\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m107526.11719\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 002 | loss: 107526.11719 - acc: 0.4257 -- iter: 208/497\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m108615.74219\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 002 | loss: 108615.74219 - acc: 0.3964 -- iter: 224/497\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m104945.10156\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 002 | loss: 104945.10156 - acc: 0.4134 -- iter: 240/497\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m103906.06250\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 002 | loss: 103906.06250 - acc: 0.4273 -- iter: 256/497\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m101976.08594\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 002 | loss: 101976.08594 - acc: 0.4190 -- iter: 272/497\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m101530.99219\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 002 | loss: 101530.99219 - acc: 0.4025 -- iter: 288/497\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m103902.67188\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 002 | loss: 103902.67188 - acc: 0.4269 -- iter: 304/497\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m103638.84375\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 002 | loss: 103638.84375 - acc: 0.4285 -- iter: 320/497\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m106655.02344\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 002 | loss: 106655.02344 - acc: 0.4298 -- iter: 336/497\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m102602.87500\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 002 | loss: 102602.87500 - acc: 0.4128 -- iter: 352/497\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m105357.50781\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 002 | loss: 105357.50781 - acc: 0.4163 -- iter: 368/497\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m104550.25000\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 002 | loss: 104550.25000 - acc: 0.4105 -- iter: 384/497\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m102863.98438\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 002 | loss: 102863.98438 - acc: 0.4316 -- iter: 400/497\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m104563.14844\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 002 | loss: 104563.14844 - acc: 0.4238 -- iter: 416/497\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m104387.65625\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 002 | loss: 104387.65625 - acc: 0.4089 -- iter: 432/497\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m108124.78125\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 002 | loss: 108124.78125 - acc: 0.4375 -- iter: 448/497\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m100794.25000\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 002 | loss: 100794.25000 - acc: 0.4538 -- iter: 464/497\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m96998.18750\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 002 | loss: 96998.18750 - acc: 0.4517 -- iter: 480/497\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m91256.05469\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 002 | loss: 91256.05469 - acc: 0.4341 -- iter: 496/497\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m87599.87500\u001b[0m\u001b[0m | time: 1.122s\n",
            "| Adam | epoch: 002 | loss: 87599.87500 - acc: 0.4189 | val_loss: 60817.21875 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m84339.40625\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 003 | loss: 84339.40625 - acc: 0.4212 -- iter: 016/497\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m75965.90625\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 003 | loss: 75965.90625 - acc: 0.4916 -- iter: 032/497\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m68701.78906\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 003 | loss: 68701.78906 - acc: 0.5526 -- iter: 048/497\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m65612.54688\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 003 | loss: 65612.54688 - acc: 0.5315 -- iter: 064/497\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m63426.36328\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 003 | loss: 63426.36328 - acc: 0.5352 -- iter: 080/497\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m62159.45312\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 003 | loss: 62159.45312 - acc: 0.5311 -- iter: 096/497\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m61845.30859\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 003 | loss: 61845.30859 - acc: 0.5204 -- iter: 112/497\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m61381.53906\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 003 | loss: 61381.53906 - acc: 0.5181 -- iter: 128/497\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m61498.83203\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 003 | loss: 61498.83203 - acc: 0.5231 -- iter: 144/497\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m60868.99609\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 003 | loss: 60868.99609 - acc: 0.5068 -- iter: 160/497\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m61424.58594\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 003 | loss: 61424.58594 - acc: 0.5129 -- iter: 176/497\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m60365.66406\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 003 | loss: 60365.66406 - acc: 0.5182 -- iter: 192/497\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m61589.64453\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 003 | loss: 61589.64453 - acc: 0.5030 -- iter: 208/497\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m60663.36719\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 003 | loss: 60663.36719 - acc: 0.5092 -- iter: 224/497\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m60705.39062\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 003 | loss: 60705.39062 - acc: 0.4760 -- iter: 240/497\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m59549.31250\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 003 | loss: 59549.31250 - acc: 0.4465 -- iter: 256/497\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m58904.78125\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 003 | loss: 58904.78125 - acc: 0.4013 -- iter: 272/497\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m58301.57031\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 003 | loss: 58301.57031 - acc: 0.3612 -- iter: 288/497\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m56416.24609\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 003 | loss: 56416.24609 - acc: 0.3438 -- iter: 304/497\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m56365.71484\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 003 | loss: 56365.71484 - acc: 0.3344 -- iter: 320/497\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m55626.05078\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 003 | loss: 55626.05078 - acc: 0.3072 -- iter: 336/497\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m55623.13672\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 003 | loss: 55623.13672 - acc: 0.2890 -- iter: 352/497\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m54123.97656\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 003 | loss: 54123.97656 - acc: 0.2664 -- iter: 368/497\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m55587.19922\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 003 | loss: 55587.19922 - acc: 0.2647 -- iter: 384/497\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m65321.22656\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 003 | loss: 65321.22656 - acc: 0.2508 -- iter: 400/497\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m74237.09375\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 003 | loss: 74237.09375 - acc: 0.2319 -- iter: 416/497\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m77020.48438\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 003 | loss: 77020.48438 - acc: 0.2275 -- iter: 432/497\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m88700.50781\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 003 | loss: 88700.50781 - acc: 0.2172 -- iter: 448/497\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m96350.36719\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 003 | loss: 96350.36719 - acc: 0.2080 -- iter: 464/497\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m99187.03906\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 003 | loss: 99187.03906 - acc: 0.2122 -- iter: 480/497\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m108633.59375\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 003 | loss: 108633.59375 - acc: 0.2035 -- iter: 496/497\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m112793.69531\u001b[0m\u001b[0m | time: 1.121s\n",
            "| Adam | epoch: 003 | loss: 112793.69531 - acc: 0.2081 | val_loss: 169769.59375 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m121008.15625\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 004 | loss: 121008.15625 - acc: 0.1873 -- iter: 016/497\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m127022.32812\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 004 | loss: 127022.32812 - acc: 0.1811 -- iter: 032/497\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m117477.85938\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 004 | loss: 117477.85938 - acc: 0.1630 -- iter: 048/497\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m108887.83594\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 004 | loss: 108887.83594 - acc: 0.1467 -- iter: 064/497\n",
            "Training Step: 101  | total loss: \u001b[1m\u001b[32m114799.95312\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 004 | loss: 114799.95312 - acc: 0.1445 -- iter: 080/497\n",
            "Training Step: 102  | total loss: \u001b[1m\u001b[32m117008.60938\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 004 | loss: 117008.60938 - acc: 0.1363 -- iter: 096/497\n",
            "Training Step: 103  | total loss: \u001b[1m\u001b[32m121635.21875\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 004 | loss: 121635.21875 - acc: 0.1352 -- iter: 112/497\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m126420.35156\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 004 | loss: 126420.35156 - acc: 0.1342 -- iter: 128/497\n",
            "Training Step: 105  | total loss: \u001b[1m\u001b[32m132618.96875\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 004 | loss: 132618.96875 - acc: 0.1270 -- iter: 144/497\n",
            "Training Step: 106  | total loss: \u001b[1m\u001b[32m129645.94531\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 004 | loss: 129645.94531 - acc: 0.1455 -- iter: 160/497\n",
            "Training Step: 107  | total loss: \u001b[1m\u001b[32m123043.42188\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 004 | loss: 123043.42188 - acc: 0.1310 -- iter: 176/497\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m115439.03125\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 004 | loss: 115439.03125 - acc: 0.1179 -- iter: 192/497\n",
            "Training Step: 109  | total loss: \u001b[1m\u001b[32m109657.78125\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 004 | loss: 109657.78125 - acc: 0.1186 -- iter: 208/497\n",
            "Training Step: 110  | total loss: \u001b[1m\u001b[32m103994.75781\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 004 | loss: 103994.75781 - acc: 0.1255 -- iter: 224/497\n",
            "Training Step: 111  | total loss: \u001b[1m\u001b[32m100192.47656\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 004 | loss: 100192.47656 - acc: 0.1317 -- iter: 240/497\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m97583.59375\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 004 | loss: 97583.59375 - acc: 0.1373 -- iter: 256/497\n",
            "Training Step: 113  | total loss: \u001b[1m\u001b[32m95374.66406\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 004 | loss: 95374.66406 - acc: 0.1423 -- iter: 272/497\n",
            "Training Step: 114  | total loss: \u001b[1m\u001b[32m91763.55469\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 004 | loss: 91763.55469 - acc: 0.1406 -- iter: 288/497\n",
            "Training Step: 115  | total loss: \u001b[1m\u001b[32m87626.92969\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 004 | loss: 87626.92969 - acc: 0.1265 -- iter: 304/497\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m84418.50000\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 004 | loss: 84418.50000 - acc: 0.1264 -- iter: 320/497\n",
            "Training Step: 117  | total loss: \u001b[1m\u001b[32m83519.97656\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 004 | loss: 83519.97656 - acc: 0.1450 -- iter: 336/497\n",
            "Training Step: 118  | total loss: \u001b[1m\u001b[32m81324.63281\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 004 | loss: 81324.63281 - acc: 0.1430 -- iter: 352/497\n",
            "Training Step: 119  | total loss: \u001b[1m\u001b[32m76103.96875\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 004 | loss: 76103.96875 - acc: 0.1537 -- iter: 368/497\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m72843.56250\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 004 | loss: 72843.56250 - acc: 0.1446 -- iter: 384/497\n",
            "Training Step: 121  | total loss: \u001b[1m\u001b[32m70893.67969\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 004 | loss: 70893.67969 - acc: 0.1364 -- iter: 400/497\n",
            "Training Step: 122  | total loss: \u001b[1m\u001b[32m69272.81250\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 004 | loss: 69272.81250 - acc: 0.1415 -- iter: 416/497\n",
            "Training Step: 123  | total loss: \u001b[1m\u001b[32m68564.60938\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 004 | loss: 68564.60938 - acc: 0.1648 -- iter: 432/497\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m66981.89062\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 004 | loss: 66981.89062 - acc: 0.1671 -- iter: 448/497\n",
            "Training Step: 125  | total loss: \u001b[1m\u001b[32m65777.12500\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 004 | loss: 65777.12500 - acc: 0.1691 -- iter: 464/497\n",
            "Training Step: 126  | total loss: \u001b[1m\u001b[32m66505.54688\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 004 | loss: 66505.54688 - acc: 0.1522 -- iter: 480/497\n",
            "Training Step: 127  | total loss: \u001b[1m\u001b[32m64265.65625\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 004 | loss: 64265.65625 - acc: 0.1432 -- iter: 496/497\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m63613.30078\u001b[0m\u001b[0m | time: 1.130s\n",
            "| Adam | epoch: 004 | loss: 63613.30078 - acc: 0.1352 | val_loss: 57941.53906 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 129  | total loss: \u001b[1m\u001b[32m63241.84375\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 005 | loss: 63241.84375 - acc: 0.1342 -- iter: 016/497\n",
            "Training Step: 130  | total loss: \u001b[1m\u001b[32m61731.46484\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 005 | loss: 61731.46484 - acc: 0.1270 -- iter: 032/497\n",
            "Training Step: 131  | total loss: \u001b[1m\u001b[32m60214.67188\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 005 | loss: 60214.67188 - acc: 0.1268 -- iter: 048/497\n",
            "Training Step: 132  | total loss: \u001b[1m\u001b[32m59817.04297\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 005 | loss: 59817.04297 - acc: 0.2141 -- iter: 064/497\n",
            "Training Step: 133  | total loss: \u001b[1m\u001b[32m59453.57812\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 005 | loss: 59453.57812 - acc: 0.2927 -- iter: 080/497\n",
            "Training Step: 134  | total loss: \u001b[1m\u001b[32m57831.94531\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 005 | loss: 57831.94531 - acc: 0.2759 -- iter: 096/497\n",
            "Training Step: 135  | total loss: \u001b[1m\u001b[32m58757.63672\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 005 | loss: 58757.63672 - acc: 0.2671 -- iter: 112/497\n",
            "Training Step: 136  | total loss: \u001b[1m\u001b[32m58142.03516\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 005 | loss: 58142.03516 - acc: 0.2779 -- iter: 128/497\n",
            "Training Step: 137  | total loss: \u001b[1m\u001b[32m58177.66797\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 005 | loss: 58177.66797 - acc: 0.2751 -- iter: 144/497\n",
            "Training Step: 138  | total loss: \u001b[1m\u001b[32m57044.67578\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 005 | loss: 57044.67578 - acc: 0.2601 -- iter: 160/497\n",
            "Training Step: 139  | total loss: \u001b[1m\u001b[32m56132.51172\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 005 | loss: 56132.51172 - acc: 0.2466 -- iter: 176/497\n",
            "Training Step: 140  | total loss: \u001b[1m\u001b[32m55287.74219\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 005 | loss: 55287.74219 - acc: 0.2344 -- iter: 192/497\n",
            "Training Step: 141  | total loss: \u001b[1m\u001b[32m55485.86719\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 005 | loss: 55485.86719 - acc: 0.2297 -- iter: 208/497\n",
            "Training Step: 142  | total loss: \u001b[1m\u001b[32m55473.61328\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 005 | loss: 55473.61328 - acc: 0.2255 -- iter: 224/497\n",
            "Training Step: 143  | total loss: \u001b[1m\u001b[32m53680.04688\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 005 | loss: 53680.04688 - acc: 0.2030 -- iter: 240/497\n",
            "Training Step: 144  | total loss: \u001b[1m\u001b[32m53996.87891\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 005 | loss: 53996.87891 - acc: 0.1952 -- iter: 256/497\n",
            "Training Step: 145  | total loss: \u001b[1m\u001b[32m55006.50781\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 005 | loss: 55006.50781 - acc: 0.1881 -- iter: 272/497\n",
            "Training Step: 146  | total loss: \u001b[1m\u001b[32m54603.71875\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 005 | loss: 54603.71875 - acc: 0.1818 -- iter: 288/497\n",
            "Training Step: 147  | total loss: \u001b[1m\u001b[32m55645.06641\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 005 | loss: 55645.06641 - acc: 0.1699 -- iter: 304/497\n",
            "Training Step: 148  | total loss: \u001b[1m\u001b[32m56741.62891\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 005 | loss: 56741.62891 - acc: 0.1592 -- iter: 320/497\n",
            "Training Step: 149  | total loss: \u001b[1m\u001b[32m56695.01953\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 005 | loss: 56695.01953 - acc: 0.1495 -- iter: 336/497\n",
            "Training Step: 150  | total loss: \u001b[1m\u001b[32m57073.07031\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 005 | loss: 57073.07031 - acc: 0.1533 -- iter: 352/497\n",
            "Training Step: 151  | total loss: \u001b[1m\u001b[32m55837.19531\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 005 | loss: 55837.19531 - acc: 0.1505 -- iter: 368/497\n",
            "Training Step: 152  | total loss: \u001b[1m\u001b[32m56470.44922\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 005 | loss: 56470.44922 - acc: 0.1417 -- iter: 384/497\n",
            "Training Step: 153  | total loss: \u001b[1m\u001b[32m55307.33984\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 005 | loss: 55307.33984 - acc: 0.1275 -- iter: 400/497\n",
            "Training Step: 154  | total loss: \u001b[1m\u001b[32m55969.76953\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 005 | loss: 55969.76953 - acc: 0.1272 -- iter: 416/497\n",
            "Training Step: 155  | total loss: \u001b[1m\u001b[32m56200.01562\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 005 | loss: 56200.01562 - acc: 0.1208 -- iter: 432/497\n",
            "Training Step: 156  | total loss: \u001b[1m\u001b[32m55751.75000\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 005 | loss: 55751.75000 - acc: 0.1212 -- iter: 448/497\n",
            "Training Step: 157  | total loss: \u001b[1m\u001b[32m55409.81641\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 005 | loss: 55409.81641 - acc: 0.1216 -- iter: 464/497\n",
            "Training Step: 158  | total loss: \u001b[1m\u001b[32m54944.83984\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 005 | loss: 54944.83984 - acc: 0.1282 -- iter: 480/497\n",
            "Training Step: 159  | total loss: \u001b[1m\u001b[32m53793.11719\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 005 | loss: 53793.11719 - acc: 0.1404 -- iter: 496/497\n",
            "Training Step: 160  | total loss: \u001b[1m\u001b[32m53084.35547\u001b[0m\u001b[0m | time: 1.131s\n",
            "| Adam | epoch: 005 | loss: 53084.35547 - acc: 0.1326 | val_loss: 54824.55078 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 161  | total loss: \u001b[1m\u001b[32m53326.38672\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 006 | loss: 53326.38672 - acc: 0.1318 -- iter: 016/497\n",
            "Training Step: 162  | total loss: \u001b[1m\u001b[32m50979.14844\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 006 | loss: 50979.14844 - acc: 0.1186 -- iter: 032/497\n",
            "Training Step: 163  | total loss: \u001b[1m\u001b[32m51155.69141\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 006 | loss: 51155.69141 - acc: 0.1193 -- iter: 048/497\n",
            "Training Step: 164  | total loss: \u001b[1m\u001b[32m59600.95312\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 006 | loss: 59600.95312 - acc: 0.1448 -- iter: 064/497\n",
            "Training Step: 165  | total loss: \u001b[1m\u001b[32m67822.02344\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 006 | loss: 67822.02344 - acc: 0.1304 -- iter: 080/497\n",
            "Training Step: 166  | total loss: \u001b[1m\u001b[32m75220.98438\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 006 | loss: 75220.98438 - acc: 0.1173 -- iter: 096/497\n",
            "Training Step: 167  | total loss: \u001b[1m\u001b[32m83586.57812\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 006 | loss: 83586.57812 - acc: 0.1181 -- iter: 112/497\n",
            "Training Step: 168  | total loss: \u001b[1m\u001b[32m91074.70312\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 006 | loss: 91074.70312 - acc: 0.1188 -- iter: 128/497\n",
            "Training Step: 169  | total loss: \u001b[1m\u001b[32m98647.76562\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 006 | loss: 98647.76562 - acc: 0.1132 -- iter: 144/497\n",
            "Training Step: 170  | total loss: \u001b[1m\u001b[32m103230.90625\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 006 | loss: 103230.90625 - acc: 0.1206 -- iter: 160/497\n",
            "Training Step: 171  | total loss: \u001b[1m\u001b[32m106716.86719\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 006 | loss: 106716.86719 - acc: 0.1335 -- iter: 176/497\n",
            "Training Step: 172  | total loss: \u001b[1m\u001b[32m110443.23438\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 006 | loss: 110443.23438 - acc: 0.1327 -- iter: 192/497\n",
            "Training Step: 173  | total loss: \u001b[1m\u001b[32m109315.45312\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 006 | loss: 109315.45312 - acc: 0.1444 -- iter: 208/497\n",
            "Training Step: 174  | total loss: \u001b[1m\u001b[32m111940.93750\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 006 | loss: 111940.93750 - acc: 0.1300 -- iter: 224/497\n",
            "Training Step: 175  | total loss: \u001b[1m\u001b[32m120164.27344\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 006 | loss: 120164.27344 - acc: 0.1420 -- iter: 240/497\n",
            "Training Step: 176  | total loss: \u001b[1m\u001b[32m127353.40625\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 006 | loss: 127353.40625 - acc: 0.1403 -- iter: 256/497\n",
            "Training Step: 177  | total loss: \u001b[1m\u001b[32m132784.92188\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 006 | loss: 132784.92188 - acc: 0.1387 -- iter: 272/497\n",
            "Training Step: 178  | total loss: \u001b[1m\u001b[32m139414.46875\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 006 | loss: 139414.46875 - acc: 0.1499 -- iter: 288/497\n",
            "Training Step: 179  | total loss: \u001b[1m\u001b[32m142691.45312\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 006 | loss: 142691.45312 - acc: 0.1536 -- iter: 304/497\n",
            "Training Step: 180  | total loss: \u001b[1m\u001b[32m145541.89062\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 006 | loss: 145541.89062 - acc: 0.1445 -- iter: 320/497\n",
            "Training Step: 181  | total loss: \u001b[1m\u001b[32m145531.93750\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 006 | loss: 145531.93750 - acc: 0.1301 -- iter: 336/497\n",
            "Training Step: 182  | total loss: \u001b[1m\u001b[32m144530.89062\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 006 | loss: 144530.89062 - acc: 0.1358 -- iter: 352/497\n",
            "Training Step: 183  | total loss: \u001b[1m\u001b[32m148566.32812\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 006 | loss: 148566.32812 - acc: 0.1410 -- iter: 368/497\n",
            "Training Step: 184  | total loss: \u001b[1m\u001b[32m148014.76562\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 006 | loss: 148014.76562 - acc: 0.1394 -- iter: 384/497\n",
            "Training Step: 185  | total loss: \u001b[1m\u001b[32m147954.56250\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 006 | loss: 147954.56250 - acc: 0.1504 -- iter: 400/497\n",
            "Training Step: 186  | total loss: \u001b[1m\u001b[32m152056.18750\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 006 | loss: 152056.18750 - acc: 0.1479 -- iter: 416/497\n",
            "Training Step: 187  | total loss: \u001b[1m\u001b[32m156636.92188\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 006 | loss: 156636.92188 - acc: 0.1581 -- iter: 432/497\n",
            "Training Step: 188  | total loss: \u001b[1m\u001b[32m159398.81250\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 006 | loss: 159398.81250 - acc: 0.1548 -- iter: 448/497\n",
            "Training Step: 189  | total loss: \u001b[1m\u001b[32m156694.82812\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 006 | loss: 156694.82812 - acc: 0.1393 -- iter: 464/497\n",
            "Training Step: 190  | total loss: \u001b[1m\u001b[32m156010.46875\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 006 | loss: 156010.46875 - acc: 0.1379 -- iter: 480/497\n",
            "Training Step: 191  | total loss: \u001b[1m\u001b[32m155326.67188\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 006 | loss: 155326.67188 - acc: 0.1303 -- iter: 496/497\n",
            "Training Step: 192  | total loss: \u001b[1m\u001b[32m157468.92188\u001b[0m\u001b[0m | time: 1.125s\n",
            "| Adam | epoch: 006 | loss: 157468.92188 - acc: 0.1298 | val_loss: 172943.25000 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 193  | total loss: \u001b[1m\u001b[32m157157.98438\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 007 | loss: 157157.98438 - acc: 0.1231 -- iter: 016/497\n",
            "Training Step: 194  | total loss: \u001b[1m\u001b[32m157655.35938\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 007 | loss: 157655.35938 - acc: 0.1170 -- iter: 032/497\n",
            "Training Step: 195  | total loss: \u001b[1m\u001b[32m156844.21875\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 007 | loss: 156844.21875 - acc: 0.1053 -- iter: 048/497\n",
            "Training Step: 196  | total loss: \u001b[1m\u001b[32m162756.81250\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 007 | loss: 162756.81250 - acc: 0.1198 -- iter: 064/497\n",
            "Training Step: 197  | total loss: \u001b[1m\u001b[32m162274.34375\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 007 | loss: 162274.34375 - acc: 0.1203 -- iter: 080/497\n",
            "Training Step: 198  | total loss: \u001b[1m\u001b[32m165556.71875\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 007 | loss: 165556.71875 - acc: 0.1083 -- iter: 096/497\n",
            "Training Step: 199  | total loss: \u001b[1m\u001b[32m168510.84375\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 007 | loss: 168510.84375 - acc: 0.0975 -- iter: 112/497\n",
            "Training Step: 200  | total loss: \u001b[1m\u001b[32m172686.06250\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 007 | loss: 172686.06250 - acc: 0.0877 -- iter: 128/497\n",
            "Training Step: 201  | total loss: \u001b[1m\u001b[32m172942.87500\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 007 | loss: 172942.87500 - acc: 0.0977 -- iter: 144/497\n",
            "Training Step: 202  | total loss: \u001b[1m\u001b[32m171343.09375\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 007 | loss: 171343.09375 - acc: 0.1004 -- iter: 160/497\n",
            "Training Step: 203  | total loss: \u001b[1m\u001b[32m172367.75000\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 007 | loss: 172367.75000 - acc: 0.1029 -- iter: 176/497\n",
            "Training Step: 204  | total loss: \u001b[1m\u001b[32m169534.06250\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 007 | loss: 169534.06250 - acc: 0.1113 -- iter: 192/497\n",
            "Training Step: 205  | total loss: \u001b[1m\u001b[32m170927.90625\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 007 | loss: 170927.90625 - acc: 0.1002 -- iter: 208/497\n",
            "Training Step: 206  | total loss: \u001b[1m\u001b[32m167028.85938\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 007 | loss: 167028.85938 - acc: 0.1089 -- iter: 224/497\n",
            "Training Step: 207  | total loss: \u001b[1m\u001b[32m163882.78125\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 007 | loss: 163882.78125 - acc: 0.1355 -- iter: 240/497\n",
            "Training Step: 208  | total loss: \u001b[1m\u001b[32m166581.35938\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 007 | loss: 166581.35938 - acc: 0.1220 -- iter: 256/497\n",
            "Training Step: 209  | total loss: \u001b[1m\u001b[32m164376.10938\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 007 | loss: 164376.10938 - acc: 0.1160 -- iter: 272/497\n",
            "Training Step: 210  | total loss: \u001b[1m\u001b[32m163668.23438\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 007 | loss: 163668.23438 - acc: 0.1044 -- iter: 288/497\n",
            "Training Step: 211  | total loss: \u001b[1m\u001b[32m162381.87500\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 007 | loss: 162381.87500 - acc: 0.1065 -- iter: 304/497\n",
            "Training Step: 212  | total loss: \u001b[1m\u001b[32m161798.78125\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 007 | loss: 161798.78125 - acc: 0.1083 -- iter: 320/497\n",
            "Training Step: 213  | total loss: \u001b[1m\u001b[32m162069.50000\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 007 | loss: 162069.50000 - acc: 0.1288 -- iter: 336/497\n",
            "Training Step: 214  | total loss: \u001b[1m\u001b[32m160120.93750\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 007 | loss: 160120.93750 - acc: 0.1159 -- iter: 352/497\n",
            "Training Step: 215  | total loss: \u001b[1m\u001b[32m155535.59375\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 007 | loss: 155535.59375 - acc: 0.1230 -- iter: 368/497\n",
            "Training Step: 216  | total loss: \u001b[1m\u001b[32m151415.04688\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 007 | loss: 151415.04688 - acc: 0.1232 -- iter: 384/497\n",
            "Training Step: 217  | total loss: \u001b[1m\u001b[32m152445.12500\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 007 | loss: 152445.12500 - acc: 0.1172 -- iter: 400/497\n",
            "Training Step: 218  | total loss: \u001b[1m\u001b[32m152169.12500\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 007 | loss: 152169.12500 - acc: 0.1117 -- iter: 416/497\n",
            "Training Step: 219  | total loss: \u001b[1m\u001b[32m151518.42188\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 007 | loss: 151518.42188 - acc: 0.1318 -- iter: 432/497\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m151749.82812\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 007 | loss: 151749.82812 - acc: 0.1311 -- iter: 448/497\n",
            "Training Step: 221  | total loss: \u001b[1m\u001b[32m155680.76562\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 007 | loss: 155680.76562 - acc: 0.1305 -- iter: 464/497\n",
            "Training Step: 222  | total loss: \u001b[1m\u001b[32m157647.04688\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 007 | loss: 157647.04688 - acc: 0.1299 -- iter: 480/497\n",
            "Training Step: 223  | total loss: \u001b[1m\u001b[32m158941.98438\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 007 | loss: 158941.98438 - acc: 0.1419 -- iter: 496/497\n",
            "Training Step: 224  | total loss: \u001b[1m\u001b[32m159669.64062\u001b[0m\u001b[0m | time: 1.133s\n",
            "| Adam | epoch: 007 | loss: 159669.64062 - acc: 0.1465 | val_loss: 172943.25000 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 225  | total loss: \u001b[1m\u001b[32m161998.42188\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 008 | loss: 161998.42188 - acc: 0.1444 -- iter: 016/497\n",
            "Training Step: 226  | total loss: \u001b[1m\u001b[32m163297.85938\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 008 | loss: 163297.85938 - acc: 0.1299 -- iter: 032/497\n",
            "Training Step: 227  | total loss: \u001b[1m\u001b[32m162174.39062\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 008 | loss: 162174.39062 - acc: 0.1169 -- iter: 048/497\n",
            "Training Step: 228  | total loss: \u001b[1m\u001b[32m169145.32812\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 008 | loss: 169145.32812 - acc: 0.1240 -- iter: 064/497\n",
            "Training Step: 229  | total loss: \u001b[1m\u001b[32m168124.35938\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 008 | loss: 168124.35938 - acc: 0.1366 -- iter: 080/497\n",
            "Training Step: 230  | total loss: \u001b[1m\u001b[32m167898.68750\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 008 | loss: 167898.68750 - acc: 0.1479 -- iter: 096/497\n",
            "Training Step: 231  | total loss: \u001b[1m\u001b[32m163665.96875\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 008 | loss: 163665.96875 - acc: 0.1331 -- iter: 112/497\n",
            "Training Step: 232  | total loss: \u001b[1m\u001b[32m159856.51562\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 008 | loss: 159856.51562 - acc: 0.1198 -- iter: 128/497\n",
            "Training Step: 233  | total loss: \u001b[1m\u001b[32m160623.76562\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 008 | loss: 160623.76562 - acc: 0.1203 -- iter: 144/497\n",
            "Training Step: 234  | total loss: \u001b[1m\u001b[32m159644.20312\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 008 | loss: 159644.20312 - acc: 0.1146 -- iter: 160/497\n",
            "Training Step: 235  | total loss: \u001b[1m\u001b[32m160455.62500\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 008 | loss: 160455.62500 - acc: 0.1031 -- iter: 176/497\n",
            "Training Step: 236  | total loss: \u001b[1m\u001b[32m165056.18750\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 008 | loss: 165056.18750 - acc: 0.1053 -- iter: 192/497\n",
            "Training Step: 237  | total loss: \u001b[1m\u001b[32m163811.56250\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 008 | loss: 163811.56250 - acc: 0.1073 -- iter: 208/497\n",
            "Training Step: 238  | total loss: \u001b[1m\u001b[32m166887.89062\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 008 | loss: 166887.89062 - acc: 0.1153 -- iter: 224/497\n",
            "Training Step: 239  | total loss: \u001b[1m\u001b[32m165977.26562\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 008 | loss: 165977.26562 - acc: 0.1350 -- iter: 240/497\n",
            "Training Step: 240  | total loss: \u001b[1m\u001b[32m161066.98438\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 008 | loss: 161066.98438 - acc: 0.1403 -- iter: 256/497\n",
            "Training Step: 241  | total loss: \u001b[1m\u001b[32m159127.50000\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 008 | loss: 159127.50000 - acc: 0.1512 -- iter: 272/497\n",
            "Training Step: 242  | total loss: \u001b[1m\u001b[32m160021.68750\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 008 | loss: 160021.68750 - acc: 0.1486 -- iter: 288/497\n",
            "Training Step: 243  | total loss: \u001b[1m\u001b[32m159939.43750\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 008 | loss: 159939.43750 - acc: 0.1525 -- iter: 304/497\n",
            "Training Step: 244  | total loss: \u001b[1m\u001b[32m158530.93750\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 008 | loss: 158530.93750 - acc: 0.1560 -- iter: 320/497\n",
            "Training Step: 245  | total loss: \u001b[1m\u001b[32m157758.81250\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 008 | loss: 157758.81250 - acc: 0.1466 -- iter: 336/497\n",
            "Training Step: 246  | total loss: \u001b[1m\u001b[32m159980.35938\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 008 | loss: 159980.35938 - acc: 0.1570 -- iter: 352/497\n",
            "Training Step: 247  | total loss: \u001b[1m\u001b[32m162511.32812\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 008 | loss: 162511.32812 - acc: 0.1600 -- iter: 368/497\n",
            "Training Step: 248  | total loss: \u001b[1m\u001b[32m160676.96875\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 008 | loss: 160676.96875 - acc: 0.1628 -- iter: 384/497\n",
            "Training Step: 249  | total loss: \u001b[1m\u001b[32m157668.78125\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 008 | loss: 157668.78125 - acc: 0.1590 -- iter: 400/497\n",
            "Training Step: 250  | total loss: \u001b[1m\u001b[32m157374.29688\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 008 | loss: 157374.29688 - acc: 0.1494 -- iter: 416/497\n",
            "Training Step: 251  | total loss: \u001b[1m\u001b[32m156968.76562\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 008 | loss: 156968.76562 - acc: 0.1407 -- iter: 432/497\n",
            "Training Step: 252  | total loss: \u001b[1m\u001b[32m157449.09375\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 008 | loss: 157449.09375 - acc: 0.1266 -- iter: 448/497\n",
            "Training Step: 253  | total loss: \u001b[1m\u001b[32m163563.25000\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 008 | loss: 163563.25000 - acc: 0.1264 -- iter: 464/497\n",
            "Training Step: 254  | total loss: \u001b[1m\u001b[32m169718.31250\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 008 | loss: 169718.31250 - acc: 0.1388 -- iter: 480/497\n",
            "Training Step: 255  | total loss: \u001b[1m\u001b[32m169114.93750\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 008 | loss: 169114.93750 - acc: 0.1374 -- iter: 496/497\n",
            "Training Step: 256  | total loss: \u001b[1m\u001b[32m165352.95312\u001b[0m\u001b[0m | time: 1.124s\n",
            "| Adam | epoch: 008 | loss: 165352.95312 - acc: 0.1299 | val_loss: 172943.25000 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 257  | total loss: \u001b[1m\u001b[32m167542.93750\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 009 | loss: 167542.93750 - acc: 0.1294 -- iter: 016/497\n",
            "Training Step: 258  | total loss: \u001b[1m\u001b[32m161147.68750\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 009 | loss: 161147.68750 - acc: 0.1290 -- iter: 032/497\n",
            "Training Step: 259  | total loss: \u001b[1m\u001b[32m162384.87500\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 009 | loss: 162384.87500 - acc: 0.1161 -- iter: 048/497\n",
            "Training Step: 260  | total loss: \u001b[1m\u001b[32m161221.65625\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 009 | loss: 161221.65625 - acc: 0.1170 -- iter: 064/497\n",
            "Training Step: 261  | total loss: \u001b[1m\u001b[32m159063.25000\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 009 | loss: 159063.25000 - acc: 0.1178 -- iter: 080/497\n",
            "Training Step: 262  | total loss: \u001b[1m\u001b[32m160555.32812\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 009 | loss: 160555.32812 - acc: 0.1310 -- iter: 096/497\n",
            "Training Step: 263  | total loss: \u001b[1m\u001b[32m162789.78125\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 009 | loss: 162789.78125 - acc: 0.1242 -- iter: 112/497\n",
            "Training Step: 264  | total loss: \u001b[1m\u001b[32m151430.73438\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 009 | loss: 151430.73438 - acc: 0.1117 -- iter: 128/497\n",
            "Training Step: 265  | total loss: \u001b[1m\u001b[32m141207.59375\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 009 | loss: 141207.59375 - acc: 0.1006 -- iter: 144/497\n",
            "Training Step: 266  | total loss: \u001b[1m\u001b[32m142187.37500\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 009 | loss: 142187.37500 - acc: 0.1093 -- iter: 160/497\n",
            "Training Step: 267  | total loss: \u001b[1m\u001b[32m146226.32812\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 009 | loss: 146226.32812 - acc: 0.1171 -- iter: 176/497\n",
            "Training Step: 268  | total loss: \u001b[1m\u001b[32m151770.73438\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 009 | loss: 151770.73438 - acc: 0.1241 -- iter: 192/497\n",
            "Training Step: 269  | total loss: \u001b[1m\u001b[32m155495.14062\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 009 | loss: 155495.14062 - acc: 0.1242 -- iter: 208/497\n",
            "Training Step: 270  | total loss: \u001b[1m\u001b[32m158600.20312\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 009 | loss: 158600.20312 - acc: 0.1180 -- iter: 224/497\n",
            "Training Step: 271  | total loss: \u001b[1m\u001b[32m159364.73438\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 009 | loss: 159364.73438 - acc: 0.1187 -- iter: 240/497\n",
            "Training Step: 272  | total loss: \u001b[1m\u001b[32m154172.39062\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 009 | loss: 154172.39062 - acc: 0.1194 -- iter: 256/497\n",
            "Training Step: 273  | total loss: \u001b[1m\u001b[32m156780.15625\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 009 | loss: 156780.15625 - acc: 0.1137 -- iter: 272/497\n",
            "Training Step: 274  | total loss: \u001b[1m\u001b[32m157086.60938\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 009 | loss: 157086.60938 - acc: 0.1273 -- iter: 288/497\n",
            "Training Step: 275  | total loss: \u001b[1m\u001b[32m157980.75000\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 009 | loss: 157980.75000 - acc: 0.1271 -- iter: 304/497\n",
            "Training Step: 276  | total loss: \u001b[1m\u001b[32m152967.90625\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 009 | loss: 152967.90625 - acc: 0.1331 -- iter: 320/497\n",
            "Training Step: 277  | total loss: \u001b[1m\u001b[32m147976.29688\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 009 | loss: 147976.29688 - acc: 0.1323 -- iter: 336/497\n",
            "Training Step: 278  | total loss: \u001b[1m\u001b[32m148021.85938\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 009 | loss: 148021.85938 - acc: 0.1253 -- iter: 352/497\n",
            "Training Step: 279  | total loss: \u001b[1m\u001b[32m145604.50000\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 009 | loss: 145604.50000 - acc: 0.1128 -- iter: 368/497\n",
            "Training Step: 280  | total loss: \u001b[1m\u001b[32m142439.60938\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 009 | loss: 142439.60938 - acc: 0.1328 -- iter: 384/497\n",
            "Training Step: 281  | total loss: \u001b[1m\u001b[32m139235.12500\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 009 | loss: 139235.12500 - acc: 0.1507 -- iter: 400/497\n",
            "Training Step: 282  | total loss: \u001b[1m\u001b[32m138134.04688\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 009 | loss: 138134.04688 - acc: 0.1419 -- iter: 416/497\n",
            "Training Step: 283  | total loss: \u001b[1m\u001b[32m136492.95312\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 009 | loss: 136492.95312 - acc: 0.1527 -- iter: 432/497\n",
            "Training Step: 284  | total loss: \u001b[1m\u001b[32m134554.46875\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 009 | loss: 134554.46875 - acc: 0.1437 -- iter: 448/497\n",
            "Training Step: 285  | total loss: \u001b[1m\u001b[32m133809.29688\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 009 | loss: 133809.29688 - acc: 0.1356 -- iter: 464/497\n",
            "Training Step: 286  | total loss: \u001b[1m\u001b[32m131529.25000\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 009 | loss: 131529.25000 - acc: 0.1345 -- iter: 480/497\n",
            "Training Step: 287  | total loss: \u001b[1m\u001b[32m127625.50000\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 009 | loss: 127625.50000 - acc: 0.1336 -- iter: 496/497\n",
            "Training Step: 288  | total loss: \u001b[1m\u001b[32m125280.57031\u001b[0m\u001b[0m | time: 1.131s\n",
            "| Adam | epoch: 009 | loss: 125280.57031 - acc: 0.1327 | val_loss: 121976.17188 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 289  | total loss: \u001b[1m\u001b[32m127441.58594\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 010 | loss: 127441.58594 - acc: 0.1257 -- iter: 016/497\n",
            "Training Step: 290  | total loss: \u001b[1m\u001b[32m127843.52344\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 010 | loss: 127843.52344 - acc: 0.1194 -- iter: 032/497\n",
            "Training Step: 291  | total loss: \u001b[1m\u001b[32m126301.22656\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 010 | loss: 126301.22656 - acc: 0.1262 -- iter: 048/497\n",
            "Training Step: 292  | total loss: \u001b[1m\u001b[32m124478.54688\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 010 | loss: 124478.54688 - acc: 0.1136 -- iter: 064/497\n",
            "Training Step: 293  | total loss: \u001b[1m\u001b[32m122037.17969\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 010 | loss: 122037.17969 - acc: 0.1210 -- iter: 080/497\n",
            "Training Step: 294  | total loss: \u001b[1m\u001b[32m121323.06250\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 010 | loss: 121323.06250 - acc: 0.1214 -- iter: 096/497\n",
            "Training Step: 295  | total loss: \u001b[1m\u001b[32m119891.29688\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 010 | loss: 119891.29688 - acc: 0.1155 -- iter: 112/497\n",
            "Training Step: 296  | total loss: \u001b[1m\u001b[32m118164.55469\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 010 | loss: 118164.55469 - acc: 0.1289 -- iter: 128/497\n",
            "Training Step: 297  | total loss: \u001b[1m\u001b[32m117939.62500\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 010 | loss: 117939.62500 - acc: 0.1160 -- iter: 144/497\n",
            "Training Step: 298  | total loss: \u001b[1m\u001b[32m117728.76562\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 010 | loss: 117728.76562 - acc: 0.1044 -- iter: 160/497\n",
            "Training Step: 299  | total loss: \u001b[1m\u001b[32m116953.52344\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 010 | loss: 116953.52344 - acc: 0.1190 -- iter: 176/497\n",
            "Training Step: 300  | total loss: \u001b[1m\u001b[32m115458.96875\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 010 | loss: 115458.96875 - acc: 0.1196 -- iter: 192/497\n",
            "Training Step: 301  | total loss: \u001b[1m\u001b[32m116022.36719\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 010 | loss: 116022.36719 - acc: 0.1076 -- iter: 208/497\n",
            "Training Step: 302  | total loss: \u001b[1m\u001b[32m115405.50781\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 010 | loss: 115405.50781 - acc: 0.0969 -- iter: 224/497\n",
            "Training Step: 303  | total loss: \u001b[1m\u001b[32m117948.05469\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 010 | loss: 117948.05469 - acc: 0.1059 -- iter: 240/497\n",
            "Training Step: 304  | total loss: \u001b[1m\u001b[32m116643.96875\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 010 | loss: 116643.96875 - acc: 0.1203 -- iter: 256/497\n",
            "Training Step: 305  | total loss: \u001b[1m\u001b[32m116076.82812\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 010 | loss: 116076.82812 - acc: 0.1146 -- iter: 272/497\n",
            "Training Step: 306  | total loss: \u001b[1m\u001b[32m113198.22656\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 010 | loss: 113198.22656 - acc: 0.1031 -- iter: 288/497\n",
            "Training Step: 307  | total loss: \u001b[1m\u001b[32m111187.72656\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 010 | loss: 111187.72656 - acc: 0.1115 -- iter: 304/497\n",
            "Training Step: 308  | total loss: \u001b[1m\u001b[32m109623.57031\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 010 | loss: 109623.57031 - acc: 0.1066 -- iter: 320/497\n",
            "Training Step: 309  | total loss: \u001b[1m\u001b[32m109191.84375\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 010 | loss: 109191.84375 - acc: 0.1147 -- iter: 336/497\n",
            "Training Step: 310  | total loss: \u001b[1m\u001b[32m112544.23438\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 010 | loss: 112544.23438 - acc: 0.1095 -- iter: 352/497\n",
            "Training Step: 311  | total loss: \u001b[1m\u001b[32m111945.50781\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 010 | loss: 111945.50781 - acc: 0.1173 -- iter: 368/497\n",
            "Training Step: 312  | total loss: \u001b[1m\u001b[32m112329.35938\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 010 | loss: 112329.35938 - acc: 0.1181 -- iter: 384/497\n",
            "Training Step: 313  | total loss: \u001b[1m\u001b[32m109398.07812\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 010 | loss: 109398.07812 - acc: 0.1250 -- iter: 400/497\n",
            "Training Step: 314  | total loss: \u001b[1m\u001b[32m111052.50000\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 010 | loss: 111052.50000 - acc: 0.1125 -- iter: 416/497\n",
            "Training Step: 315  | total loss: \u001b[1m\u001b[32m109001.53125\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 010 | loss: 109001.53125 - acc: 0.1075 -- iter: 432/497\n",
            "Training Step: 316  | total loss: \u001b[1m\u001b[32m106812.05469\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 010 | loss: 106812.05469 - acc: 0.1093 -- iter: 448/497\n",
            "Training Step: 317  | total loss: \u001b[1m\u001b[32m108801.34375\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 010 | loss: 108801.34375 - acc: 0.0983 -- iter: 464/497\n",
            "Training Step: 318  | total loss: \u001b[1m\u001b[32m114631.98438\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 010 | loss: 114631.98438 - acc: 0.1010 -- iter: 480/497\n",
            "Training Step: 319  | total loss: \u001b[1m\u001b[32m112815.65625\u001b[0m\u001b[0m | time: 0.131s\n",
            "| Adam | epoch: 010 | loss: 112815.65625 - acc: 0.1284 -- iter: 496/497\n",
            "Training Step: 320  | total loss: \u001b[1m\u001b[32m112172.10938\u001b[0m\u001b[0m | time: 1.138s\n",
            "| Adam | epoch: 010 | loss: 112172.10938 - acc: 0.1531 | val_loss: 117813.89062 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 321  | total loss: \u001b[1m\u001b[32m114422.47656\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 011 | loss: 114422.47656 - acc: 0.1565 -- iter: 016/497\n",
            "Training Step: 322  | total loss: \u001b[1m\u001b[32m113888.96875\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 011 | loss: 113888.96875 - acc: 0.1596 -- iter: 032/497\n",
            "Training Step: 323  | total loss: \u001b[1m\u001b[32m114642.73438\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 011 | loss: 114642.73438 - acc: 0.1624 -- iter: 048/497\n",
            "Training Step: 324  | total loss: \u001b[1m\u001b[32m117348.69531\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 011 | loss: 117348.69531 - acc: 0.1649 -- iter: 064/497\n",
            "Training Step: 325  | total loss: \u001b[1m\u001b[32m115055.45312\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 011 | loss: 115055.45312 - acc: 0.1547 -- iter: 080/497\n",
            "Training Step: 326  | total loss: \u001b[1m\u001b[32m116983.66406\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 011 | loss: 116983.66406 - acc: 0.1517 -- iter: 096/497\n",
            "Training Step: 327  | total loss: \u001b[1m\u001b[32m115004.00000\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 011 | loss: 115004.00000 - acc: 0.1428 -- iter: 112/497\n",
            "Training Step: 328  | total loss: \u001b[1m\u001b[32m114993.17188\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 011 | loss: 114993.17188 - acc: 0.1472 -- iter: 128/497\n",
            "Training Step: 329  | total loss: \u001b[1m\u001b[32m116112.00781\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 011 | loss: 116112.00781 - acc: 0.1388 -- iter: 144/497\n",
            "Training Step: 330  | total loss: \u001b[1m\u001b[32m109602.15625\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 011 | loss: 109602.15625 - acc: 0.1249 -- iter: 160/497\n",
            "Training Step: 331  | total loss: \u001b[1m\u001b[32m103740.19531\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 011 | loss: 103740.19531 - acc: 0.1124 -- iter: 176/497\n",
            "Training Step: 332  | total loss: \u001b[1m\u001b[32m106225.34375\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 011 | loss: 106225.34375 - acc: 0.1262 -- iter: 192/497\n",
            "Training Step: 333  | total loss: \u001b[1m\u001b[32m106712.52344\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 011 | loss: 106712.52344 - acc: 0.1448 -- iter: 208/497\n",
            "Training Step: 334  | total loss: \u001b[1m\u001b[32m105202.50781\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 011 | loss: 105202.50781 - acc: 0.1553 -- iter: 224/497\n",
            "Training Step: 335  | total loss: \u001b[1m\u001b[32m108745.41406\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 011 | loss: 108745.41406 - acc: 0.1585 -- iter: 240/497\n",
            "Training Step: 336  | total loss: \u001b[1m\u001b[32m106110.82031\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 011 | loss: 106110.82031 - acc: 0.1427 -- iter: 256/497\n",
            "Training Step: 337  | total loss: \u001b[1m\u001b[32m106830.16406\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 011 | loss: 106830.16406 - acc: 0.1472 -- iter: 272/497\n",
            "Training Step: 338  | total loss: \u001b[1m\u001b[32m108851.05469\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 011 | loss: 108851.05469 - acc: 0.1324 -- iter: 288/497\n",
            "Training Step: 339  | total loss: \u001b[1m\u001b[32m106325.61719\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 011 | loss: 106325.61719 - acc: 0.1317 -- iter: 304/497\n",
            "Training Step: 340  | total loss: \u001b[1m\u001b[32m108901.80469\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 011 | loss: 108901.80469 - acc: 0.1185 -- iter: 320/497\n",
            "Training Step: 341  | total loss: \u001b[1m\u001b[32m110054.78906\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 011 | loss: 110054.78906 - acc: 0.1067 -- iter: 336/497\n",
            "Training Step: 342  | total loss: \u001b[1m\u001b[32m110938.70312\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 011 | loss: 110938.70312 - acc: 0.1085 -- iter: 352/497\n",
            "Training Step: 343  | total loss: \u001b[1m\u001b[32m111947.00000\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 011 | loss: 111947.00000 - acc: 0.0977 -- iter: 368/497\n",
            "Training Step: 344  | total loss: \u001b[1m\u001b[32m110983.35156\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 011 | loss: 110983.35156 - acc: 0.0941 -- iter: 384/497\n",
            "Training Step: 345  | total loss: \u001b[1m\u001b[32m110347.18750\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 011 | loss: 110347.18750 - acc: 0.0972 -- iter: 400/497\n",
            "Training Step: 346  | total loss: \u001b[1m\u001b[32m110876.21094\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 011 | loss: 110876.21094 - acc: 0.0938 -- iter: 416/497\n",
            "Training Step: 347  | total loss: \u001b[1m\u001b[32m115034.66406\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 011 | loss: 115034.66406 - acc: 0.0969 -- iter: 432/497\n",
            "Training Step: 348  | total loss: \u001b[1m\u001b[32m114566.84375\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 011 | loss: 114566.84375 - acc: 0.0872 -- iter: 448/497\n",
            "Training Step: 349  | total loss: \u001b[1m\u001b[32m113409.47656\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 011 | loss: 113409.47656 - acc: 0.1035 -- iter: 464/497\n",
            "Training Step: 350  | total loss: \u001b[1m\u001b[32m115074.30469\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 011 | loss: 115074.30469 - acc: 0.1056 -- iter: 480/497\n",
            "Training Step: 351  | total loss: \u001b[1m\u001b[32m115473.03906\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 011 | loss: 115473.03906 - acc: 0.1138 -- iter: 496/497\n",
            "Training Step: 352  | total loss: \u001b[1m\u001b[32m111877.43750\u001b[0m\u001b[0m | time: 1.130s\n",
            "| Adam | epoch: 011 | loss: 111877.43750 - acc: 0.1212 | val_loss: 115564.66406 - val_acc: 0.1000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 353  | total loss: \u001b[1m\u001b[32m112177.27344\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 012 | loss: 112177.27344 - acc: 0.1153 -- iter: 016/497\n",
            "Training Step: 354  | total loss: \u001b[1m\u001b[32m108606.68750\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 012 | loss: 108606.68750 - acc: 0.1225 -- iter: 032/497\n",
            "Training Step: 355  | total loss: \u001b[1m\u001b[32m109318.04688\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 012 | loss: 109318.04688 - acc: 0.1415 -- iter: 048/497\n",
            "Training Step: 356  | total loss: \u001b[1m\u001b[32m110846.14844\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 012 | loss: 110846.14844 - acc: 0.1399 -- iter: 064/497\n",
            "Training Step: 357  | total loss: \u001b[1m\u001b[32m110143.28125\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 012 | loss: 110143.28125 - acc: 0.1446 -- iter: 080/497\n",
            "Training Step: 358  | total loss: \u001b[1m\u001b[32m107033.74219\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 012 | loss: 107033.74219 - acc: 0.1489 -- iter: 096/497\n",
            "Training Step: 359  | total loss: \u001b[1m\u001b[32m107446.05469\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 012 | loss: 107446.05469 - acc: 0.1403 -- iter: 112/497\n",
            "Training Step: 360  | total loss: \u001b[1m\u001b[32m105868.68750\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 012 | loss: 105868.68750 - acc: 0.1513 -- iter: 128/497\n",
            "Training Step: 361  | total loss: \u001b[1m\u001b[32m102633.04688\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 012 | loss: 102633.04688 - acc: 0.1424 -- iter: 144/497\n",
            "Training Step: 362  | total loss: \u001b[1m\u001b[32m106433.44531\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 012 | loss: 106433.44531 - acc: 0.1406 -- iter: 160/497\n",
            "Training Step: 363  | total loss: \u001b[1m\u001b[32m114369.89062\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 012 | loss: 114369.89062 - acc: 0.2266 -- iter: 176/497\n",
            "Training Step: 364  | total loss: \u001b[1m\u001b[32m121512.69531\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 012 | loss: 121512.69531 - acc: 0.3039 -- iter: 192/497\n",
            "Training Step: 365  | total loss: \u001b[1m\u001b[32m120808.65625\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 012 | loss: 120808.65625 - acc: 0.2985 -- iter: 208/497\n",
            "Training Step: 366  | total loss: \u001b[1m\u001b[32m121851.18750\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 012 | loss: 121851.18750 - acc: 0.2999 -- iter: 224/497\n",
            "Training Step: 367  | total loss: \u001b[1m\u001b[32m122677.18750\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 012 | loss: 122677.18750 - acc: 0.2887 -- iter: 240/497\n",
            "Training Step: 368  | total loss: \u001b[1m\u001b[32m118740.07812\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 012 | loss: 118740.07812 - acc: 0.2973 -- iter: 256/497\n",
            "Training Step: 369  | total loss: \u001b[1m\u001b[32m116286.43750\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 012 | loss: 116286.43750 - acc: 0.2801 -- iter: 272/497\n",
            "Training Step: 370  | total loss: \u001b[1m\u001b[32m119195.40625\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 012 | loss: 119195.40625 - acc: 0.2646 -- iter: 288/497\n",
            "Training Step: 371  | total loss: \u001b[1m\u001b[32m117535.45312\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 012 | loss: 117535.45312 - acc: 0.2756 -- iter: 304/497\n",
            "Training Step: 372  | total loss: \u001b[1m\u001b[32m114877.35938\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 012 | loss: 114877.35938 - acc: 0.2793 -- iter: 320/497\n",
            "Training Step: 373  | total loss: \u001b[1m\u001b[32m113632.77344\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 012 | loss: 113632.77344 - acc: 0.2826 -- iter: 336/497\n",
            "Training Step: 374  | total loss: \u001b[1m\u001b[32m114876.29688\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 012 | loss: 114876.29688 - acc: 0.2919 -- iter: 352/497\n",
            "Training Step: 375  | total loss: \u001b[1m\u001b[32m116530.43750\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 012 | loss: 116530.43750 - acc: 0.2939 -- iter: 368/497\n",
            "Training Step: 376  | total loss: \u001b[1m\u001b[32m116533.35156\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 012 | loss: 116533.35156 - acc: 0.2708 -- iter: 384/497\n",
            "Training Step: 377  | total loss: \u001b[1m\u001b[32m112173.74219\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 012 | loss: 112173.74219 - acc: 0.2687 -- iter: 400/497\n",
            "Training Step: 378  | total loss: \u001b[1m\u001b[32m109136.78906\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 012 | loss: 109136.78906 - acc: 0.2793 -- iter: 416/497\n",
            "Training Step: 379  | total loss: \u001b[1m\u001b[32m107831.52344\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 012 | loss: 107831.52344 - acc: 0.2764 -- iter: 432/497\n",
            "Training Step: 380  | total loss: \u001b[1m\u001b[32m107734.76562\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 012 | loss: 107734.76562 - acc: 0.2863 -- iter: 448/497\n",
            "Training Step: 381  | total loss: \u001b[1m\u001b[32m109035.24219\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 012 | loss: 109035.24219 - acc: 0.2764 -- iter: 464/497\n",
            "Training Step: 382  | total loss: \u001b[1m\u001b[32m109620.00781\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 012 | loss: 109620.00781 - acc: 0.2612 -- iter: 480/497\n",
            "Training Step: 383  | total loss: \u001b[1m\u001b[32m109981.50000\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 012 | loss: 109981.50000 - acc: 0.2476 -- iter: 496/497\n",
            "Training Step: 384  | total loss: \u001b[1m\u001b[32m105746.50000\u001b[0m\u001b[0m | time: 1.127s\n",
            "| Adam | epoch: 012 | loss: 105746.50000 - acc: 0.2354 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 385  | total loss: \u001b[1m\u001b[32m105973.67969\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 013 | loss: 105973.67969 - acc: 0.2306 -- iter: 016/497\n",
            "Training Step: 386  | total loss: \u001b[1m\u001b[32m102112.13281\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 013 | loss: 102112.13281 - acc: 0.2200 -- iter: 032/497\n",
            "Training Step: 387  | total loss: \u001b[1m\u001b[32m101903.44531\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 013 | loss: 101903.44531 - acc: 0.2355 -- iter: 048/497\n",
            "Training Step: 388  | total loss: \u001b[1m\u001b[32m101998.76562\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 013 | loss: 101998.76562 - acc: 0.2245 -- iter: 064/497\n",
            "Training Step: 389  | total loss: \u001b[1m\u001b[32m103393.62500\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 013 | loss: 103393.62500 - acc: 0.2145 -- iter: 080/497\n",
            "Training Step: 390  | total loss: \u001b[1m\u001b[32m104219.00781\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 013 | loss: 104219.00781 - acc: 0.2118 -- iter: 096/497\n",
            "Training Step: 391  | total loss: \u001b[1m\u001b[32m109268.69531\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 013 | loss: 109268.69531 - acc: 0.2156 -- iter: 112/497\n",
            "Training Step: 392  | total loss: \u001b[1m\u001b[32m108728.84375\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 013 | loss: 108728.84375 - acc: 0.2191 -- iter: 128/497\n",
            "Training Step: 393  | total loss: \u001b[1m\u001b[32m109645.15625\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 013 | loss: 109645.15625 - acc: 0.2409 -- iter: 144/497\n",
            "Training Step: 394  | total loss: \u001b[1m\u001b[32m109656.24219\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 013 | loss: 109656.24219 - acc: 0.2543 -- iter: 160/497\n",
            "Training Step: 395  | total loss: \u001b[1m\u001b[32m107158.41406\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 013 | loss: 107158.41406 - acc: 0.2539 -- iter: 176/497\n",
            "Training Step: 396  | total loss: \u001b[1m\u001b[32m98899.65625\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 013 | loss: 98899.65625 - acc: 0.2285 -- iter: 192/497\n",
            "Training Step: 397  | total loss: \u001b[1m\u001b[32m91466.78125\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 013 | loss: 91466.78125 - acc: 0.2057 -- iter: 208/497\n",
            "Training Step: 398  | total loss: \u001b[1m\u001b[32m94417.22656\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 013 | loss: 94417.22656 - acc: 0.2163 -- iter: 224/497\n",
            "Training Step: 399  | total loss: \u001b[1m\u001b[32m92428.32031\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 013 | loss: 92428.32031 - acc: 0.2072 -- iter: 240/497\n",
            "Training Step: 400  | total loss: \u001b[1m\u001b[32m93850.12500\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 013 | loss: 93850.12500 - acc: 0.2177 -- iter: 256/497\n",
            "Training Step: 401  | total loss: \u001b[1m\u001b[32m94107.05469\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 013 | loss: 94107.05469 - acc: 0.2210 -- iter: 272/497\n",
            "Training Step: 402  | total loss: \u001b[1m\u001b[32m97191.39844\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 013 | loss: 97191.39844 - acc: 0.2239 -- iter: 288/497\n",
            "Training Step: 403  | total loss: \u001b[1m\u001b[32m99726.37500\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 013 | loss: 99726.37500 - acc: 0.2265 -- iter: 304/497\n",
            "Training Step: 404  | total loss: \u001b[1m\u001b[32m101190.28906\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 013 | loss: 101190.28906 - acc: 0.2226 -- iter: 320/497\n",
            "Training Step: 405  | total loss: \u001b[1m\u001b[32m100342.12500\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 013 | loss: 100342.12500 - acc: 0.2316 -- iter: 336/497\n",
            "Training Step: 406  | total loss: \u001b[1m\u001b[32m100417.65625\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 013 | loss: 100417.65625 - acc: 0.2272 -- iter: 352/497\n",
            "Training Step: 407  | total loss: \u001b[1m\u001b[32m99016.38281\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 013 | loss: 99016.38281 - acc: 0.2294 -- iter: 368/497\n",
            "Training Step: 408  | total loss: \u001b[1m\u001b[32m99170.00781\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 013 | loss: 99170.00781 - acc: 0.2315 -- iter: 384/497\n",
            "Training Step: 409  | total loss: \u001b[1m\u001b[32m97673.32812\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 013 | loss: 97673.32812 - acc: 0.2396 -- iter: 400/497\n",
            "Training Step: 410  | total loss: \u001b[1m\u001b[32m96065.82031\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 013 | loss: 96065.82031 - acc: 0.2406 -- iter: 416/497\n",
            "Training Step: 411  | total loss: \u001b[1m\u001b[32m97296.02344\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 013 | loss: 97296.02344 - acc: 0.2416 -- iter: 432/497\n",
            "Training Step: 412  | total loss: \u001b[1m\u001b[32m96538.38281\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 013 | loss: 96538.38281 - acc: 0.2549 -- iter: 448/497\n",
            "Training Step: 413  | total loss: \u001b[1m\u001b[32m98293.21875\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 013 | loss: 98293.21875 - acc: 0.2607 -- iter: 464/497\n",
            "Training Step: 414  | total loss: \u001b[1m\u001b[32m99069.15625\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 013 | loss: 99069.15625 - acc: 0.2596 -- iter: 480/497\n",
            "Training Step: 415  | total loss: \u001b[1m\u001b[32m98336.32812\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 013 | loss: 98336.32812 - acc: 0.2524 -- iter: 496/497\n",
            "Training Step: 416  | total loss: \u001b[1m\u001b[32m100110.56250\u001b[0m\u001b[0m | time: 1.123s\n",
            "| Adam | epoch: 013 | loss: 100110.56250 - acc: 0.2584 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 417  | total loss: \u001b[1m\u001b[32m97572.05469\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 014 | loss: 97572.05469 - acc: 0.2451 -- iter: 016/497\n",
            "Training Step: 418  | total loss: \u001b[1m\u001b[32m97127.02344\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 014 | loss: 97127.02344 - acc: 0.2456 -- iter: 032/497\n",
            "Training Step: 419  | total loss: \u001b[1m\u001b[32m100156.10938\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 014 | loss: 100156.10938 - acc: 0.2398 -- iter: 048/497\n",
            "Training Step: 420  | total loss: \u001b[1m\u001b[32m99560.18750\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 014 | loss: 99560.18750 - acc: 0.2408 -- iter: 064/497\n",
            "Training Step: 421  | total loss: \u001b[1m\u001b[32m99738.91406\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 014 | loss: 99738.91406 - acc: 0.2542 -- iter: 080/497\n",
            "Training Step: 422  | total loss: \u001b[1m\u001b[32m102444.40625\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 014 | loss: 102444.40625 - acc: 0.2350 -- iter: 096/497\n",
            "Training Step: 423  | total loss: \u001b[1m\u001b[32m100962.94531\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 014 | loss: 100962.94531 - acc: 0.2240 -- iter: 112/497\n",
            "Training Step: 424  | total loss: \u001b[1m\u001b[32m100046.13281\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 014 | loss: 100046.13281 - acc: 0.2266 -- iter: 128/497\n",
            "Training Step: 425  | total loss: \u001b[1m\u001b[32m101623.82812\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 014 | loss: 101623.82812 - acc: 0.2290 -- iter: 144/497\n",
            "Training Step: 426  | total loss: \u001b[1m\u001b[32m104773.45312\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 014 | loss: 104773.45312 - acc: 0.2498 -- iter: 160/497\n",
            "Training Step: 427  | total loss: \u001b[1m\u001b[32m106777.12500\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 014 | loss: 106777.12500 - acc: 0.2373 -- iter: 176/497\n",
            "Training Step: 428  | total loss: \u001b[1m\u001b[32m106397.55469\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 014 | loss: 106397.55469 - acc: 0.2386 -- iter: 192/497\n",
            "Training Step: 429  | total loss: \u001b[1m\u001b[32m114026.96875\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 014 | loss: 114026.96875 - acc: 0.2147 -- iter: 208/497\n",
            "Training Step: 430  | total loss: \u001b[1m\u001b[32m120893.44531\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 014 | loss: 120893.44531 - acc: 0.1933 -- iter: 224/497\n",
            "Training Step: 431  | total loss: \u001b[1m\u001b[32m118729.78125\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 014 | loss: 118729.78125 - acc: 0.1864 -- iter: 240/497\n",
            "Training Step: 432  | total loss: \u001b[1m\u001b[32m118982.11719\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 014 | loss: 118982.11719 - acc: 0.1803 -- iter: 256/497\n",
            "Training Step: 433  | total loss: \u001b[1m\u001b[32m115229.12500\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 014 | loss: 115229.12500 - acc: 0.1935 -- iter: 272/497\n",
            "Training Step: 434  | total loss: \u001b[1m\u001b[32m112883.49219\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 014 | loss: 112883.49219 - acc: 0.2054 -- iter: 288/497\n",
            "Training Step: 435  | total loss: \u001b[1m\u001b[32m115878.59375\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 014 | loss: 115878.59375 - acc: 0.2161 -- iter: 304/497\n",
            "Training Step: 436  | total loss: \u001b[1m\u001b[32m112262.25000\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 014 | loss: 112262.25000 - acc: 0.2008 -- iter: 320/497\n",
            "Training Step: 437  | total loss: \u001b[1m\u001b[32m115652.30469\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 014 | loss: 115652.30469 - acc: 0.2182 -- iter: 336/497\n",
            "Training Step: 438  | total loss: \u001b[1m\u001b[32m111712.68750\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 014 | loss: 111712.68750 - acc: 0.2026 -- iter: 352/497\n",
            "Training Step: 439  | total loss: \u001b[1m\u001b[32m109009.73438\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 014 | loss: 109009.73438 - acc: 0.1886 -- iter: 368/497\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m107379.91406\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 014 | loss: 107379.91406 - acc: 0.2010 -- iter: 384/497\n",
            "Training Step: 441  | total loss: \u001b[1m\u001b[32m103692.53906\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 014 | loss: 103692.53906 - acc: 0.2246 -- iter: 400/497\n",
            "Training Step: 442  | total loss: \u001b[1m\u001b[32m103731.93750\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 014 | loss: 103731.93750 - acc: 0.2147 -- iter: 416/497\n",
            "Training Step: 443  | total loss: \u001b[1m\u001b[32m104626.96094\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 014 | loss: 104626.96094 - acc: 0.1995 -- iter: 432/497\n",
            "Training Step: 444  | total loss: \u001b[1m\u001b[32m104802.31250\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 014 | loss: 104802.31250 - acc: 0.2108 -- iter: 448/497\n",
            "Training Step: 445  | total loss: \u001b[1m\u001b[32m102956.71094\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 014 | loss: 102956.71094 - acc: 0.2334 -- iter: 464/497\n",
            "Training Step: 446  | total loss: \u001b[1m\u001b[32m102973.84375\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 014 | loss: 102973.84375 - acc: 0.2288 -- iter: 480/497\n",
            "Training Step: 447  | total loss: \u001b[1m\u001b[32m102845.45312\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 014 | loss: 102845.45312 - acc: 0.2122 -- iter: 496/497\n",
            "Training Step: 448  | total loss: \u001b[1m\u001b[32m104678.86719\u001b[0m\u001b[0m | time: 1.134s\n",
            "| Adam | epoch: 014 | loss: 104678.86719 - acc: 0.2222 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 449  | total loss: \u001b[1m\u001b[32m105700.14844\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 015 | loss: 105700.14844 - acc: 0.2313 -- iter: 016/497\n",
            "Training Step: 450  | total loss: \u001b[1m\u001b[32m107873.92188\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 015 | loss: 107873.92188 - acc: 0.2331 -- iter: 032/497\n",
            "Training Step: 451  | total loss: \u001b[1m\u001b[32m108328.72656\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 015 | loss: 108328.72656 - acc: 0.2411 -- iter: 048/497\n",
            "Training Step: 452  | total loss: \u001b[1m\u001b[32m107182.74219\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 015 | loss: 107182.74219 - acc: 0.2357 -- iter: 064/497\n",
            "Training Step: 453  | total loss: \u001b[1m\u001b[32m104670.85156\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 015 | loss: 104670.85156 - acc: 0.2246 -- iter: 080/497\n",
            "Training Step: 454  | total loss: \u001b[1m\u001b[32m104146.00000\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 015 | loss: 104146.00000 - acc: 0.2459 -- iter: 096/497\n",
            "Training Step: 455  | total loss: \u001b[1m\u001b[32m104804.61719\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 015 | loss: 104804.61719 - acc: 0.2651 -- iter: 112/497\n",
            "Training Step: 456  | total loss: \u001b[1m\u001b[32m102948.89062\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 015 | loss: 102948.89062 - acc: 0.2636 -- iter: 128/497\n",
            "Training Step: 457  | total loss: \u001b[1m\u001b[32m103137.07812\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 015 | loss: 103137.07812 - acc: 0.2622 -- iter: 144/497\n",
            "Training Step: 458  | total loss: \u001b[1m\u001b[32m102039.95312\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 015 | loss: 102039.95312 - acc: 0.2547 -- iter: 160/497\n",
            "Training Step: 459  | total loss: \u001b[1m\u001b[32m104156.64062\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 015 | loss: 104156.64062 - acc: 0.2418 -- iter: 176/497\n",
            "Training Step: 460  | total loss: \u001b[1m\u001b[32m107080.50000\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 015 | loss: 107080.50000 - acc: 0.2488 -- iter: 192/497\n",
            "Training Step: 461  | total loss: \u001b[1m\u001b[32m106475.03906\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 015 | loss: 106475.03906 - acc: 0.2677 -- iter: 208/497\n",
            "Training Step: 462  | total loss: \u001b[1m\u001b[32m114331.34375\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 015 | loss: 114331.34375 - acc: 0.2409 -- iter: 224/497\n",
            "Training Step: 463  | total loss: \u001b[1m\u001b[32m121402.01562\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 015 | loss: 121402.01562 - acc: 0.2168 -- iter: 240/497\n",
            "Training Step: 464  | total loss: \u001b[1m\u001b[32m115357.66406\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 015 | loss: 115357.66406 - acc: 0.2264 -- iter: 256/497\n",
            "Training Step: 465  | total loss: \u001b[1m\u001b[32m111344.55469\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 015 | loss: 111344.55469 - acc: 0.2350 -- iter: 272/497\n",
            "Training Step: 466  | total loss: \u001b[1m\u001b[32m110599.71875\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 015 | loss: 110599.71875 - acc: 0.2303 -- iter: 288/497\n",
            "Training Step: 467  | total loss: \u001b[1m\u001b[32m111818.77344\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 015 | loss: 111818.77344 - acc: 0.2510 -- iter: 304/497\n",
            "Training Step: 468  | total loss: \u001b[1m\u001b[32m114203.05469\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 015 | loss: 114203.05469 - acc: 0.2571 -- iter: 320/497\n",
            "Training Step: 469  | total loss: \u001b[1m\u001b[32m111196.19531\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 015 | loss: 111196.19531 - acc: 0.2439 -- iter: 336/497\n",
            "Training Step: 470  | total loss: \u001b[1m\u001b[32m109245.74219\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 015 | loss: 109245.74219 - acc: 0.2445 -- iter: 352/497\n",
            "Training Step: 471  | total loss: \u001b[1m\u001b[32m109140.78125\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 015 | loss: 109140.78125 - acc: 0.2263 -- iter: 368/497\n",
            "Training Step: 472  | total loss: \u001b[1m\u001b[32m107652.64844\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 015 | loss: 107652.64844 - acc: 0.2162 -- iter: 384/497\n",
            "Training Step: 473  | total loss: \u001b[1m\u001b[32m107865.47656\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 015 | loss: 107865.47656 - acc: 0.2321 -- iter: 400/497\n",
            "Training Step: 474  | total loss: \u001b[1m\u001b[32m105995.17188\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 015 | loss: 105995.17188 - acc: 0.2214 -- iter: 416/497\n",
            "Training Step: 475  | total loss: \u001b[1m\u001b[32m103466.92188\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 015 | loss: 103466.92188 - acc: 0.2305 -- iter: 432/497\n",
            "Training Step: 476  | total loss: \u001b[1m\u001b[32m102019.15625\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 015 | loss: 102019.15625 - acc: 0.2324 -- iter: 448/497\n",
            "Training Step: 477  | total loss: \u001b[1m\u001b[32m100038.01562\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 015 | loss: 100038.01562 - acc: 0.2467 -- iter: 464/497\n",
            "Training Step: 478  | total loss: \u001b[1m\u001b[32m101192.03906\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 015 | loss: 101192.03906 - acc: 0.2533 -- iter: 480/497\n",
            "Training Step: 479  | total loss: \u001b[1m\u001b[32m99152.30469\u001b[0m\u001b[0m | time: 0.126s\n",
            "| Adam | epoch: 015 | loss: 99152.30469 - acc: 0.2404 -- iter: 496/497\n",
            "Training Step: 480  | total loss: \u001b[1m\u001b[32m100673.96875\u001b[0m\u001b[0m | time: 1.133s\n",
            "| Adam | epoch: 015 | loss: 100673.96875 - acc: 0.2477 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 481  | total loss: \u001b[1m\u001b[32m101700.15625\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 016 | loss: 101700.15625 - acc: 0.2416 -- iter: 016/497\n",
            "Training Step: 482  | total loss: \u001b[1m\u001b[32m102064.81250\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 016 | loss: 102064.81250 - acc: 0.2425 -- iter: 032/497\n",
            "Training Step: 483  | total loss: \u001b[1m\u001b[32m102147.49219\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 016 | loss: 102147.49219 - acc: 0.2307 -- iter: 048/497\n",
            "Training Step: 484  | total loss: \u001b[1m\u001b[32m104653.85156\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 016 | loss: 104653.85156 - acc: 0.2327 -- iter: 064/497\n",
            "Training Step: 485  | total loss: \u001b[1m\u001b[32m104844.15625\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 016 | loss: 104844.15625 - acc: 0.2281 -- iter: 080/497\n",
            "Training Step: 486  | total loss: \u001b[1m\u001b[32m106955.17969\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 016 | loss: 106955.17969 - acc: 0.2366 -- iter: 096/497\n",
            "Training Step: 487  | total loss: \u001b[1m\u001b[32m105418.21094\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 016 | loss: 105418.21094 - acc: 0.2379 -- iter: 112/497\n",
            "Training Step: 488  | total loss: \u001b[1m\u001b[32m108008.62500\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 016 | loss: 108008.62500 - acc: 0.2391 -- iter: 128/497\n",
            "Training Step: 489  | total loss: \u001b[1m\u001b[32m106474.25781\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 016 | loss: 106474.25781 - acc: 0.2402 -- iter: 144/497\n",
            "Training Step: 490  | total loss: \u001b[1m\u001b[32m110808.71875\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 016 | loss: 110808.71875 - acc: 0.2474 -- iter: 160/497\n",
            "Training Step: 491  | total loss: \u001b[1m\u001b[32m111197.72656\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 016 | loss: 111197.72656 - acc: 0.2352 -- iter: 176/497\n",
            "Training Step: 492  | total loss: \u001b[1m\u001b[32m111601.08594\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 016 | loss: 111601.08594 - acc: 0.2367 -- iter: 192/497\n",
            "Training Step: 493  | total loss: \u001b[1m\u001b[32m110329.74219\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 016 | loss: 110329.74219 - acc: 0.2443 -- iter: 208/497\n",
            "Training Step: 494  | total loss: \u001b[1m\u001b[32m109262.97656\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 016 | loss: 109262.97656 - acc: 0.2448 -- iter: 224/497\n",
            "Training Step: 495  | total loss: \u001b[1m\u001b[32m115995.89844\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 016 | loss: 115995.89844 - acc: 0.2204 -- iter: 240/497\n",
            "Training Step: 496  | total loss: \u001b[1m\u001b[32m122055.52344\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 016 | loss: 122055.52344 - acc: 0.1983 -- iter: 256/497\n",
            "Training Step: 497  | total loss: \u001b[1m\u001b[32m121512.33594\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 016 | loss: 121512.33594 - acc: 0.1847 -- iter: 272/497\n",
            "Training Step: 498  | total loss: \u001b[1m\u001b[32m121907.99219\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 016 | loss: 121907.99219 - acc: 0.2038 -- iter: 288/497\n",
            "Training Step: 499  | total loss: \u001b[1m\u001b[32m119800.75781\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 016 | loss: 119800.75781 - acc: 0.1959 -- iter: 304/497\n",
            "Training Step: 500  | total loss: \u001b[1m\u001b[32m118783.32812\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 016 | loss: 118783.32812 - acc: 0.2013 -- iter: 320/497\n",
            "Training Step: 501  | total loss: \u001b[1m\u001b[32m115764.14844\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 016 | loss: 115764.14844 - acc: 0.2062 -- iter: 336/497\n",
            "Training Step: 502  | total loss: \u001b[1m\u001b[32m111639.01562\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 016 | loss: 111639.01562 - acc: 0.2168 -- iter: 352/497\n",
            "Training Step: 503  | total loss: \u001b[1m\u001b[32m111041.87500\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 016 | loss: 111041.87500 - acc: 0.2139 -- iter: 368/497\n",
            "Training Step: 504  | total loss: \u001b[1m\u001b[32m109821.69531\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 016 | loss: 109821.69531 - acc: 0.2175 -- iter: 384/497\n",
            "Training Step: 505  | total loss: \u001b[1m\u001b[32m109835.37500\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 016 | loss: 109835.37500 - acc: 0.2020 -- iter: 400/497\n",
            "Training Step: 506  | total loss: \u001b[1m\u001b[32m111642.65625\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 016 | loss: 111642.65625 - acc: 0.2130 -- iter: 416/497\n",
            "Training Step: 507  | total loss: \u001b[1m\u001b[32m110416.10156\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 016 | loss: 110416.10156 - acc: 0.2230 -- iter: 432/497\n",
            "Training Step: 508  | total loss: \u001b[1m\u001b[32m110818.92969\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 016 | loss: 110818.92969 - acc: 0.2194 -- iter: 448/497\n",
            "Training Step: 509  | total loss: \u001b[1m\u001b[32m108435.65625\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 016 | loss: 108435.65625 - acc: 0.2475 -- iter: 464/497\n",
            "Training Step: 510  | total loss: \u001b[1m\u001b[32m107185.46875\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 016 | loss: 107185.46875 - acc: 0.2477 -- iter: 480/497\n",
            "Training Step: 511  | total loss: \u001b[1m\u001b[32m102844.27344\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 016 | loss: 102844.27344 - acc: 0.2542 -- iter: 496/497\n",
            "Training Step: 512  | total loss: \u001b[1m\u001b[32m100966.29688\u001b[0m\u001b[0m | time: 1.133s\n",
            "| Adam | epoch: 016 | loss: 100966.29688 - acc: 0.2413 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 513  | total loss: \u001b[1m\u001b[32m101329.52344\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 017 | loss: 101329.52344 - acc: 0.2547 -- iter: 016/497\n",
            "Training Step: 514  | total loss: \u001b[1m\u001b[32m101062.90625\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 017 | loss: 101062.90625 - acc: 0.2542 -- iter: 032/497\n",
            "Training Step: 515  | total loss: \u001b[1m\u001b[32m99819.53906\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 017 | loss: 99819.53906 - acc: 0.2413 -- iter: 048/497\n",
            "Training Step: 516  | total loss: \u001b[1m\u001b[32m97934.74219\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 017 | loss: 97934.74219 - acc: 0.2547 -- iter: 064/497\n",
            "Training Step: 517  | total loss: \u001b[1m\u001b[32m99315.77344\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 017 | loss: 99315.77344 - acc: 0.2542 -- iter: 080/497\n",
            "Training Step: 518  | total loss: \u001b[1m\u001b[32m102528.80469\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 017 | loss: 102528.80469 - acc: 0.2663 -- iter: 096/497\n",
            "Training Step: 519  | total loss: \u001b[1m\u001b[32m103915.69531\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 017 | loss: 103915.69531 - acc: 0.2709 -- iter: 112/497\n",
            "Training Step: 520  | total loss: \u001b[1m\u001b[32m106595.41406\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 017 | loss: 106595.41406 - acc: 0.2563 -- iter: 128/497\n",
            "Training Step: 521  | total loss: \u001b[1m\u001b[32m108705.73438\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 017 | loss: 108705.73438 - acc: 0.2557 -- iter: 144/497\n",
            "Training Step: 522  | total loss: \u001b[1m\u001b[32m104759.13281\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 017 | loss: 104759.13281 - acc: 0.2489 -- iter: 160/497\n",
            "Training Step: 523  | total loss: \u001b[1m\u001b[32m105113.92188\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 017 | loss: 105113.92188 - acc: 0.2365 -- iter: 176/497\n",
            "Training Step: 524  | total loss: \u001b[1m\u001b[32m107316.55469\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 017 | loss: 107316.55469 - acc: 0.2316 -- iter: 192/497\n",
            "Training Step: 525  | total loss: \u001b[1m\u001b[32m104028.75000\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 017 | loss: 104028.75000 - acc: 0.2397 -- iter: 208/497\n",
            "Training Step: 526  | total loss: \u001b[1m\u001b[32m106322.27344\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 017 | loss: 106322.27344 - acc: 0.2344 -- iter: 224/497\n",
            "Training Step: 527  | total loss: \u001b[1m\u001b[32m107436.96094\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 017 | loss: 107436.96094 - acc: 0.2423 -- iter: 240/497\n",
            "Training Step: 528  | total loss: \u001b[1m\u001b[32m98581.84375\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 017 | loss: 98581.84375 - acc: 0.2180 -- iter: 256/497\n",
            "Training Step: 529  | total loss: \u001b[1m\u001b[32m90612.23438\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 017 | loss: 90612.23438 - acc: 0.1962 -- iter: 272/497\n",
            "Training Step: 530  | total loss: \u001b[1m\u001b[32m93148.74219\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 017 | loss: 93148.74219 - acc: 0.2266 -- iter: 288/497\n",
            "Training Step: 531  | total loss: \u001b[1m\u001b[32m94638.90625\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 017 | loss: 94638.90625 - acc: 0.2352 -- iter: 304/497\n",
            "Training Step: 532  | total loss: \u001b[1m\u001b[32m95640.48438\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 017 | loss: 95640.48438 - acc: 0.2429 -- iter: 320/497\n",
            "Training Step: 533  | total loss: \u001b[1m\u001b[32m96295.52344\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 017 | loss: 96295.52344 - acc: 0.2311 -- iter: 336/497\n",
            "Training Step: 534  | total loss: \u001b[1m\u001b[32m98889.73438\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 017 | loss: 98889.73438 - acc: 0.2205 -- iter: 352/497\n",
            "Training Step: 535  | total loss: \u001b[1m\u001b[32m96831.23438\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 017 | loss: 96831.23438 - acc: 0.2110 -- iter: 368/497\n",
            "Training Step: 536  | total loss: \u001b[1m\u001b[32m94997.74219\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 017 | loss: 94997.74219 - acc: 0.2086 -- iter: 384/497\n",
            "Training Step: 537  | total loss: \u001b[1m\u001b[32m93905.94531\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 017 | loss: 93905.94531 - acc: 0.2128 -- iter: 400/497\n",
            "Training Step: 538  | total loss: \u001b[1m\u001b[32m96870.73438\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 017 | loss: 96870.73438 - acc: 0.2040 -- iter: 416/497\n",
            "Training Step: 539  | total loss: \u001b[1m\u001b[32m97733.71875\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 017 | loss: 97733.71875 - acc: 0.1961 -- iter: 432/497\n",
            "Training Step: 540  | total loss: \u001b[1m\u001b[32m97043.69531\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 017 | loss: 97043.69531 - acc: 0.2202 -- iter: 448/497\n",
            "Training Step: 541  | total loss: \u001b[1m\u001b[32m97324.10156\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 017 | loss: 97324.10156 - acc: 0.2107 -- iter: 464/497\n",
            "Training Step: 542  | total loss: \u001b[1m\u001b[32m96273.38281\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 017 | loss: 96273.38281 - acc: 0.2209 -- iter: 480/497\n",
            "Training Step: 543  | total loss: \u001b[1m\u001b[32m95684.58594\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 017 | loss: 95684.58594 - acc: 0.2238 -- iter: 496/497\n",
            "Training Step: 544  | total loss: \u001b[1m\u001b[32m99401.65625\u001b[0m\u001b[0m | time: 1.130s\n",
            "| Adam | epoch: 017 | loss: 99401.65625 - acc: 0.2389 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 545  | total loss: \u001b[1m\u001b[32m99223.42969\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 018 | loss: 99223.42969 - acc: 0.2275 -- iter: 016/497\n",
            "Training Step: 546  | total loss: \u001b[1m\u001b[32m99177.50781\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 018 | loss: 99177.50781 - acc: 0.2298 -- iter: 032/497\n",
            "Training Step: 547  | total loss: \u001b[1m\u001b[32m101451.03906\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 018 | loss: 101451.03906 - acc: 0.2443 -- iter: 048/497\n",
            "Training Step: 548  | total loss: \u001b[1m\u001b[32m101978.08594\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 018 | loss: 101978.08594 - acc: 0.2636 -- iter: 064/497\n",
            "Training Step: 549  | total loss: \u001b[1m\u001b[32m102035.67188\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 018 | loss: 102035.67188 - acc: 0.2560 -- iter: 080/497\n",
            "Training Step: 550  | total loss: \u001b[1m\u001b[32m101012.38281\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 018 | loss: 101012.38281 - acc: 0.2367 -- iter: 096/497\n",
            "Training Step: 551  | total loss: \u001b[1m\u001b[32m100479.16406\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 018 | loss: 100479.16406 - acc: 0.2380 -- iter: 112/497\n",
            "Training Step: 552  | total loss: \u001b[1m\u001b[32m100694.10156\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 018 | loss: 100694.10156 - acc: 0.2329 -- iter: 128/497\n",
            "Training Step: 553  | total loss: \u001b[1m\u001b[32m104541.17188\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 018 | loss: 104541.17188 - acc: 0.2221 -- iter: 144/497\n",
            "Training Step: 554  | total loss: \u001b[1m\u001b[32m105122.16406\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 018 | loss: 105122.16406 - acc: 0.2062 -- iter: 160/497\n",
            "Training Step: 555  | total loss: \u001b[1m\u001b[32m103784.32031\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 018 | loss: 103784.32031 - acc: 0.2168 -- iter: 176/497\n",
            "Training Step: 556  | total loss: \u001b[1m\u001b[32m103497.57031\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 018 | loss: 103497.57031 - acc: 0.2514 -- iter: 192/497\n",
            "Training Step: 557  | total loss: \u001b[1m\u001b[32m102736.72656\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 018 | loss: 102736.72656 - acc: 0.2575 -- iter: 208/497\n",
            "Training Step: 558  | total loss: \u001b[1m\u001b[32m103693.91406\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 018 | loss: 103693.91406 - acc: 0.2567 -- iter: 224/497\n",
            "Training Step: 559  | total loss: \u001b[1m\u001b[32m105066.31250\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 018 | loss: 105066.31250 - acc: 0.2561 -- iter: 240/497\n",
            "Training Step: 560  | total loss: \u001b[1m\u001b[32m104280.04688\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 018 | loss: 104280.04688 - acc: 0.2492 -- iter: 256/497\n",
            "Training Step: 561  | total loss: \u001b[1m\u001b[32m112653.34375\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 018 | loss: 112653.34375 - acc: 0.2243 -- iter: 272/497\n",
            "Training Step: 562  | total loss: \u001b[1m\u001b[32m120189.30469\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 018 | loss: 120189.30469 - acc: 0.2019 -- iter: 288/497\n",
            "Training Step: 563  | total loss: \u001b[1m\u001b[32m121839.14062\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 018 | loss: 121839.14062 - acc: 0.2129 -- iter: 304/497\n",
            "Training Step: 564  | total loss: \u001b[1m\u001b[32m123745.66406\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 018 | loss: 123745.66406 - acc: 0.2104 -- iter: 320/497\n",
            "Training Step: 565  | total loss: \u001b[1m\u001b[32m123174.42188\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 018 | loss: 123174.42188 - acc: 0.2143 -- iter: 336/497\n",
            "Training Step: 566  | total loss: \u001b[1m\u001b[32m119715.98438\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 018 | loss: 119715.98438 - acc: 0.2367 -- iter: 352/497\n",
            "Training Step: 567  | total loss: \u001b[1m\u001b[32m116372.89062\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 018 | loss: 116372.89062 - acc: 0.2317 -- iter: 368/497\n",
            "Training Step: 568  | total loss: \u001b[1m\u001b[32m112985.73438\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 018 | loss: 112985.73438 - acc: 0.2336 -- iter: 384/497\n",
            "Training Step: 569  | total loss: \u001b[1m\u001b[32m109117.24219\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 018 | loss: 109117.24219 - acc: 0.2352 -- iter: 400/497\n",
            "Training Step: 570  | total loss: \u001b[1m\u001b[32m111214.05469\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 018 | loss: 111214.05469 - acc: 0.2429 -- iter: 416/497\n",
            "Training Step: 571  | total loss: \u001b[1m\u001b[32m112598.46094\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 018 | loss: 112598.46094 - acc: 0.2374 -- iter: 432/497\n",
            "Training Step: 572  | total loss: \u001b[1m\u001b[32m116211.92969\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 018 | loss: 116211.92969 - acc: 0.2324 -- iter: 448/497\n",
            "Training Step: 573  | total loss: \u001b[1m\u001b[32m114769.89062\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 018 | loss: 114769.89062 - acc: 0.2342 -- iter: 464/497\n",
            "Training Step: 574  | total loss: \u001b[1m\u001b[32m112384.44531\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 018 | loss: 112384.44531 - acc: 0.2358 -- iter: 480/497\n",
            "Training Step: 575  | total loss: \u001b[1m\u001b[32m110876.86719\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 018 | loss: 110876.86719 - acc: 0.2372 -- iter: 496/497\n",
            "Training Step: 576  | total loss: \u001b[1m\u001b[32m112503.01562\u001b[0m\u001b[0m | time: 1.135s\n",
            "| Adam | epoch: 018 | loss: 112503.01562 - acc: 0.2322 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 577  | total loss: \u001b[1m\u001b[32m108229.70312\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 019 | loss: 108229.70312 - acc: 0.2152 -- iter: 016/497\n",
            "Training Step: 578  | total loss: \u001b[1m\u001b[32m108076.55469\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 019 | loss: 108076.55469 - acc: 0.2125 -- iter: 032/497\n",
            "Training Step: 579  | total loss: \u001b[1m\u001b[32m106720.90625\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 019 | loss: 106720.90625 - acc: 0.2037 -- iter: 048/497\n",
            "Training Step: 580  | total loss: \u001b[1m\u001b[32m107167.06250\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 019 | loss: 107167.06250 - acc: 0.2146 -- iter: 064/497\n",
            "Training Step: 581  | total loss: \u001b[1m\u001b[32m108541.15625\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 019 | loss: 108541.15625 - acc: 0.2369 -- iter: 080/497\n",
            "Training Step: 582  | total loss: \u001b[1m\u001b[32m107785.71875\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 019 | loss: 107785.71875 - acc: 0.2444 -- iter: 096/497\n",
            "Training Step: 583  | total loss: \u001b[1m\u001b[32m103909.47656\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 019 | loss: 103909.47656 - acc: 0.2388 -- iter: 112/497\n",
            "Training Step: 584  | total loss: \u001b[1m\u001b[32m100823.04688\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 019 | loss: 100823.04688 - acc: 0.2461 -- iter: 128/497\n",
            "Training Step: 585  | total loss: \u001b[1m\u001b[32m98538.32031\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 019 | loss: 98538.32031 - acc: 0.2528 -- iter: 144/497\n",
            "Training Step: 586  | total loss: \u001b[1m\u001b[32m98974.03906\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 019 | loss: 98974.03906 - acc: 0.2587 -- iter: 160/497\n",
            "Training Step: 587  | total loss: \u001b[1m\u001b[32m98183.08594\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 019 | loss: 98183.08594 - acc: 0.2516 -- iter: 176/497\n",
            "Training Step: 588  | total loss: \u001b[1m\u001b[32m97570.05469\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 019 | loss: 97570.05469 - acc: 0.2452 -- iter: 192/497\n",
            "Training Step: 589  | total loss: \u001b[1m\u001b[32m100362.77344\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 019 | loss: 100362.77344 - acc: 0.2394 -- iter: 208/497\n",
            "Training Step: 590  | total loss: \u001b[1m\u001b[32m104160.80469\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 019 | loss: 104160.80469 - acc: 0.2405 -- iter: 224/497\n",
            "Training Step: 591  | total loss: \u001b[1m\u001b[32m100878.16406\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 019 | loss: 100878.16406 - acc: 0.2227 -- iter: 240/497\n",
            "Training Step: 592  | total loss: \u001b[1m\u001b[32m101498.28906\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 019 | loss: 101498.28906 - acc: 0.2442 -- iter: 256/497\n",
            "Training Step: 593  | total loss: \u001b[1m\u001b[32m103105.03906\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 019 | loss: 103105.03906 - acc: 0.2385 -- iter: 272/497\n",
            "Training Step: 594  | total loss: \u001b[1m\u001b[32m92697.13281\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 019 | loss: 92697.13281 - acc: 0.3147 -- iter: 288/497\n",
            "Training Step: 595  | total loss: \u001b[1m\u001b[32m83330.01562\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 019 | loss: 83330.01562 - acc: 0.3832 -- iter: 304/497\n",
            "Training Step: 596  | total loss: \u001b[1m\u001b[32m83031.02344\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 019 | loss: 83031.02344 - acc: 0.3636 -- iter: 320/497\n",
            "Training Step: 597  | total loss: \u001b[1m\u001b[32m81224.99219\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 019 | loss: 81224.99219 - acc: 0.3460 -- iter: 336/497\n",
            "Training Step: 598  | total loss: \u001b[1m\u001b[32m83862.60156\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 019 | loss: 83862.60156 - acc: 0.3552 -- iter: 352/497\n",
            "Training Step: 599  | total loss: \u001b[1m\u001b[32m87577.37500\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 019 | loss: 87577.37500 - acc: 0.3509 -- iter: 368/497\n",
            "Training Step: 600  | total loss: \u001b[1m\u001b[32m87202.07031\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 019 | loss: 87202.07031 - acc: 0.3471 -- iter: 384/497\n",
            "Training Step: 601  | total loss: \u001b[1m\u001b[32m89159.00781\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 019 | loss: 89159.00781 - acc: 0.3373 -- iter: 400/497\n",
            "Training Step: 602  | total loss: \u001b[1m\u001b[32m93638.96875\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 019 | loss: 93638.96875 - acc: 0.3161 -- iter: 416/497\n",
            "Training Step: 603  | total loss: \u001b[1m\u001b[32m94486.71875\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 019 | loss: 94486.71875 - acc: 0.3220 -- iter: 432/497\n",
            "Training Step: 604  | total loss: \u001b[1m\u001b[32m98881.49219\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 019 | loss: 98881.49219 - acc: 0.3211 -- iter: 448/497\n",
            "Training Step: 605  | total loss: \u001b[1m\u001b[32m101961.73438\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 019 | loss: 101961.73438 - acc: 0.3077 -- iter: 464/497\n",
            "Training Step: 606  | total loss: \u001b[1m\u001b[32m100305.80469\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 019 | loss: 100305.80469 - acc: 0.2832 -- iter: 480/497\n",
            "Training Step: 607  | total loss: \u001b[1m\u001b[32m100459.03906\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 019 | loss: 100459.03906 - acc: 0.2736 -- iter: 496/497\n",
            "Training Step: 608  | total loss: \u001b[1m\u001b[32m98536.10938\u001b[0m\u001b[0m | time: 1.121s\n",
            "| Adam | epoch: 019 | loss: 98536.10938 - acc: 0.2712 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 609  | total loss: \u001b[1m\u001b[32m102296.73438\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 020 | loss: 102296.73438 - acc: 0.2566 -- iter: 016/497\n",
            "Training Step: 610  | total loss: \u001b[1m\u001b[32m104424.03906\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 020 | loss: 104424.03906 - acc: 0.2685 -- iter: 032/497\n",
            "Training Step: 611  | total loss: \u001b[1m\u001b[32m104414.57812\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 020 | loss: 104414.57812 - acc: 0.2479 -- iter: 048/497\n",
            "Training Step: 612  | total loss: \u001b[1m\u001b[32m104148.03906\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 020 | loss: 104148.03906 - acc: 0.2543 -- iter: 064/497\n",
            "Training Step: 613  | total loss: \u001b[1m\u001b[32m102560.72656\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 020 | loss: 102560.72656 - acc: 0.2601 -- iter: 080/497\n",
            "Training Step: 614  | total loss: \u001b[1m\u001b[32m101248.48438\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 020 | loss: 101248.48438 - acc: 0.2654 -- iter: 096/497\n",
            "Training Step: 615  | total loss: \u001b[1m\u001b[32m101367.47656\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 020 | loss: 101367.47656 - acc: 0.2701 -- iter: 112/497\n",
            "Training Step: 616  | total loss: \u001b[1m\u001b[32m103040.07031\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 020 | loss: 103040.07031 - acc: 0.2806 -- iter: 128/497\n",
            "Training Step: 617  | total loss: \u001b[1m\u001b[32m102033.42969\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 020 | loss: 102033.42969 - acc: 0.2963 -- iter: 144/497\n",
            "Training Step: 618  | total loss: \u001b[1m\u001b[32m100199.55469\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 020 | loss: 100199.55469 - acc: 0.2791 -- iter: 160/497\n",
            "Training Step: 619  | total loss: \u001b[1m\u001b[32m99330.37500\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 020 | loss: 99330.37500 - acc: 0.2887 -- iter: 176/497\n",
            "Training Step: 620  | total loss: \u001b[1m\u001b[32m99507.77344\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 020 | loss: 99507.77344 - acc: 0.2849 -- iter: 192/497\n",
            "Training Step: 621  | total loss: \u001b[1m\u001b[32m98102.53906\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 020 | loss: 98102.53906 - acc: 0.2876 -- iter: 208/497\n",
            "Training Step: 622  | total loss: \u001b[1m\u001b[32m101375.03125\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 020 | loss: 101375.03125 - acc: 0.2776 -- iter: 224/497\n",
            "Training Step: 623  | total loss: \u001b[1m\u001b[32m101654.67969\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 020 | loss: 101654.67969 - acc: 0.2561 -- iter: 240/497\n",
            "Training Step: 624  | total loss: \u001b[1m\u001b[32m101825.37500\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 020 | loss: 101825.37500 - acc: 0.2430 -- iter: 256/497\n",
            "Training Step: 625  | total loss: \u001b[1m\u001b[32m99925.69531\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 020 | loss: 99925.69531 - acc: 0.2374 -- iter: 272/497\n",
            "Training Step: 626  | total loss: \u001b[1m\u001b[32m102893.62500\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 020 | loss: 102893.62500 - acc: 0.2324 -- iter: 288/497\n",
            "Training Step: 627  | total loss: \u001b[1m\u001b[32m110951.71875\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 020 | loss: 110951.71875 - acc: 0.2092 -- iter: 304/497\n",
            "Training Step: 628  | total loss: \u001b[1m\u001b[32m118204.00781\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 020 | loss: 118204.00781 - acc: 0.1883 -- iter: 320/497\n",
            "Training Step: 629  | total loss: \u001b[1m\u001b[32m119971.49219\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 020 | loss: 119971.49219 - acc: 0.1945 -- iter: 336/497\n",
            "Training Step: 630  | total loss: \u001b[1m\u001b[32m120182.32031\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 020 | loss: 120182.32031 - acc: 0.2000 -- iter: 352/497\n",
            "Training Step: 631  | total loss: \u001b[1m\u001b[32m114803.17188\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 020 | loss: 114803.17188 - acc: 0.1988 -- iter: 368/497\n",
            "Training Step: 632  | total loss: \u001b[1m\u001b[32m115390.07031\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 020 | loss: 115390.07031 - acc: 0.2039 -- iter: 384/497\n",
            "Training Step: 633  | total loss: \u001b[1m\u001b[32m116668.30469\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 020 | loss: 116668.30469 - acc: 0.2147 -- iter: 400/497\n",
            "Training Step: 634  | total loss: \u001b[1m\u001b[32m116958.07812\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 020 | loss: 116958.07812 - acc: 0.2183 -- iter: 416/497\n",
            "Training Step: 635  | total loss: \u001b[1m\u001b[32m114378.80469\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 020 | loss: 114378.80469 - acc: 0.2152 -- iter: 432/497\n",
            "Training Step: 636  | total loss: \u001b[1m\u001b[32m110744.98438\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 020 | loss: 110744.98438 - acc: 0.2124 -- iter: 448/497\n",
            "Training Step: 637  | total loss: \u001b[1m\u001b[32m108779.57031\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 020 | loss: 108779.57031 - acc: 0.2537 -- iter: 464/497\n",
            "Training Step: 638  | total loss: \u001b[1m\u001b[32m110830.92969\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 020 | loss: 110830.92969 - acc: 0.2596 -- iter: 480/497\n",
            "Training Step: 639  | total loss: \u001b[1m\u001b[32m107186.57031\u001b[0m\u001b[0m | time: 0.126s\n",
            "| Adam | epoch: 020 | loss: 107186.57031 - acc: 0.2586 -- iter: 496/497\n",
            "Training Step: 640  | total loss: \u001b[1m\u001b[32m106185.52344\u001b[0m\u001b[0m | time: 1.133s\n",
            "| Adam | epoch: 020 | loss: 106185.52344 - acc: 0.2577 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 641  | total loss: \u001b[1m\u001b[32m104428.79688\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 021 | loss: 104428.79688 - acc: 0.2382 -- iter: 016/497\n",
            "Training Step: 642  | total loss: \u001b[1m\u001b[32m103491.80469\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 021 | loss: 103491.80469 - acc: 0.2269 -- iter: 032/497\n",
            "Training Step: 643  | total loss: \u001b[1m\u001b[32m102992.17969\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 021 | loss: 102992.17969 - acc: 0.2480 -- iter: 048/497\n",
            "Training Step: 644  | total loss: \u001b[1m\u001b[32m104829.45312\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 021 | loss: 104829.45312 - acc: 0.2482 -- iter: 064/497\n",
            "Training Step: 645  | total loss: \u001b[1m\u001b[32m107325.30469\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 021 | loss: 107325.30469 - acc: 0.2546 -- iter: 080/497\n",
            "Training Step: 646  | total loss: \u001b[1m\u001b[32m105610.92188\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 021 | loss: 105610.92188 - acc: 0.2541 -- iter: 096/497\n",
            "Training Step: 647  | total loss: \u001b[1m\u001b[32m101936.05469\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 021 | loss: 101936.05469 - acc: 0.2475 -- iter: 112/497\n",
            "Training Step: 648  | total loss: \u001b[1m\u001b[32m103022.52344\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 021 | loss: 103022.52344 - acc: 0.2540 -- iter: 128/497\n",
            "Training Step: 649  | total loss: \u001b[1m\u001b[32m102789.00000\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 021 | loss: 102789.00000 - acc: 0.2348 -- iter: 144/497\n",
            "Training Step: 650  | total loss: \u001b[1m\u001b[32m105027.26562\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 021 | loss: 105027.26562 - acc: 0.2363 -- iter: 160/497\n",
            "Training Step: 651  | total loss: \u001b[1m\u001b[32m105742.75000\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 021 | loss: 105742.75000 - acc: 0.2502 -- iter: 176/497\n",
            "Training Step: 652  | total loss: \u001b[1m\u001b[32m107606.35156\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 021 | loss: 107606.35156 - acc: 0.2502 -- iter: 192/497\n",
            "Training Step: 653  | total loss: \u001b[1m\u001b[32m107272.79688\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 021 | loss: 107272.79688 - acc: 0.2377 -- iter: 208/497\n",
            "Training Step: 654  | total loss: \u001b[1m\u001b[32m106218.70312\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 021 | loss: 106218.70312 - acc: 0.2514 -- iter: 224/497\n",
            "Training Step: 655  | total loss: \u001b[1m\u001b[32m106704.21875\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 021 | loss: 106704.21875 - acc: 0.2388 -- iter: 240/497\n",
            "Training Step: 656  | total loss: \u001b[1m\u001b[32m102988.93750\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 021 | loss: 102988.93750 - acc: 0.2336 -- iter: 256/497\n",
            "Training Step: 657  | total loss: \u001b[1m\u001b[32m104197.96094\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 021 | loss: 104197.96094 - acc: 0.2290 -- iter: 272/497\n",
            "Training Step: 658  | total loss: \u001b[1m\u001b[32m105133.35156\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 021 | loss: 105133.35156 - acc: 0.2249 -- iter: 288/497\n",
            "Training Step: 659  | total loss: \u001b[1m\u001b[32m102381.98438\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 021 | loss: 102381.98438 - acc: 0.2274 -- iter: 304/497\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m106275.90625\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 021 | loss: 106275.90625 - acc: 0.2046 -- iter: 320/497\n",
            "Training Step: 661  | total loss: \u001b[1m\u001b[32m109780.42969\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 021 | loss: 109780.42969 - acc: 0.1842 -- iter: 336/497\n",
            "Training Step: 662  | total loss: \u001b[1m\u001b[32m109048.80469\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 021 | loss: 109048.80469 - acc: 0.1970 -- iter: 352/497\n",
            "Training Step: 663  | total loss: \u001b[1m\u001b[32m109606.91406\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 021 | loss: 109606.91406 - acc: 0.1836 -- iter: 368/497\n",
            "Training Step: 664  | total loss: \u001b[1m\u001b[32m107911.09375\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 021 | loss: 107911.09375 - acc: 0.1965 -- iter: 384/497\n",
            "Training Step: 665  | total loss: \u001b[1m\u001b[32m109509.24219\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 021 | loss: 109509.24219 - acc: 0.2081 -- iter: 400/497\n",
            "Training Step: 666  | total loss: \u001b[1m\u001b[32m108790.61719\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 021 | loss: 108790.61719 - acc: 0.2185 -- iter: 416/497\n",
            "Training Step: 667  | total loss: \u001b[1m\u001b[32m108182.14844\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 021 | loss: 108182.14844 - acc: 0.2217 -- iter: 432/497\n",
            "Training Step: 668  | total loss: \u001b[1m\u001b[32m107485.73438\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 021 | loss: 107485.73438 - acc: 0.2245 -- iter: 448/497\n",
            "Training Step: 669  | total loss: \u001b[1m\u001b[32m106026.13281\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 021 | loss: 106026.13281 - acc: 0.2270 -- iter: 464/497\n",
            "Training Step: 670  | total loss: \u001b[1m\u001b[32m106525.46094\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 021 | loss: 106525.46094 - acc: 0.2231 -- iter: 480/497\n",
            "Training Step: 671  | total loss: \u001b[1m\u001b[32m104742.24219\u001b[0m\u001b[0m | time: 0.127s\n",
            "| Adam | epoch: 021 | loss: 104742.24219 - acc: 0.2258 -- iter: 496/497\n",
            "Training Step: 672  | total loss: \u001b[1m\u001b[32m108784.33594\u001b[0m\u001b[0m | time: 1.135s\n",
            "| Adam | epoch: 021 | loss: 108784.33594 - acc: 0.2157 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 673  | total loss: \u001b[1m\u001b[32m108618.30469\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 022 | loss: 108618.30469 - acc: 0.2316 -- iter: 016/497\n",
            "Training Step: 674  | total loss: \u001b[1m\u001b[32m105321.39844\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 022 | loss: 105321.39844 - acc: 0.2335 -- iter: 032/497\n",
            "Training Step: 675  | total loss: \u001b[1m\u001b[32m104533.65625\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 022 | loss: 104533.65625 - acc: 0.2226 -- iter: 048/497\n",
            "Training Step: 676  | total loss: \u001b[1m\u001b[32m105841.88281\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 022 | loss: 105841.88281 - acc: 0.2254 -- iter: 064/497\n",
            "Training Step: 677  | total loss: \u001b[1m\u001b[32m109131.34375\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 022 | loss: 109131.34375 - acc: 0.2278 -- iter: 080/497\n",
            "Training Step: 678  | total loss: \u001b[1m\u001b[32m109261.83594\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 022 | loss: 109261.83594 - acc: 0.2425 -- iter: 096/497\n",
            "Training Step: 679  | total loss: \u001b[1m\u001b[32m103225.39062\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 022 | loss: 103225.39062 - acc: 0.2495 -- iter: 112/497\n",
            "Training Step: 680  | total loss: \u001b[1m\u001b[32m104820.27344\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 022 | loss: 104820.27344 - acc: 0.2558 -- iter: 128/497\n",
            "Training Step: 681  | total loss: \u001b[1m\u001b[32m105690.76562\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 022 | loss: 105690.76562 - acc: 0.2740 -- iter: 144/497\n",
            "Training Step: 682  | total loss: \u001b[1m\u001b[32m104806.63281\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 022 | loss: 104806.63281 - acc: 0.2778 -- iter: 160/497\n",
            "Training Step: 683  | total loss: \u001b[1m\u001b[32m103289.92188\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 022 | loss: 103289.92188 - acc: 0.2626 -- iter: 176/497\n",
            "Training Step: 684  | total loss: \u001b[1m\u001b[32m104649.87500\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 022 | loss: 104649.87500 - acc: 0.2551 -- iter: 192/497\n",
            "Training Step: 685  | total loss: \u001b[1m\u001b[32m100061.46875\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 022 | loss: 100061.46875 - acc: 0.2358 -- iter: 208/497\n",
            "Training Step: 686  | total loss: \u001b[1m\u001b[32m99495.64844\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 022 | loss: 99495.64844 - acc: 0.2247 -- iter: 224/497\n",
            "Training Step: 687  | total loss: \u001b[1m\u001b[32m103263.34375\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 022 | loss: 103263.34375 - acc: 0.2335 -- iter: 240/497\n",
            "Training Step: 688  | total loss: \u001b[1m\u001b[32m104718.07031\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 022 | loss: 104718.07031 - acc: 0.2414 -- iter: 256/497\n",
            "Training Step: 689  | total loss: \u001b[1m\u001b[32m106084.34375\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 022 | loss: 106084.34375 - acc: 0.2360 -- iter: 272/497\n",
            "Training Step: 690  | total loss: \u001b[1m\u001b[32m101329.82812\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 022 | loss: 101329.82812 - acc: 0.2374 -- iter: 288/497\n",
            "Training Step: 691  | total loss: \u001b[1m\u001b[32m102596.35156\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 022 | loss: 102596.35156 - acc: 0.2449 -- iter: 304/497\n",
            "Training Step: 692  | total loss: \u001b[1m\u001b[32m102113.23438\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 022 | loss: 102113.23438 - acc: 0.2329 -- iter: 320/497\n",
            "Training Step: 693  | total loss: \u001b[1m\u001b[32m110706.89062\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 022 | loss: 110706.89062 - acc: 0.2096 -- iter: 336/497\n",
            "Training Step: 694  | total loss: \u001b[1m\u001b[32m118441.18750\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 022 | loss: 118441.18750 - acc: 0.1887 -- iter: 352/497\n",
            "Training Step: 695  | total loss: \u001b[1m\u001b[32m114883.57812\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 022 | loss: 114883.57812 - acc: 0.1886 -- iter: 368/497\n",
            "Training Step: 696  | total loss: \u001b[1m\u001b[32m114164.65625\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 022 | loss: 114164.65625 - acc: 0.1822 -- iter: 384/497\n",
            "Training Step: 697  | total loss: \u001b[1m\u001b[32m116087.24219\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 022 | loss: 116087.24219 - acc: 0.1765 -- iter: 400/497\n",
            "Training Step: 698  | total loss: \u001b[1m\u001b[32m115151.47656\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 022 | loss: 115151.47656 - acc: 0.1713 -- iter: 416/497\n",
            "Training Step: 699  | total loss: \u001b[1m\u001b[32m114388.85156\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 022 | loss: 114388.85156 - acc: 0.1729 -- iter: 432/497\n",
            "Training Step: 700  | total loss: \u001b[1m\u001b[32m112988.67188\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 022 | loss: 112988.67188 - acc: 0.1619 -- iter: 448/497\n",
            "Training Step: 701  | total loss: \u001b[1m\u001b[32m112066.78125\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 022 | loss: 112066.78125 - acc: 0.1707 -- iter: 464/497\n",
            "Training Step: 702  | total loss: \u001b[1m\u001b[32m113052.60938\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 022 | loss: 113052.60938 - acc: 0.1849 -- iter: 480/497\n",
            "Training Step: 703  | total loss: \u001b[1m\u001b[32m111605.63281\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 022 | loss: 111605.63281 - acc: 0.1914 -- iter: 496/497\n",
            "Training Step: 704  | total loss: \u001b[1m\u001b[32m112950.21094\u001b[0m\u001b[0m | time: 1.130s\n",
            "| Adam | epoch: 022 | loss: 112950.21094 - acc: 0.2098 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 705  | total loss: \u001b[1m\u001b[32m109831.23438\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 023 | loss: 109831.23438 - acc: 0.2138 -- iter: 016/497\n",
            "Training Step: 706  | total loss: \u001b[1m\u001b[32m108555.26562\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 023 | loss: 108555.26562 - acc: 0.2299 -- iter: 032/497\n",
            "Training Step: 707  | total loss: \u001b[1m\u001b[32m106583.03906\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 023 | loss: 106583.03906 - acc: 0.2132 -- iter: 048/497\n",
            "Training Step: 708  | total loss: \u001b[1m\u001b[32m104572.21094\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 023 | loss: 104572.21094 - acc: 0.2231 -- iter: 064/497\n",
            "Training Step: 709  | total loss: \u001b[1m\u001b[32m107249.53906\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 023 | loss: 107249.53906 - acc: 0.2320 -- iter: 080/497\n",
            "Training Step: 710  | total loss: \u001b[1m\u001b[32m107448.46875\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 023 | loss: 107448.46875 - acc: 0.2401 -- iter: 096/497\n",
            "Training Step: 711  | total loss: \u001b[1m\u001b[32m109140.61719\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 023 | loss: 109140.61719 - acc: 0.2411 -- iter: 112/497\n",
            "Training Step: 712  | total loss: \u001b[1m\u001b[32m107123.92969\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 023 | loss: 107123.92969 - acc: 0.2545 -- iter: 128/497\n",
            "Training Step: 713  | total loss: \u001b[1m\u001b[32m107148.47656\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 023 | loss: 107148.47656 - acc: 0.2665 -- iter: 144/497\n",
            "Training Step: 714  | total loss: \u001b[1m\u001b[32m105284.69531\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 023 | loss: 105284.69531 - acc: 0.2586 -- iter: 160/497\n",
            "Training Step: 715  | total loss: \u001b[1m\u001b[32m109076.23438\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 023 | loss: 109076.23438 - acc: 0.2703 -- iter: 176/497\n",
            "Training Step: 716  | total loss: \u001b[1m\u001b[32m107774.69531\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 023 | loss: 107774.69531 - acc: 0.2495 -- iter: 192/497\n",
            "Training Step: 717  | total loss: \u001b[1m\u001b[32m104094.12500\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 023 | loss: 104094.12500 - acc: 0.2495 -- iter: 208/497\n",
            "Training Step: 718  | total loss: \u001b[1m\u001b[32m101919.14844\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 023 | loss: 101919.14844 - acc: 0.2621 -- iter: 224/497\n",
            "Training Step: 719  | total loss: \u001b[1m\u001b[32m101038.31250\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 023 | loss: 101038.31250 - acc: 0.2609 -- iter: 240/497\n",
            "Training Step: 720  | total loss: \u001b[1m\u001b[32m98961.40625\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 023 | loss: 98961.40625 - acc: 0.2535 -- iter: 256/497\n",
            "Training Step: 721  | total loss: \u001b[1m\u001b[32m101103.50781\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 023 | loss: 101103.50781 - acc: 0.2594 -- iter: 272/497\n",
            "Training Step: 722  | total loss: \u001b[1m\u001b[32m101808.10938\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 023 | loss: 101808.10938 - acc: 0.2585 -- iter: 288/497\n",
            "Training Step: 723  | total loss: \u001b[1m\u001b[32m101774.03125\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 023 | loss: 101774.03125 - acc: 0.2639 -- iter: 304/497\n",
            "Training Step: 724  | total loss: \u001b[1m\u001b[32m99881.89062\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 023 | loss: 99881.89062 - acc: 0.2625 -- iter: 320/497\n",
            "Training Step: 725  | total loss: \u001b[1m\u001b[32m104655.40625\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 023 | loss: 104655.40625 - acc: 0.2550 -- iter: 336/497\n",
            "Training Step: 726  | total loss: \u001b[1m\u001b[32m94066.44531\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 023 | loss: 94066.44531 - acc: 0.2295 -- iter: 352/497\n",
            "Training Step: 727  | total loss: \u001b[1m\u001b[32m84536.38281\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 023 | loss: 84536.38281 - acc: 0.2066 -- iter: 368/497\n",
            "Training Step: 728  | total loss: \u001b[1m\u001b[32m84042.75000\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 023 | loss: 84042.75000 - acc: 0.1984 -- iter: 384/497\n",
            "Training Step: 729  | total loss: \u001b[1m\u001b[32m86484.87500\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 023 | loss: 86484.87500 - acc: 0.2036 -- iter: 400/497\n",
            "Training Step: 730  | total loss: \u001b[1m\u001b[32m91834.67969\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 023 | loss: 91834.67969 - acc: 0.1895 -- iter: 416/497\n",
            "Training Step: 731  | total loss: \u001b[1m\u001b[32m95503.23438\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 023 | loss: 95503.23438 - acc: 0.1955 -- iter: 432/497\n",
            "Training Step: 732  | total loss: \u001b[1m\u001b[32m98966.60156\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 023 | loss: 98966.60156 - acc: 0.2135 -- iter: 448/497\n",
            "Training Step: 733  | total loss: \u001b[1m\u001b[32m97669.42188\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 023 | loss: 97669.42188 - acc: 0.2109 -- iter: 464/497\n",
            "Training Step: 734  | total loss: \u001b[1m\u001b[32m97360.80469\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 023 | loss: 97360.80469 - acc: 0.2085 -- iter: 480/497\n",
            "Training Step: 735  | total loss: \u001b[1m\u001b[32m98269.73438\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 023 | loss: 98269.73438 - acc: 0.2189 -- iter: 496/497\n",
            "Training Step: 736  | total loss: \u001b[1m\u001b[32m99742.67969\u001b[0m\u001b[0m | time: 1.127s\n",
            "| Adam | epoch: 023 | loss: 99742.67969 - acc: 0.2220 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 737  | total loss: \u001b[1m\u001b[32m99782.15625\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 024 | loss: 99782.15625 - acc: 0.2186 -- iter: 016/497\n",
            "Training Step: 738  | total loss: \u001b[1m\u001b[32m101066.89062\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 024 | loss: 101066.89062 - acc: 0.2280 -- iter: 032/497\n",
            "Training Step: 739  | total loss: \u001b[1m\u001b[32m103177.30469\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 024 | loss: 103177.30469 - acc: 0.2427 -- iter: 048/497\n",
            "Training Step: 740  | total loss: \u001b[1m\u001b[32m103177.30469\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 024 | loss: 103177.30469 - acc: 0.2434 -- iter: 064/497\n",
            "Training Step: 741  | total loss: \u001b[1m\u001b[32m103268.09375\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 024 | loss: 103268.09375 - acc: 0.2316 -- iter: 080/497\n",
            "Training Step: 742  | total loss: \u001b[1m\u001b[32m101468.69531\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 024 | loss: 101468.69531 - acc: 0.2334 -- iter: 096/497\n",
            "Training Step: 743  | total loss: \u001b[1m\u001b[32m98406.25781\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 024 | loss: 98406.25781 - acc: 0.2413 -- iter: 112/497\n",
            "Training Step: 744  | total loss: \u001b[1m\u001b[32m97692.27344\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 024 | loss: 97692.27344 - acc: 0.2484 -- iter: 128/497\n",
            "Training Step: 745  | total loss: \u001b[1m\u001b[32m96273.55469\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 024 | loss: 96273.55469 - acc: 0.2361 -- iter: 144/497\n",
            "Training Step: 746  | total loss: \u001b[1m\u001b[32m100604.80469\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 024 | loss: 100604.80469 - acc: 0.2250 -- iter: 160/497\n",
            "Training Step: 747  | total loss: \u001b[1m\u001b[32m100676.99219\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 024 | loss: 100676.99219 - acc: 0.2337 -- iter: 176/497\n",
            "Training Step: 748  | total loss: \u001b[1m\u001b[32m97015.70312\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 024 | loss: 97015.70312 - acc: 0.2229 -- iter: 192/497\n",
            "Training Step: 749  | total loss: \u001b[1m\u001b[32m98091.58594\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 024 | loss: 98091.58594 - acc: 0.2381 -- iter: 208/497\n",
            "Training Step: 750  | total loss: \u001b[1m\u001b[32m100851.88281\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 024 | loss: 100851.88281 - acc: 0.2580 -- iter: 224/497\n",
            "Training Step: 751  | total loss: \u001b[1m\u001b[32m98226.14844\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 024 | loss: 98226.14844 - acc: 0.2635 -- iter: 240/497\n",
            "Training Step: 752  | total loss: \u001b[1m\u001b[32m98786.10938\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 024 | loss: 98786.10938 - acc: 0.2496 -- iter: 256/497\n",
            "Training Step: 753  | total loss: \u001b[1m\u001b[32m99889.78906\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 024 | loss: 99889.78906 - acc: 0.2684 -- iter: 272/497\n",
            "Training Step: 754  | total loss: \u001b[1m\u001b[32m101063.75781\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 024 | loss: 101063.75781 - acc: 0.2791 -- iter: 288/497\n",
            "Training Step: 755  | total loss: \u001b[1m\u001b[32m102549.73438\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 024 | loss: 102549.73438 - acc: 0.2574 -- iter: 304/497\n",
            "Training Step: 756  | total loss: \u001b[1m\u001b[32m102899.66406\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 024 | loss: 102899.66406 - acc: 0.2504 -- iter: 320/497\n",
            "Training Step: 757  | total loss: \u001b[1m\u001b[32m103420.89844\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 024 | loss: 103420.89844 - acc: 0.2504 -- iter: 336/497\n",
            "Training Step: 758  | total loss: \u001b[1m\u001b[32m102069.65625\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 024 | loss: 102069.65625 - acc: 0.2503 -- iter: 352/497\n",
            "Training Step: 759  | total loss: \u001b[1m\u001b[32m92444.32031\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 024 | loss: 92444.32031 - acc: 0.2253 -- iter: 368/497\n",
            "Training Step: 760  | total loss: \u001b[1m\u001b[32m83781.51562\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 024 | loss: 83781.51562 - acc: 0.2028 -- iter: 384/497\n",
            "Training Step: 761  | total loss: \u001b[1m\u001b[32m85404.82812\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 024 | loss: 85404.82812 - acc: 0.2200 -- iter: 400/497\n",
            "Training Step: 762  | total loss: \u001b[1m\u001b[32m86336.10156\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 024 | loss: 86336.10156 - acc: 0.2105 -- iter: 416/497\n",
            "Training Step: 763  | total loss: \u001b[1m\u001b[32m87106.36719\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 024 | loss: 87106.36719 - acc: 0.2269 -- iter: 432/497\n",
            "Training Step: 764  | total loss: \u001b[1m\u001b[32m86228.69531\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 024 | loss: 86228.69531 - acc: 0.2168 -- iter: 448/497\n",
            "Training Step: 765  | total loss: \u001b[1m\u001b[32m90400.39844\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 024 | loss: 90400.39844 - acc: 0.2201 -- iter: 464/497\n",
            "Training Step: 766  | total loss: \u001b[1m\u001b[32m90404.48438\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 024 | loss: 90404.48438 - acc: 0.2106 -- iter: 480/497\n",
            "Training Step: 767  | total loss: \u001b[1m\u001b[32m94316.40625\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 024 | loss: 94316.40625 - acc: 0.2208 -- iter: 496/497\n",
            "Training Step: 768  | total loss: \u001b[1m\u001b[32m93798.59375\u001b[0m\u001b[0m | time: 1.132s\n",
            "| Adam | epoch: 024 | loss: 93798.59375 - acc: 0.2174 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 769  | total loss: \u001b[1m\u001b[32m94716.12500\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 025 | loss: 94716.12500 - acc: 0.2082 -- iter: 016/497\n",
            "Training Step: 770  | total loss: \u001b[1m\u001b[32m94310.92969\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 025 | loss: 94310.92969 - acc: 0.2311 -- iter: 032/497\n",
            "Training Step: 771  | total loss: \u001b[1m\u001b[32m95975.23438\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 025 | loss: 95975.23438 - acc: 0.2268 -- iter: 048/497\n",
            "Training Step: 772  | total loss: \u001b[1m\u001b[32m97296.02344\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 025 | loss: 97296.02344 - acc: 0.2166 -- iter: 064/497\n",
            "Training Step: 773  | total loss: \u001b[1m\u001b[32m96648.81250\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 025 | loss: 96648.81250 - acc: 0.2137 -- iter: 080/497\n",
            "Training Step: 774  | total loss: \u001b[1m\u001b[32m96884.05469\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 025 | loss: 96884.05469 - acc: 0.2423 -- iter: 096/497\n",
            "Training Step: 775  | total loss: \u001b[1m\u001b[32m99426.10156\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 025 | loss: 99426.10156 - acc: 0.2556 -- iter: 112/497\n",
            "Training Step: 776  | total loss: \u001b[1m\u001b[32m99290.58594\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 025 | loss: 99290.58594 - acc: 0.2675 -- iter: 128/497\n",
            "Training Step: 777  | total loss: \u001b[1m\u001b[32m96732.59375\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 025 | loss: 96732.59375 - acc: 0.2533 -- iter: 144/497\n",
            "Training Step: 778  | total loss: \u001b[1m\u001b[32m97635.67188\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 025 | loss: 97635.67188 - acc: 0.2529 -- iter: 160/497\n",
            "Training Step: 779  | total loss: \u001b[1m\u001b[32m99534.77344\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 025 | loss: 99534.77344 - acc: 0.2526 -- iter: 176/497\n",
            "Training Step: 780  | total loss: \u001b[1m\u001b[32m100907.49219\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 025 | loss: 100907.49219 - acc: 0.2524 -- iter: 192/497\n",
            "Training Step: 781  | total loss: \u001b[1m\u001b[32m100162.93750\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 025 | loss: 100162.93750 - acc: 0.2396 -- iter: 208/497\n",
            "Training Step: 782  | total loss: \u001b[1m\u001b[32m98225.14844\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 025 | loss: 98225.14844 - acc: 0.2532 -- iter: 224/497\n",
            "Training Step: 783  | total loss: \u001b[1m\u001b[32m95258.71094\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 025 | loss: 95258.71094 - acc: 0.2529 -- iter: 240/497\n",
            "Training Step: 784  | total loss: \u001b[1m\u001b[32m98267.57812\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 025 | loss: 98267.57812 - acc: 0.2463 -- iter: 256/497\n",
            "Training Step: 785  | total loss: \u001b[1m\u001b[32m102182.86719\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 025 | loss: 102182.86719 - acc: 0.2342 -- iter: 272/497\n",
            "Training Step: 786  | total loss: \u001b[1m\u001b[32m103921.73438\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 025 | loss: 103921.73438 - acc: 0.2295 -- iter: 288/497\n",
            "Training Step: 787  | total loss: \u001b[1m\u001b[32m104960.05469\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 025 | loss: 104960.05469 - acc: 0.2191 -- iter: 304/497\n",
            "Training Step: 788  | total loss: \u001b[1m\u001b[32m107200.60938\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 025 | loss: 107200.60938 - acc: 0.2284 -- iter: 320/497\n",
            "Training Step: 789  | total loss: \u001b[1m\u001b[32m107934.82812\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 025 | loss: 107934.82812 - acc: 0.2368 -- iter: 336/497\n",
            "Training Step: 790  | total loss: \u001b[1m\u001b[32m104039.19531\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 025 | loss: 104039.19531 - acc: 0.2381 -- iter: 352/497\n",
            "Training Step: 791  | total loss: \u001b[1m\u001b[32m105587.11719\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 025 | loss: 105587.11719 - acc: 0.2456 -- iter: 368/497\n",
            "Training Step: 792  | total loss: \u001b[1m\u001b[32m112419.60156\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 025 | loss: 112419.60156 - acc: 0.2210 -- iter: 384/497\n",
            "Training Step: 793  | total loss: \u001b[1m\u001b[32m118568.83594\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 025 | loss: 118568.83594 - acc: 0.1989 -- iter: 400/497\n",
            "Training Step: 794  | total loss: \u001b[1m\u001b[32m115041.82812\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 025 | loss: 115041.82812 - acc: 0.2228 -- iter: 416/497\n",
            "Training Step: 795  | total loss: \u001b[1m\u001b[32m115682.21094\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 025 | loss: 115682.21094 - acc: 0.2317 -- iter: 432/497\n",
            "Training Step: 796  | total loss: \u001b[1m\u001b[32m113854.07031\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 025 | loss: 113854.07031 - acc: 0.2273 -- iter: 448/497\n",
            "Training Step: 797  | total loss: \u001b[1m\u001b[32m115652.68750\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 025 | loss: 115652.68750 - acc: 0.2296 -- iter: 464/497\n",
            "Training Step: 798  | total loss: \u001b[1m\u001b[32m115201.78125\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 025 | loss: 115201.78125 - acc: 0.2254 -- iter: 480/497\n",
            "Training Step: 799  | total loss: \u001b[1m\u001b[32m115628.55469\u001b[0m\u001b[0m | time: 0.126s\n",
            "| Adam | epoch: 025 | loss: 115628.55469 - acc: 0.2278 -- iter: 496/497\n",
            "Training Step: 800  | total loss: \u001b[1m\u001b[32m114046.10938\u001b[0m\u001b[0m | time: 1.136s\n",
            "| Adam | epoch: 025 | loss: 114046.10938 - acc: 0.2363 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 801  | total loss: \u001b[1m\u001b[32m112749.60156\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 026 | loss: 112749.60156 - acc: 0.2314 -- iter: 016/497\n",
            "Training Step: 802  | total loss: \u001b[1m\u001b[32m111312.11719\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 026 | loss: 111312.11719 - acc: 0.2333 -- iter: 032/497\n",
            "Training Step: 803  | total loss: \u001b[1m\u001b[32m106976.11719\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 026 | loss: 106976.11719 - acc: 0.2412 -- iter: 048/497\n",
            "Training Step: 804  | total loss: \u001b[1m\u001b[32m108765.92969\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 026 | loss: 108765.92969 - acc: 0.2421 -- iter: 064/497\n",
            "Training Step: 805  | total loss: \u001b[1m\u001b[32m109014.46094\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 026 | loss: 109014.46094 - acc: 0.2429 -- iter: 080/497\n",
            "Training Step: 806  | total loss: \u001b[1m\u001b[32m106526.53125\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 026 | loss: 106526.53125 - acc: 0.2623 -- iter: 096/497\n",
            "Training Step: 807  | total loss: \u001b[1m\u001b[32m105836.97656\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 026 | loss: 105836.97656 - acc: 0.2549 -- iter: 112/497\n",
            "Training Step: 808  | total loss: \u001b[1m\u001b[32m109683.91406\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 026 | loss: 109683.91406 - acc: 0.2294 -- iter: 128/497\n",
            "Training Step: 809  | total loss: \u001b[1m\u001b[32m107139.35938\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 026 | loss: 107139.35938 - acc: 0.2252 -- iter: 144/497\n",
            "Training Step: 810  | total loss: \u001b[1m\u001b[32m108441.46094\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 026 | loss: 108441.46094 - acc: 0.2339 -- iter: 160/497\n",
            "Training Step: 811  | total loss: \u001b[1m\u001b[32m106696.55469\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 026 | loss: 106696.55469 - acc: 0.2230 -- iter: 176/497\n",
            "Training Step: 812  | total loss: \u001b[1m\u001b[32m107907.60156\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 026 | loss: 107907.60156 - acc: 0.2320 -- iter: 192/497\n",
            "Training Step: 813  | total loss: \u001b[1m\u001b[32m107607.40625\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 026 | loss: 107607.40625 - acc: 0.2400 -- iter: 208/497\n",
            "Training Step: 814  | total loss: \u001b[1m\u001b[32m109834.78906\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 026 | loss: 109834.78906 - acc: 0.2598 -- iter: 224/497\n",
            "Training Step: 815  | total loss: \u001b[1m\u001b[32m105155.58594\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 026 | loss: 105155.58594 - acc: 0.2525 -- iter: 240/497\n",
            "Training Step: 816  | total loss: \u001b[1m\u001b[32m106694.35156\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 026 | loss: 106694.35156 - acc: 0.2460 -- iter: 256/497\n",
            "Training Step: 817  | total loss: \u001b[1m\u001b[32m102256.20312\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 026 | loss: 102256.20312 - acc: 0.2402 -- iter: 272/497\n",
            "Training Step: 818  | total loss: \u001b[1m\u001b[32m103350.64062\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 026 | loss: 103350.64062 - acc: 0.2662 -- iter: 288/497\n",
            "Training Step: 819  | total loss: \u001b[1m\u001b[32m100298.18750\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 026 | loss: 100298.18750 - acc: 0.2708 -- iter: 304/497\n",
            "Training Step: 820  | total loss: \u001b[1m\u001b[32m99046.53125\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 026 | loss: 99046.53125 - acc: 0.2625 -- iter: 320/497\n",
            "Training Step: 821  | total loss: \u001b[1m\u001b[32m97906.59375\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 026 | loss: 97906.59375 - acc: 0.2487 -- iter: 336/497\n",
            "Training Step: 822  | total loss: \u001b[1m\u001b[32m100309.13281\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 026 | loss: 100309.13281 - acc: 0.2301 -- iter: 352/497\n",
            "Training Step: 823  | total loss: \u001b[1m\u001b[32m104723.51562\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 026 | loss: 104723.51562 - acc: 0.2258 -- iter: 368/497\n",
            "Training Step: 824  | total loss: \u001b[1m\u001b[32m103865.67969\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 026 | loss: 103865.67969 - acc: 0.2220 -- iter: 384/497\n",
            "Training Step: 825  | total loss: \u001b[1m\u001b[32m98817.88281\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 026 | loss: 98817.88281 - acc: 0.1998 -- iter: 400/497\n",
            "Training Step: 826  | total loss: \u001b[1m\u001b[32m94274.86719\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 026 | loss: 94274.86719 - acc: 0.1798 -- iter: 416/497\n",
            "Training Step: 827  | total loss: \u001b[1m\u001b[32m97745.21094\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 026 | loss: 97745.21094 - acc: 0.2056 -- iter: 432/497\n",
            "Training Step: 828  | total loss: \u001b[1m\u001b[32m96651.17188\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 026 | loss: 96651.17188 - acc: 0.2038 -- iter: 448/497\n",
            "Training Step: 829  | total loss: \u001b[1m\u001b[32m98825.58594\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 026 | loss: 98825.58594 - acc: 0.2147 -- iter: 464/497\n",
            "Training Step: 830  | total loss: \u001b[1m\u001b[32m99091.96094\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 026 | loss: 99091.96094 - acc: 0.2182 -- iter: 480/497\n",
            "Training Step: 831  | total loss: \u001b[1m\u001b[32m98865.45312\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 026 | loss: 98865.45312 - acc: 0.2214 -- iter: 496/497\n",
            "Training Step: 832  | total loss: \u001b[1m\u001b[32m99095.90625\u001b[0m\u001b[0m | time: 1.130s\n",
            "| Adam | epoch: 026 | loss: 99095.90625 - acc: 0.2367 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 833  | total loss: \u001b[1m\u001b[32m102101.31250\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 027 | loss: 102101.31250 - acc: 0.2443 -- iter: 016/497\n",
            "Training Step: 834  | total loss: \u001b[1m\u001b[32m100782.35156\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 027 | loss: 100782.35156 - acc: 0.2386 -- iter: 032/497\n",
            "Training Step: 835  | total loss: \u001b[1m\u001b[32m99406.28906\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 027 | loss: 99406.28906 - acc: 0.2210 -- iter: 048/497\n",
            "Training Step: 836  | total loss: \u001b[1m\u001b[32m99853.90625\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 027 | loss: 99853.90625 - acc: 0.2302 -- iter: 064/497\n",
            "Training Step: 837  | total loss: \u001b[1m\u001b[32m100272.96094\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 027 | loss: 100272.96094 - acc: 0.2259 -- iter: 080/497\n",
            "Training Step: 838  | total loss: \u001b[1m\u001b[32m99840.71094\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 027 | loss: 99840.71094 - acc: 0.2596 -- iter: 096/497\n",
            "Training Step: 839  | total loss: \u001b[1m\u001b[32m98591.39844\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 027 | loss: 98591.39844 - acc: 0.2711 -- iter: 112/497\n",
            "Training Step: 840  | total loss: \u001b[1m\u001b[32m97126.86719\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 027 | loss: 97126.86719 - acc: 0.2627 -- iter: 128/497\n",
            "Training Step: 841  | total loss: \u001b[1m\u001b[32m94615.41406\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 027 | loss: 94615.41406 - acc: 0.2740 -- iter: 144/497\n",
            "Training Step: 842  | total loss: \u001b[1m\u001b[32m92878.35156\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 027 | loss: 92878.35156 - acc: 0.2528 -- iter: 160/497\n",
            "Training Step: 843  | total loss: \u001b[1m\u001b[32m94114.77344\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 027 | loss: 94114.77344 - acc: 0.2525 -- iter: 176/497\n",
            "Training Step: 844  | total loss: \u001b[1m\u001b[32m95650.25781\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 027 | loss: 95650.25781 - acc: 0.2460 -- iter: 192/497\n",
            "Training Step: 845  | total loss: \u001b[1m\u001b[32m95272.63281\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 027 | loss: 95272.63281 - acc: 0.2527 -- iter: 208/497\n",
            "Training Step: 846  | total loss: \u001b[1m\u001b[32m98192.25000\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 027 | loss: 98192.25000 - acc: 0.2462 -- iter: 224/497\n",
            "Training Step: 847  | total loss: \u001b[1m\u001b[32m101509.92188\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 027 | loss: 101509.92188 - acc: 0.2528 -- iter: 240/497\n",
            "Training Step: 848  | total loss: \u001b[1m\u001b[32m101844.74219\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 027 | loss: 101844.74219 - acc: 0.2463 -- iter: 256/497\n",
            "Training Step: 849  | total loss: \u001b[1m\u001b[32m99369.45312\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 027 | loss: 99369.45312 - acc: 0.2341 -- iter: 272/497\n",
            "Training Step: 850  | total loss: \u001b[1m\u001b[32m101022.31250\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 027 | loss: 101022.31250 - acc: 0.2107 -- iter: 288/497\n",
            "Training Step: 851  | total loss: \u001b[1m\u001b[32m101553.59375\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 027 | loss: 101553.59375 - acc: 0.2084 -- iter: 304/497\n",
            "Training Step: 852  | total loss: \u001b[1m\u001b[32m101818.32031\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 027 | loss: 101818.32031 - acc: 0.2063 -- iter: 320/497\n",
            "Training Step: 853  | total loss: \u001b[1m\u001b[32m101729.00781\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 027 | loss: 101729.00781 - acc: 0.2169 -- iter: 336/497\n",
            "Training Step: 854  | total loss: \u001b[1m\u001b[32m103295.13281\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 027 | loss: 103295.13281 - acc: 0.2202 -- iter: 352/497\n",
            "Training Step: 855  | total loss: \u001b[1m\u001b[32m102630.76562\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 027 | loss: 102630.76562 - acc: 0.2232 -- iter: 368/497\n",
            "Training Step: 856  | total loss: \u001b[1m\u001b[32m103461.28125\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 027 | loss: 103461.28125 - acc: 0.2259 -- iter: 384/497\n",
            "Training Step: 857  | total loss: \u001b[1m\u001b[32m107784.14844\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 027 | loss: 107784.14844 - acc: 0.2221 -- iter: 400/497\n",
            "Training Step: 858  | total loss: \u001b[1m\u001b[32m115626.28125\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 027 | loss: 115626.28125 - acc: 0.1998 -- iter: 416/497\n",
            "Training Step: 859  | total loss: \u001b[1m\u001b[32m122684.19531\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 027 | loss: 122684.19531 - acc: 0.1799 -- iter: 432/497\n",
            "Training Step: 860  | total loss: \u001b[1m\u001b[32m123549.14844\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 027 | loss: 123549.14844 - acc: 0.1806 -- iter: 448/497\n",
            "Training Step: 861  | total loss: \u001b[1m\u001b[32m117637.43750\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 027 | loss: 117637.43750 - acc: 0.1876 -- iter: 464/497\n",
            "Training Step: 862  | total loss: \u001b[1m\u001b[32m116953.68750\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 027 | loss: 116953.68750 - acc: 0.1938 -- iter: 480/497\n",
            "Training Step: 863  | total loss: \u001b[1m\u001b[32m117767.54688\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 027 | loss: 117767.54688 - acc: 0.1994 -- iter: 496/497\n",
            "Training Step: 864  | total loss: \u001b[1m\u001b[32m114942.35156\u001b[0m\u001b[0m | time: 1.133s\n",
            "| Adam | epoch: 027 | loss: 114942.35156 - acc: 0.1920 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 865  | total loss: \u001b[1m\u001b[32m113921.39844\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 028 | loss: 113921.39844 - acc: 0.2040 -- iter: 016/497\n",
            "Training Step: 866  | total loss: \u001b[1m\u001b[32m111211.12500\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 028 | loss: 111211.12500 - acc: 0.2024 -- iter: 032/497\n",
            "Training Step: 867  | total loss: \u001b[1m\u001b[32m109060.28125\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 028 | loss: 109060.28125 - acc: 0.2009 -- iter: 048/497\n",
            "Training Step: 868  | total loss: \u001b[1m\u001b[32m109164.11719\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 028 | loss: 109164.11719 - acc: 0.2183 -- iter: 064/497\n",
            "Training Step: 869  | total loss: \u001b[1m\u001b[32m107494.17969\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 028 | loss: 107494.17969 - acc: 0.2090 -- iter: 080/497\n",
            "Training Step: 870  | total loss: \u001b[1m\u001b[32m108954.06250\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 028 | loss: 108954.06250 - acc: 0.2193 -- iter: 096/497\n",
            "Training Step: 871  | total loss: \u001b[1m\u001b[32m107407.10938\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 028 | loss: 107407.10938 - acc: 0.2286 -- iter: 112/497\n",
            "Training Step: 872  | total loss: \u001b[1m\u001b[32m105204.67188\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 028 | loss: 105204.67188 - acc: 0.2308 -- iter: 128/497\n",
            "Training Step: 873  | total loss: \u001b[1m\u001b[32m104960.53906\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 028 | loss: 104960.53906 - acc: 0.2140 -- iter: 144/497\n",
            "Training Step: 874  | total loss: \u001b[1m\u001b[32m105094.92969\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 028 | loss: 105094.92969 - acc: 0.2051 -- iter: 160/497\n",
            "Training Step: 875  | total loss: \u001b[1m\u001b[32m105021.78906\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 028 | loss: 105021.78906 - acc: 0.2158 -- iter: 176/497\n",
            "Training Step: 876  | total loss: \u001b[1m\u001b[32m106433.47656\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 028 | loss: 106433.47656 - acc: 0.2317 -- iter: 192/497\n",
            "Training Step: 877  | total loss: \u001b[1m\u001b[32m108015.78906\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 028 | loss: 108015.78906 - acc: 0.2273 -- iter: 208/497\n",
            "Training Step: 878  | total loss: \u001b[1m\u001b[32m108914.35156\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 028 | loss: 108914.35156 - acc: 0.2358 -- iter: 224/497\n",
            "Training Step: 879  | total loss: \u001b[1m\u001b[32m105769.66406\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 028 | loss: 105769.66406 - acc: 0.2372 -- iter: 240/497\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m103099.73438\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 028 | loss: 103099.73438 - acc: 0.2385 -- iter: 256/497\n",
            "Training Step: 881  | total loss: \u001b[1m\u001b[32m103928.29688\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 028 | loss: 103928.29688 - acc: 0.2397 -- iter: 272/497\n",
            "Training Step: 882  | total loss: \u001b[1m\u001b[32m104384.91406\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 028 | loss: 104384.91406 - acc: 0.2407 -- iter: 288/497\n",
            "Training Step: 883  | total loss: \u001b[1m\u001b[32m106217.47656\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 028 | loss: 106217.47656 - acc: 0.2541 -- iter: 304/497\n",
            "Training Step: 884  | total loss: \u001b[1m\u001b[32m107145.81250\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 028 | loss: 107145.81250 - acc: 0.2600 -- iter: 320/497\n",
            "Training Step: 885  | total loss: \u001b[1m\u001b[32m104205.62500\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 028 | loss: 104205.62500 - acc: 0.2715 -- iter: 336/497\n",
            "Training Step: 886  | total loss: \u001b[1m\u001b[32m104440.92188\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 028 | loss: 104440.92188 - acc: 0.2818 -- iter: 352/497\n",
            "Training Step: 887  | total loss: \u001b[1m\u001b[32m103165.56250\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 028 | loss: 103165.56250 - acc: 0.2599 -- iter: 368/497\n",
            "Training Step: 888  | total loss: \u001b[1m\u001b[32m106200.24219\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 028 | loss: 106200.24219 - acc: 0.2589 -- iter: 384/497\n",
            "Training Step: 889  | total loss: \u001b[1m\u001b[32m105755.77344\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 028 | loss: 105755.77344 - acc: 0.2455 -- iter: 400/497\n",
            "Training Step: 890  | total loss: \u001b[1m\u001b[32m105960.02344\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 028 | loss: 105960.02344 - acc: 0.2397 -- iter: 416/497\n",
            "Training Step: 891  | total loss: \u001b[1m\u001b[32m106164.53125\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 028 | loss: 106164.53125 - acc: 0.3157 -- iter: 432/497\n",
            "Training Step: 892  | total loss: \u001b[1m\u001b[32m106348.58594\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 028 | loss: 106348.58594 - acc: 0.3842 -- iter: 448/497\n",
            "Training Step: 893  | total loss: \u001b[1m\u001b[32m106896.47656\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 028 | loss: 106896.47656 - acc: 0.3707 -- iter: 464/497\n",
            "Training Step: 894  | total loss: \u001b[1m\u001b[32m109008.84375\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 028 | loss: 109008.84375 - acc: 0.3587 -- iter: 480/497\n",
            "Training Step: 895  | total loss: \u001b[1m\u001b[32m109628.72656\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 028 | loss: 109628.72656 - acc: 0.3478 -- iter: 496/497\n",
            "Training Step: 896  | total loss: \u001b[1m\u001b[32m108623.96094\u001b[0m\u001b[0m | time: 1.122s\n",
            "| Adam | epoch: 028 | loss: 108623.96094 - acc: 0.3318 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 897  | total loss: \u001b[1m\u001b[32m108104.60156\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 029 | loss: 108104.60156 - acc: 0.3361 -- iter: 016/497\n",
            "Training Step: 898  | total loss: \u001b[1m\u001b[32m106348.16406\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 029 | loss: 106348.16406 - acc: 0.3400 -- iter: 032/497\n",
            "Training Step: 899  | total loss: \u001b[1m\u001b[32m103739.04688\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 029 | loss: 103739.04688 - acc: 0.3435 -- iter: 048/497\n",
            "Training Step: 900  | total loss: \u001b[1m\u001b[32m103049.92969\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 029 | loss: 103049.92969 - acc: 0.3216 -- iter: 064/497\n",
            "Training Step: 901  | total loss: \u001b[1m\u001b[32m103231.63281\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 029 | loss: 103231.63281 - acc: 0.3145 -- iter: 080/497\n",
            "Training Step: 902  | total loss: \u001b[1m\u001b[32m99571.93750\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 029 | loss: 99571.93750 - acc: 0.3018 -- iter: 096/497\n",
            "Training Step: 903  | total loss: \u001b[1m\u001b[32m101286.90625\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 029 | loss: 101286.90625 - acc: 0.3091 -- iter: 112/497\n",
            "Training Step: 904  | total loss: \u001b[1m\u001b[32m103028.22656\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 029 | loss: 103028.22656 - acc: 0.3032 -- iter: 128/497\n",
            "Training Step: 905  | total loss: \u001b[1m\u001b[32m101292.77344\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 029 | loss: 101292.77344 - acc: 0.2916 -- iter: 144/497\n",
            "Training Step: 906  | total loss: \u001b[1m\u001b[32m103088.85938\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 029 | loss: 103088.85938 - acc: 0.2687 -- iter: 160/497\n",
            "Training Step: 907  | total loss: \u001b[1m\u001b[32m102061.71094\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 029 | loss: 102061.71094 - acc: 0.2793 -- iter: 176/497\n",
            "Training Step: 908  | total loss: \u001b[1m\u001b[32m102259.36719\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 029 | loss: 102259.36719 - acc: 0.2764 -- iter: 192/497\n",
            "Training Step: 909  | total loss: \u001b[1m\u001b[32m103330.75000\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 029 | loss: 103330.75000 - acc: 0.2675 -- iter: 208/497\n",
            "Training Step: 910  | total loss: \u001b[1m\u001b[32m105687.21094\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 029 | loss: 105687.21094 - acc: 0.2595 -- iter: 224/497\n",
            "Training Step: 911  | total loss: \u001b[1m\u001b[32m102952.22656\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 029 | loss: 102952.22656 - acc: 0.2586 -- iter: 240/497\n",
            "Training Step: 912  | total loss: \u001b[1m\u001b[32m103091.05469\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 029 | loss: 103091.05469 - acc: 0.2640 -- iter: 256/497\n",
            "Training Step: 913  | total loss: \u001b[1m\u001b[32m103221.59375\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 029 | loss: 103221.59375 - acc: 0.2563 -- iter: 272/497\n",
            "Training Step: 914  | total loss: \u001b[1m\u001b[32m106068.65625\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 029 | loss: 106068.65625 - acc: 0.2494 -- iter: 288/497\n",
            "Training Step: 915  | total loss: \u001b[1m\u001b[32m107002.49219\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 029 | loss: 107002.49219 - acc: 0.2370 -- iter: 304/497\n",
            "Training Step: 916  | total loss: \u001b[1m\u001b[32m102076.59375\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 029 | loss: 102076.59375 - acc: 0.2383 -- iter: 320/497\n",
            "Training Step: 917  | total loss: \u001b[1m\u001b[32m101333.75000\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 029 | loss: 101333.75000 - acc: 0.2457 -- iter: 336/497\n",
            "Training Step: 918  | total loss: \u001b[1m\u001b[32m103257.32812\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 029 | loss: 103257.32812 - acc: 0.2399 -- iter: 352/497\n",
            "Training Step: 919  | total loss: \u001b[1m\u001b[32m102623.82031\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 029 | loss: 102623.82031 - acc: 0.2346 -- iter: 368/497\n",
            "Training Step: 920  | total loss: \u001b[1m\u001b[32m103143.47656\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 029 | loss: 103143.47656 - acc: 0.2424 -- iter: 384/497\n",
            "Training Step: 921  | total loss: \u001b[1m\u001b[32m102957.66406\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 029 | loss: 102957.66406 - acc: 0.2369 -- iter: 400/497\n",
            "Training Step: 922  | total loss: \u001b[1m\u001b[32m103228.20312\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 029 | loss: 103228.20312 - acc: 0.2382 -- iter: 416/497\n",
            "Training Step: 923  | total loss: \u001b[1m\u001b[32m105102.67969\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 029 | loss: 105102.67969 - acc: 0.2519 -- iter: 432/497\n",
            "Training Step: 924  | total loss: \u001b[1m\u001b[32m99599.14844\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 029 | loss: 99599.14844 - acc: 0.2267 -- iter: 448/497\n",
            "Training Step: 925  | total loss: \u001b[1m\u001b[32m94645.97656\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 029 | loss: 94645.97656 - acc: 0.2041 -- iter: 464/497\n",
            "Training Step: 926  | total loss: \u001b[1m\u001b[32m92138.79688\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 029 | loss: 92138.79688 - acc: 0.2087 -- iter: 480/497\n",
            "Training Step: 927  | total loss: \u001b[1m\u001b[32m93997.84375\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 029 | loss: 93997.84375 - acc: 0.2190 -- iter: 496/497\n",
            "Training Step: 928  | total loss: \u001b[1m\u001b[32m93914.08594\u001b[0m\u001b[0m | time: 1.123s\n",
            "| Adam | epoch: 029 | loss: 93914.08594 - acc: 0.2159 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 929  | total loss: \u001b[1m\u001b[32m93867.07031\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 030 | loss: 93867.07031 - acc: 0.2068 -- iter: 016/497\n",
            "Training Step: 930  | total loss: \u001b[1m\u001b[32m95685.83594\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 030 | loss: 95685.83594 - acc: 0.2049 -- iter: 032/497\n",
            "Training Step: 931  | total loss: \u001b[1m\u001b[32m93146.36719\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 030 | loss: 93146.36719 - acc: 0.2094 -- iter: 048/497\n",
            "Training Step: 932  | total loss: \u001b[1m\u001b[32m95085.73438\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 030 | loss: 95085.73438 - acc: 0.2197 -- iter: 064/497\n",
            "Training Step: 933  | total loss: \u001b[1m\u001b[32m96273.57812\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 030 | loss: 96273.57812 - acc: 0.2165 -- iter: 080/497\n",
            "Training Step: 934  | total loss: \u001b[1m\u001b[32m100415.25000\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 030 | loss: 100415.25000 - acc: 0.2261 -- iter: 096/497\n",
            "Training Step: 935  | total loss: \u001b[1m\u001b[32m100442.79688\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 030 | loss: 100442.79688 - acc: 0.2347 -- iter: 112/497\n",
            "Training Step: 936  | total loss: \u001b[1m\u001b[32m100466.91406\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 030 | loss: 100466.91406 - acc: 0.2550 -- iter: 128/497\n",
            "Training Step: 937  | total loss: \u001b[1m\u001b[32m101363.75000\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 030 | loss: 101363.75000 - acc: 0.2545 -- iter: 144/497\n",
            "Training Step: 938  | total loss: \u001b[1m\u001b[32m101228.95312\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 030 | loss: 101228.95312 - acc: 0.2603 -- iter: 160/497\n",
            "Training Step: 939  | total loss: \u001b[1m\u001b[32m104177.18750\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 030 | loss: 104177.18750 - acc: 0.2655 -- iter: 176/497\n",
            "Training Step: 940  | total loss: \u001b[1m\u001b[32m103931.65625\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 030 | loss: 103931.65625 - acc: 0.2515 -- iter: 192/497\n",
            "Training Step: 941  | total loss: \u001b[1m\u001b[32m105542.91406\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 030 | loss: 105542.91406 - acc: 0.2576 -- iter: 208/497\n",
            "Training Step: 942  | total loss: \u001b[1m\u001b[32m106292.07031\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 030 | loss: 106292.07031 - acc: 0.2443 -- iter: 224/497\n",
            "Training Step: 943  | total loss: \u001b[1m\u001b[32m102021.62500\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 030 | loss: 102021.62500 - acc: 0.2386 -- iter: 240/497\n",
            "Training Step: 944  | total loss: \u001b[1m\u001b[32m101441.47656\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 030 | loss: 101441.47656 - acc: 0.2335 -- iter: 256/497\n",
            "Training Step: 945  | total loss: \u001b[1m\u001b[32m100450.14844\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 030 | loss: 100450.14844 - acc: 0.2289 -- iter: 272/497\n",
            "Training Step: 946  | total loss: \u001b[1m\u001b[32m103930.51562\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 030 | loss: 103930.51562 - acc: 0.2310 -- iter: 288/497\n",
            "Training Step: 947  | total loss: \u001b[1m\u001b[32m105378.86719\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 030 | loss: 105378.86719 - acc: 0.2267 -- iter: 304/497\n",
            "Training Step: 948  | total loss: \u001b[1m\u001b[32m104403.64062\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 030 | loss: 104403.64062 - acc: 0.2353 -- iter: 320/497\n",
            "Training Step: 949  | total loss: \u001b[1m\u001b[32m104034.40625\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 030 | loss: 104034.40625 - acc: 0.2555 -- iter: 336/497\n",
            "Training Step: 950  | total loss: \u001b[1m\u001b[32m101126.68750\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 030 | loss: 101126.68750 - acc: 0.2549 -- iter: 352/497\n",
            "Training Step: 951  | total loss: \u001b[1m\u001b[32m99731.61719\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 030 | loss: 99731.61719 - acc: 0.2544 -- iter: 368/497\n",
            "Training Step: 952  | total loss: \u001b[1m\u001b[32m103262.25000\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 030 | loss: 103262.25000 - acc: 0.2415 -- iter: 384/497\n",
            "Training Step: 953  | total loss: \u001b[1m\u001b[32m106383.96875\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 030 | loss: 106383.96875 - acc: 0.2423 -- iter: 400/497\n",
            "Training Step: 954  | total loss: \u001b[1m\u001b[32m104204.67969\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 030 | loss: 104204.67969 - acc: 0.2431 -- iter: 416/497\n",
            "Training Step: 955  | total loss: \u001b[1m\u001b[32m105649.97656\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 030 | loss: 105649.97656 - acc: 0.2500 -- iter: 432/497\n",
            "Training Step: 956  | total loss: \u001b[1m\u001b[32m104923.10156\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 030 | loss: 104923.10156 - acc: 0.2313 -- iter: 448/497\n",
            "Training Step: 957  | total loss: \u001b[1m\u001b[32m95721.39062\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 030 | loss: 95721.39062 - acc: 0.3082 -- iter: 464/497\n",
            "Training Step: 958  | total loss: \u001b[1m\u001b[32m96951.60156\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 030 | loss: 96951.60156 - acc: 0.2773 -- iter: 480/497\n",
            "Training Step: 959  | total loss: \u001b[1m\u001b[32m95148.17969\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 030 | loss: 95148.17969 - acc: 0.2746 -- iter: 496/497\n",
            "Training Step: 960  | total loss: \u001b[1m\u001b[32m96601.07812\u001b[0m\u001b[0m | time: 1.124s\n",
            "| Adam | epoch: 030 | loss: 96601.07812 - acc: 0.2909 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 961  | total loss: \u001b[1m\u001b[32m98527.08594\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 031 | loss: 98527.08594 - acc: 0.2993 -- iter: 016/497\n",
            "Training Step: 962  | total loss: \u001b[1m\u001b[32m100629.48438\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 031 | loss: 100629.48438 - acc: 0.2881 -- iter: 032/497\n",
            "Training Step: 963  | total loss: \u001b[1m\u001b[32m101703.39062\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 031 | loss: 101703.39062 - acc: 0.3031 -- iter: 048/497\n",
            "Training Step: 964  | total loss: \u001b[1m\u001b[32m100968.79688\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 031 | loss: 100968.79688 - acc: 0.3040 -- iter: 064/497\n",
            "Training Step: 965  | total loss: \u001b[1m\u001b[32m101815.78906\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 031 | loss: 101815.78906 - acc: 0.2861 -- iter: 080/497\n",
            "Training Step: 966  | total loss: \u001b[1m\u001b[32m103279.37500\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 031 | loss: 103279.37500 - acc: 0.2762 -- iter: 096/497\n",
            "Training Step: 967  | total loss: \u001b[1m\u001b[32m100360.82812\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 031 | loss: 100360.82812 - acc: 0.2674 -- iter: 112/497\n",
            "Training Step: 968  | total loss: \u001b[1m\u001b[32m99609.97656\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 031 | loss: 99609.97656 - acc: 0.2531 -- iter: 128/497\n",
            "Training Step: 969  | total loss: \u001b[1m\u001b[32m101814.39844\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 031 | loss: 101814.39844 - acc: 0.2466 -- iter: 144/497\n",
            "Training Step: 970  | total loss: \u001b[1m\u001b[32m103934.69531\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 031 | loss: 103934.69531 - acc: 0.2532 -- iter: 160/497\n",
            "Training Step: 971  | total loss: \u001b[1m\u001b[32m100522.20312\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 031 | loss: 100522.20312 - acc: 0.2403 -- iter: 176/497\n",
            "Training Step: 972  | total loss: \u001b[1m\u001b[32m100879.16406\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 031 | loss: 100879.16406 - acc: 0.2476 -- iter: 192/497\n",
            "Training Step: 973  | total loss: \u001b[1m\u001b[32m101742.92969\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 031 | loss: 101742.92969 - acc: 0.2541 -- iter: 208/497\n",
            "Training Step: 974  | total loss: \u001b[1m\u001b[32m104145.77344\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 031 | loss: 104145.77344 - acc: 0.2599 -- iter: 224/497\n",
            "Training Step: 975  | total loss: \u001b[1m\u001b[32m105845.51562\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 031 | loss: 105845.51562 - acc: 0.2464 -- iter: 240/497\n",
            "Training Step: 976  | total loss: \u001b[1m\u001b[32m107367.00781\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 031 | loss: 107367.00781 - acc: 0.2530 -- iter: 256/497\n",
            "Training Step: 977  | total loss: \u001b[1m\u001b[32m103584.31250\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 031 | loss: 103584.31250 - acc: 0.2465 -- iter: 272/497\n",
            "Training Step: 978  | total loss: \u001b[1m\u001b[32m101023.51562\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 031 | loss: 101023.51562 - acc: 0.2406 -- iter: 288/497\n",
            "Training Step: 979  | total loss: \u001b[1m\u001b[32m102100.75781\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 031 | loss: 102100.75781 - acc: 0.2353 -- iter: 304/497\n",
            "Training Step: 980  | total loss: \u001b[1m\u001b[32m99562.35938\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 031 | loss: 99562.35938 - acc: 0.2430 -- iter: 320/497\n",
            "Training Step: 981  | total loss: \u001b[1m\u001b[32m100411.06250\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 031 | loss: 100411.06250 - acc: 0.2499 -- iter: 336/497\n",
            "Training Step: 982  | total loss: \u001b[1m\u001b[32m99810.32812\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 031 | loss: 99810.32812 - acc: 0.2749 -- iter: 352/497\n",
            "Training Step: 983  | total loss: \u001b[1m\u001b[32m102570.71094\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 031 | loss: 102570.71094 - acc: 0.2600 -- iter: 368/497\n",
            "Training Step: 984  | total loss: \u001b[1m\u001b[32m103936.19531\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 031 | loss: 103936.19531 - acc: 0.2465 -- iter: 384/497\n",
            "Training Step: 985  | total loss: \u001b[1m\u001b[32m105725.82812\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 031 | loss: 105725.82812 - acc: 0.2531 -- iter: 400/497\n",
            "Training Step: 986  | total loss: \u001b[1m\u001b[32m102829.96094\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 031 | loss: 102829.96094 - acc: 0.2590 -- iter: 416/497\n",
            "Training Step: 987  | total loss: \u001b[1m\u001b[32m101797.72656\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 031 | loss: 101797.72656 - acc: 0.2519 -- iter: 432/497\n",
            "Training Step: 988  | total loss: \u001b[1m\u001b[32m102419.82812\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 031 | loss: 102419.82812 - acc: 0.2392 -- iter: 448/497\n",
            "Training Step: 989  | total loss: \u001b[1m\u001b[32m103335.94531\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 031 | loss: 103335.94531 - acc: 0.2278 -- iter: 464/497\n",
            "Training Step: 990  | total loss: \u001b[1m\u001b[32m103804.69531\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 031 | loss: 103804.69531 - acc: 0.2050 -- iter: 480/497\n",
            "Training Step: 991  | total loss: \u001b[1m\u001b[32m104226.57031\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 031 | loss: 104226.57031 - acc: 0.1845 -- iter: 496/497\n",
            "Training Step: 992  | total loss: \u001b[1m\u001b[32m103874.85938\u001b[0m\u001b[0m | time: 1.131s\n",
            "| Adam | epoch: 031 | loss: 103874.85938 - acc: 0.1785 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 993  | total loss: \u001b[1m\u001b[32m103566.06250\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 032 | loss: 103566.06250 - acc: 0.1794 -- iter: 016/497\n",
            "Training Step: 994  | total loss: \u001b[1m\u001b[32m102962.65625\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 032 | loss: 102962.65625 - acc: 0.1865 -- iter: 032/497\n",
            "Training Step: 995  | total loss: \u001b[1m\u001b[32m103442.97656\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 032 | loss: 103442.97656 - acc: 0.1866 -- iter: 048/497\n",
            "Training Step: 996  | total loss: \u001b[1m\u001b[32m106559.56250\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 032 | loss: 106559.56250 - acc: 0.2054 -- iter: 064/497\n",
            "Training Step: 997  | total loss: \u001b[1m\u001b[32m105237.60938\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 032 | loss: 105237.60938 - acc: 0.1974 -- iter: 080/497\n",
            "Training Step: 998  | total loss: \u001b[1m\u001b[32m107988.45312\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 032 | loss: 107988.45312 - acc: 0.2089 -- iter: 096/497\n",
            "Training Step: 999  | total loss: \u001b[1m\u001b[32m108096.63281\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 032 | loss: 108096.63281 - acc: 0.2130 -- iter: 112/497\n",
            "Training Step: 1000  | total loss: \u001b[1m\u001b[32m106440.76562\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 032 | loss: 106440.76562 - acc: 0.1917 -- iter: 128/497\n",
            "Training Step: 1001  | total loss: \u001b[1m\u001b[32m108655.39062\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 032 | loss: 108655.39062 - acc: 0.2038 -- iter: 144/497\n",
            "Training Step: 1002  | total loss: \u001b[1m\u001b[32m108894.21094\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 032 | loss: 108894.21094 - acc: 0.2272 -- iter: 160/497\n",
            "Training Step: 1003  | total loss: \u001b[1m\u001b[32m112630.95312\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 032 | loss: 112630.95312 - acc: 0.2357 -- iter: 176/497\n",
            "Training Step: 1004  | total loss: \u001b[1m\u001b[32m113088.21875\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 032 | loss: 113088.21875 - acc: 0.2434 -- iter: 192/497\n",
            "Training Step: 1005  | total loss: \u001b[1m\u001b[32m111591.24219\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 032 | loss: 111591.24219 - acc: 0.2440 -- iter: 208/497\n",
            "Training Step: 1006  | total loss: \u001b[1m\u001b[32m110895.79688\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 032 | loss: 110895.79688 - acc: 0.2634 -- iter: 224/497\n",
            "Training Step: 1007  | total loss: \u001b[1m\u001b[32m109822.24219\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 032 | loss: 109822.24219 - acc: 0.2745 -- iter: 240/497\n",
            "Training Step: 1008  | total loss: \u001b[1m\u001b[32m108444.22656\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 032 | loss: 108444.22656 - acc: 0.2596 -- iter: 256/497\n",
            "Training Step: 1009  | total loss: \u001b[1m\u001b[32m107126.49219\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 032 | loss: 107126.49219 - acc: 0.2524 -- iter: 272/497\n",
            "Training Step: 1010  | total loss: \u001b[1m\u001b[32m107090.42188\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 032 | loss: 107090.42188 - acc: 0.2646 -- iter: 288/497\n",
            "Training Step: 1011  | total loss: \u001b[1m\u001b[32m107836.35156\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 032 | loss: 107836.35156 - acc: 0.2632 -- iter: 304/497\n",
            "Training Step: 1012  | total loss: \u001b[1m\u001b[32m106194.14844\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 032 | loss: 106194.14844 - acc: 0.2619 -- iter: 320/497\n",
            "Training Step: 1013  | total loss: \u001b[1m\u001b[32m102030.71875\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 032 | loss: 102030.71875 - acc: 0.2482 -- iter: 336/497\n",
            "Training Step: 1014  | total loss: \u001b[1m\u001b[32m98734.20312\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 032 | loss: 98734.20312 - acc: 0.2484 -- iter: 352/497\n",
            "Training Step: 1015  | total loss: \u001b[1m\u001b[32m98654.12500\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 032 | loss: 98654.12500 - acc: 0.2360 -- iter: 368/497\n",
            "Training Step: 1016  | total loss: \u001b[1m\u001b[32m100106.01562\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 032 | loss: 100106.01562 - acc: 0.2124 -- iter: 384/497\n",
            "Training Step: 1017  | total loss: \u001b[1m\u001b[32m99200.96875\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 032 | loss: 99200.96875 - acc: 0.2162 -- iter: 400/497\n",
            "Training Step: 1018  | total loss: \u001b[1m\u001b[32m97362.74219\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 032 | loss: 97362.74219 - acc: 0.2196 -- iter: 416/497\n",
            "Training Step: 1019  | total loss: \u001b[1m\u001b[32m97997.16406\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 032 | loss: 97997.16406 - acc: 0.2476 -- iter: 432/497\n",
            "Training Step: 1020  | total loss: \u001b[1m\u001b[32m98680.25781\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 032 | loss: 98680.25781 - acc: 0.2416 -- iter: 448/497\n",
            "Training Step: 1021  | total loss: \u001b[1m\u001b[32m99164.65625\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 032 | loss: 99164.65625 - acc: 0.2424 -- iter: 464/497\n",
            "Training Step: 1022  | total loss: \u001b[1m\u001b[32m99505.08594\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 032 | loss: 99505.08594 - acc: 0.2432 -- iter: 480/497\n",
            "Training Step: 1023  | total loss: \u001b[1m\u001b[32m103686.69531\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 032 | loss: 103686.69531 - acc: 0.2189 -- iter: 496/497\n",
            "Training Step: 1024  | total loss: \u001b[1m\u001b[32m107450.14062\u001b[0m\u001b[0m | time: 1.133s\n",
            "| Adam | epoch: 032 | loss: 107450.14062 - acc: 0.1970 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1025  | total loss: \u001b[1m\u001b[32m105340.50000\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 033 | loss: 105340.50000 - acc: 0.1835 -- iter: 016/497\n",
            "Training Step: 1026  | total loss: \u001b[1m\u001b[32m107126.76562\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 033 | loss: 107126.76562 - acc: 0.1839 -- iter: 032/497\n",
            "Training Step: 1027  | total loss: \u001b[1m\u001b[32m107320.99219\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 033 | loss: 107320.99219 - acc: 0.1968 -- iter: 048/497\n",
            "Training Step: 1028  | total loss: \u001b[1m\u001b[32m108368.17188\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 033 | loss: 108368.17188 - acc: 0.2084 -- iter: 064/497\n",
            "Training Step: 1029  | total loss: \u001b[1m\u001b[32m108922.57031\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 033 | loss: 108922.57031 - acc: 0.2063 -- iter: 080/497\n",
            "Training Step: 1030  | total loss: \u001b[1m\u001b[32m112474.90625\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 033 | loss: 112474.90625 - acc: 0.2106 -- iter: 096/497\n",
            "Training Step: 1031  | total loss: \u001b[1m\u001b[32m109973.79688\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 033 | loss: 109973.79688 - acc: 0.2083 -- iter: 112/497\n",
            "Training Step: 1032  | total loss: \u001b[1m\u001b[32m108588.64062\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 033 | loss: 108588.64062 - acc: 0.2062 -- iter: 128/497\n",
            "Training Step: 1033  | total loss: \u001b[1m\u001b[32m105815.87500\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 033 | loss: 105815.87500 - acc: 0.2044 -- iter: 144/497\n",
            "Training Step: 1034  | total loss: \u001b[1m\u001b[32m101107.73438\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 033 | loss: 101107.73438 - acc: 0.2214 -- iter: 160/497\n",
            "Training Step: 1035  | total loss: \u001b[1m\u001b[32m102209.36719\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 033 | loss: 102209.36719 - acc: 0.2305 -- iter: 176/497\n",
            "Training Step: 1036  | total loss: \u001b[1m\u001b[32m103313.62500\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 033 | loss: 103313.62500 - acc: 0.2200 -- iter: 192/497\n",
            "Training Step: 1037  | total loss: \u001b[1m\u001b[32m105550.73438\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 033 | loss: 105550.73438 - acc: 0.2292 -- iter: 208/497\n",
            "Training Step: 1038  | total loss: \u001b[1m\u001b[32m104499.21875\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 033 | loss: 104499.21875 - acc: 0.2376 -- iter: 224/497\n",
            "Training Step: 1039  | total loss: \u001b[1m\u001b[32m103737.89844\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 033 | loss: 103737.89844 - acc: 0.2513 -- iter: 240/497\n",
            "Training Step: 1040  | total loss: \u001b[1m\u001b[32m106508.30469\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 033 | loss: 106508.30469 - acc: 0.2574 -- iter: 256/497\n",
            "Training Step: 1041  | total loss: \u001b[1m\u001b[32m106299.76562\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 033 | loss: 106299.76562 - acc: 0.2379 -- iter: 272/497\n",
            "Training Step: 1042  | total loss: \u001b[1m\u001b[32m103798.44531\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 033 | loss: 103798.44531 - acc: 0.2391 -- iter: 288/497\n",
            "Training Step: 1043  | total loss: \u001b[1m\u001b[32m104652.28125\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 033 | loss: 104652.28125 - acc: 0.2465 -- iter: 304/497\n",
            "Training Step: 1044  | total loss: \u001b[1m\u001b[32m106234.65625\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 033 | loss: 106234.65625 - acc: 0.2468 -- iter: 320/497\n",
            "Training Step: 1045  | total loss: \u001b[1m\u001b[32m106633.45312\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 033 | loss: 106633.45312 - acc: 0.2596 -- iter: 336/497\n",
            "Training Step: 1046  | total loss: \u001b[1m\u001b[32m105517.82031\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 033 | loss: 105517.82031 - acc: 0.2524 -- iter: 352/497\n",
            "Training Step: 1047  | total loss: \u001b[1m\u001b[32m105461.32031\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 033 | loss: 105461.32031 - acc: 0.2647 -- iter: 368/497\n",
            "Training Step: 1048  | total loss: \u001b[1m\u001b[32m107840.79688\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 033 | loss: 107840.79688 - acc: 0.2570 -- iter: 384/497\n",
            "Training Step: 1049  | total loss: \u001b[1m\u001b[32m108122.35156\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 033 | loss: 108122.35156 - acc: 0.2375 -- iter: 400/497\n",
            "Training Step: 1050  | total loss: \u001b[1m\u001b[32m107102.43750\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 033 | loss: 107102.43750 - acc: 0.2513 -- iter: 416/497\n",
            "Training Step: 1051  | total loss: \u001b[1m\u001b[32m104045.68750\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 033 | loss: 104045.68750 - acc: 0.2574 -- iter: 432/497\n",
            "Training Step: 1052  | total loss: \u001b[1m\u001b[32m104538.23438\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 033 | loss: 104538.23438 - acc: 0.2442 -- iter: 448/497\n",
            "Training Step: 1053  | total loss: \u001b[1m\u001b[32m103768.52344\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 033 | loss: 103768.52344 - acc: 0.2572 -- iter: 464/497\n",
            "Training Step: 1054  | total loss: \u001b[1m\u001b[32m103446.65625\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 033 | loss: 103446.65625 - acc: 0.2565 -- iter: 480/497\n",
            "Training Step: 1055  | total loss: \u001b[1m\u001b[32m99532.69531\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 033 | loss: 99532.69531 - acc: 0.2371 -- iter: 496/497\n",
            "Training Step: 1056  | total loss: \u001b[1m\u001b[32m108167.73438\u001b[0m\u001b[0m | time: 1.131s\n",
            "| Adam | epoch: 033 | loss: 108167.73438 - acc: 0.3134 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1057  | total loss: \u001b[1m\u001b[32m115939.27344\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 034 | loss: 115939.27344 - acc: 0.3821 -- iter: 016/497\n",
            "Training Step: 1058  | total loss: \u001b[1m\u001b[32m115942.03125\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 034 | loss: 115942.03125 - acc: 0.3751 -- iter: 032/497\n",
            "Training Step: 1059  | total loss: \u001b[1m\u001b[32m116333.94531\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 034 | loss: 116333.94531 - acc: 0.3626 -- iter: 048/497\n",
            "Training Step: 1060  | total loss: \u001b[1m\u001b[32m114127.56250\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 034 | loss: 114127.56250 - acc: 0.3513 -- iter: 064/497\n",
            "Training Step: 1061  | total loss: \u001b[1m\u001b[32m117394.92188\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 034 | loss: 117394.92188 - acc: 0.3350 -- iter: 080/497\n",
            "Training Step: 1062  | total loss: \u001b[1m\u001b[32m116800.27344\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 034 | loss: 116800.27344 - acc: 0.3390 -- iter: 096/497\n",
            "Training Step: 1063  | total loss: \u001b[1m\u001b[32m116998.20312\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 034 | loss: 116998.20312 - acc: 0.3363 -- iter: 112/497\n",
            "Training Step: 1064  | total loss: \u001b[1m\u001b[32m117419.66406\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 034 | loss: 117419.66406 - acc: 0.3277 -- iter: 128/497\n",
            "Training Step: 1065  | total loss: \u001b[1m\u001b[32m115525.85156\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 034 | loss: 115525.85156 - acc: 0.3199 -- iter: 144/497\n",
            "Training Step: 1066  | total loss: \u001b[1m\u001b[32m111808.18750\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 034 | loss: 111808.18750 - acc: 0.3067 -- iter: 160/497\n",
            "Training Step: 1067  | total loss: \u001b[1m\u001b[32m111087.89844\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 034 | loss: 111087.89844 - acc: 0.2885 -- iter: 176/497\n",
            "Training Step: 1068  | total loss: \u001b[1m\u001b[32m111754.41406\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 034 | loss: 111754.41406 - acc: 0.2972 -- iter: 192/497\n",
            "Training Step: 1069  | total loss: \u001b[1m\u001b[32m110321.32812\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 034 | loss: 110321.32812 - acc: 0.3049 -- iter: 208/497\n",
            "Training Step: 1070  | total loss: \u001b[1m\u001b[32m105539.96094\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 034 | loss: 105539.96094 - acc: 0.2807 -- iter: 224/497\n",
            "Training Step: 1071  | total loss: \u001b[1m\u001b[32m105956.31250\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 034 | loss: 105956.31250 - acc: 0.2714 -- iter: 240/497\n",
            "Training Step: 1072  | total loss: \u001b[1m\u001b[32m107713.96094\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 034 | loss: 107713.96094 - acc: 0.2567 -- iter: 256/497\n",
            "Training Step: 1073  | total loss: \u001b[1m\u001b[32m107490.64844\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 034 | loss: 107490.64844 - acc: 0.2498 -- iter: 272/497\n",
            "Training Step: 1074  | total loss: \u001b[1m\u001b[32m108691.89844\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 034 | loss: 108691.89844 - acc: 0.2373 -- iter: 288/497\n",
            "Training Step: 1075  | total loss: \u001b[1m\u001b[32m108437.75781\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 034 | loss: 108437.75781 - acc: 0.2323 -- iter: 304/497\n",
            "Training Step: 1076  | total loss: \u001b[1m\u001b[32m103565.70312\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 034 | loss: 103565.70312 - acc: 0.2279 -- iter: 320/497\n",
            "Training Step: 1077  | total loss: \u001b[1m\u001b[32m103517.27344\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 034 | loss: 103517.27344 - acc: 0.2176 -- iter: 336/497\n",
            "Training Step: 1078  | total loss: \u001b[1m\u001b[32m102709.83594\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 034 | loss: 102709.83594 - acc: 0.1958 -- iter: 352/497\n",
            "Training Step: 1079  | total loss: \u001b[1m\u001b[32m102491.20312\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 034 | loss: 102491.20312 - acc: 0.2012 -- iter: 368/497\n",
            "Training Step: 1080  | total loss: \u001b[1m\u001b[32m101752.03125\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 034 | loss: 101752.03125 - acc: 0.2186 -- iter: 384/497\n",
            "Training Step: 1081  | total loss: \u001b[1m\u001b[32m102441.71875\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 034 | loss: 102441.71875 - acc: 0.2280 -- iter: 400/497\n",
            "Training Step: 1082  | total loss: \u001b[1m\u001b[32m102116.71094\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 034 | loss: 102116.71094 - acc: 0.2427 -- iter: 416/497\n",
            "Training Step: 1083  | total loss: \u001b[1m\u001b[32m101959.59375\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 034 | loss: 101959.59375 - acc: 0.2497 -- iter: 432/497\n",
            "Training Step: 1084  | total loss: \u001b[1m\u001b[32m100921.11719\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 034 | loss: 100921.11719 - acc: 0.2435 -- iter: 448/497\n",
            "Training Step: 1085  | total loss: \u001b[1m\u001b[32m101783.22656\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 034 | loss: 101783.22656 - acc: 0.2629 -- iter: 464/497\n",
            "Training Step: 1086  | total loss: \u001b[1m\u001b[32m100952.46094\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 034 | loss: 100952.46094 - acc: 0.2741 -- iter: 480/497\n",
            "Training Step: 1087  | total loss: \u001b[1m\u001b[32m100079.89062\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 034 | loss: 100079.89062 - acc: 0.2779 -- iter: 496/497\n",
            "Training Step: 1088  | total loss: \u001b[1m\u001b[32m100655.47656\u001b[0m\u001b[0m | time: 1.129s\n",
            "| Adam | epoch: 034 | loss: 100655.47656 - acc: 0.2751 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1089  | total loss: \u001b[1m\u001b[32m108954.42969\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 035 | loss: 108954.42969 - acc: 0.2476 -- iter: 016/497\n",
            "Training Step: 1090  | total loss: \u001b[1m\u001b[32m116423.48438\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 035 | loss: 116423.48438 - acc: 0.2229 -- iter: 032/497\n",
            "Training Step: 1091  | total loss: \u001b[1m\u001b[32m116858.58594\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 035 | loss: 116858.58594 - acc: 0.2068 -- iter: 048/497\n",
            "Training Step: 1092  | total loss: \u001b[1m\u001b[32m112133.31250\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 035 | loss: 112133.31250 - acc: 0.2174 -- iter: 064/497\n",
            "Training Step: 1093  | total loss: \u001b[1m\u001b[32m112324.60938\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 035 | loss: 112324.60938 - acc: 0.2269 -- iter: 080/497\n",
            "Training Step: 1094  | total loss: \u001b[1m\u001b[32m109154.56250\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 035 | loss: 109154.56250 - acc: 0.2355 -- iter: 096/497\n",
            "Training Step: 1095  | total loss: \u001b[1m\u001b[32m108502.27344\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 035 | loss: 108502.27344 - acc: 0.2369 -- iter: 112/497\n",
            "Training Step: 1096  | total loss: \u001b[1m\u001b[32m108566.21094\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 035 | loss: 108566.21094 - acc: 0.2507 -- iter: 128/497\n",
            "Training Step: 1097  | total loss: \u001b[1m\u001b[32m108227.48438\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 035 | loss: 108227.48438 - acc: 0.2506 -- iter: 144/497\n",
            "Training Step: 1098  | total loss: \u001b[1m\u001b[32m105418.21875\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 035 | loss: 105418.21875 - acc: 0.2568 -- iter: 160/497\n",
            "Training Step: 1099  | total loss: \u001b[1m\u001b[32m103439.17969\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 035 | loss: 103439.17969 - acc: 0.2499 -- iter: 176/497\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m104152.95312\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 035 | loss: 104152.95312 - acc: 0.2374 -- iter: 192/497\n",
            "Training Step: 1101  | total loss: \u001b[1m\u001b[32m105115.52344\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 035 | loss: 105115.52344 - acc: 0.2387 -- iter: 208/497\n",
            "Training Step: 1102  | total loss: \u001b[1m\u001b[32m103109.86719\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 035 | loss: 103109.86719 - acc: 0.2461 -- iter: 224/497\n",
            "Training Step: 1103  | total loss: \u001b[1m\u001b[32m101192.33594\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 035 | loss: 101192.33594 - acc: 0.2464 -- iter: 240/497\n",
            "Training Step: 1104  | total loss: \u001b[1m\u001b[32m100860.15625\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 035 | loss: 100860.15625 - acc: 0.2281 -- iter: 256/497\n",
            "Training Step: 1105  | total loss: \u001b[1m\u001b[32m101168.00781\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 035 | loss: 101168.00781 - acc: 0.2302 -- iter: 272/497\n",
            "Training Step: 1106  | total loss: \u001b[1m\u001b[32m101311.57031\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 035 | loss: 101311.57031 - acc: 0.2385 -- iter: 288/497\n",
            "Training Step: 1107  | total loss: \u001b[1m\u001b[32m101873.24219\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 035 | loss: 101873.24219 - acc: 0.2396 -- iter: 304/497\n",
            "Training Step: 1108  | total loss: \u001b[1m\u001b[32m101005.65625\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 035 | loss: 101005.65625 - acc: 0.2282 -- iter: 320/497\n",
            "Training Step: 1109  | total loss: \u001b[1m\u001b[32m103971.19531\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 035 | loss: 103971.19531 - acc: 0.2491 -- iter: 336/497\n",
            "Training Step: 1110  | total loss: \u001b[1m\u001b[32m104088.67188\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 035 | loss: 104088.67188 - acc: 0.2429 -- iter: 352/497\n",
            "Training Step: 1111  | total loss: \u001b[1m\u001b[32m105986.77344\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 035 | loss: 105986.77344 - acc: 0.2436 -- iter: 368/497\n",
            "Training Step: 1112  | total loss: \u001b[1m\u001b[32m106384.25000\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 035 | loss: 106384.25000 - acc: 0.2505 -- iter: 384/497\n",
            "Training Step: 1113  | total loss: \u001b[1m\u001b[32m103555.39844\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 035 | loss: 103555.39844 - acc: 0.2567 -- iter: 400/497\n",
            "Training Step: 1114  | total loss: \u001b[1m\u001b[32m100825.60156\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 035 | loss: 100825.60156 - acc: 0.2623 -- iter: 416/497\n",
            "Training Step: 1115  | total loss: \u001b[1m\u001b[32m102961.11719\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 035 | loss: 102961.11719 - acc: 0.2548 -- iter: 432/497\n",
            "Training Step: 1116  | total loss: \u001b[1m\u001b[32m105377.30469\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 035 | loss: 105377.30469 - acc: 0.2481 -- iter: 448/497\n",
            "Training Step: 1117  | total loss: \u001b[1m\u001b[32m106289.17969\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 035 | loss: 106289.17969 - acc: 0.2545 -- iter: 464/497\n",
            "Training Step: 1118  | total loss: \u001b[1m\u001b[32m106630.21094\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 035 | loss: 106630.21094 - acc: 0.2478 -- iter: 480/497\n",
            "Training Step: 1119  | total loss: \u001b[1m\u001b[32m109028.18750\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 035 | loss: 109028.18750 - acc: 0.2480 -- iter: 496/497\n",
            "Training Step: 1120  | total loss: \u001b[1m\u001b[32m111456.96094\u001b[0m\u001b[0m | time: 1.128s\n",
            "| Adam | epoch: 035 | loss: 111456.96094 - acc: 0.2482 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1121  | total loss: \u001b[1m\u001b[32m109384.91406\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 036 | loss: 109384.91406 - acc: 0.2359 -- iter: 016/497\n",
            "Training Step: 1122  | total loss: \u001b[1m\u001b[32m100379.67188\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 036 | loss: 100379.67188 - acc: 0.2123 -- iter: 032/497\n",
            "Training Step: 1123  | total loss: \u001b[1m\u001b[32m92274.95312\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 036 | loss: 92274.95312 - acc: 0.1911 -- iter: 048/497\n",
            "Training Step: 1124  | total loss: \u001b[1m\u001b[32m93786.03906\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 036 | loss: 93786.03906 - acc: 0.1782 -- iter: 064/497\n",
            "Training Step: 1125  | total loss: \u001b[1m\u001b[32m92340.90625\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 036 | loss: 92340.90625 - acc: 0.1854 -- iter: 080/497\n",
            "Training Step: 1126  | total loss: \u001b[1m\u001b[32m94705.28125\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 036 | loss: 94705.28125 - acc: 0.1981 -- iter: 096/497\n",
            "Training Step: 1127  | total loss: \u001b[1m\u001b[32m93088.96875\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 036 | loss: 93088.96875 - acc: 0.2158 -- iter: 112/497\n",
            "Training Step: 1128  | total loss: \u001b[1m\u001b[32m96652.00000\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 036 | loss: 96652.00000 - acc: 0.2255 -- iter: 128/497\n",
            "Training Step: 1129  | total loss: \u001b[1m\u001b[32m101790.92969\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 036 | loss: 101790.92969 - acc: 0.2342 -- iter: 144/497\n",
            "Training Step: 1130  | total loss: \u001b[1m\u001b[32m99522.53906\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 036 | loss: 99522.53906 - acc: 0.2233 -- iter: 160/497\n",
            "Training Step: 1131  | total loss: \u001b[1m\u001b[32m100487.47656\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 036 | loss: 100487.47656 - acc: 0.2259 -- iter: 176/497\n",
            "Training Step: 1132  | total loss: \u001b[1m\u001b[32m105794.39844\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 036 | loss: 105794.39844 - acc: 0.2408 -- iter: 192/497\n",
            "Training Step: 1133  | total loss: \u001b[1m\u001b[32m102857.68750\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 036 | loss: 102857.68750 - acc: 0.2355 -- iter: 208/497\n",
            "Training Step: 1134  | total loss: \u001b[1m\u001b[32m106339.09375\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 036 | loss: 106339.09375 - acc: 0.2307 -- iter: 224/497\n",
            "Training Step: 1135  | total loss: \u001b[1m\u001b[32m107208.53906\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 036 | loss: 107208.53906 - acc: 0.2264 -- iter: 240/497\n",
            "Training Step: 1136  | total loss: \u001b[1m\u001b[32m106285.07031\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 036 | loss: 106285.07031 - acc: 0.2162 -- iter: 256/497\n",
            "Training Step: 1137  | total loss: \u001b[1m\u001b[32m105902.82031\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 036 | loss: 105902.82031 - acc: 0.2321 -- iter: 272/497\n",
            "Training Step: 1138  | total loss: \u001b[1m\u001b[32m104054.82031\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 036 | loss: 104054.82031 - acc: 0.2277 -- iter: 288/497\n",
            "Training Step: 1139  | total loss: \u001b[1m\u001b[32m101931.25781\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 036 | loss: 101931.25781 - acc: 0.2424 -- iter: 304/497\n",
            "Training Step: 1140  | total loss: \u001b[1m\u001b[32m98915.90625\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 036 | loss: 98915.90625 - acc: 0.2557 -- iter: 320/497\n",
            "Training Step: 1141  | total loss: \u001b[1m\u001b[32m100106.07031\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 036 | loss: 100106.07031 - acc: 0.2426 -- iter: 336/497\n",
            "Training Step: 1142  | total loss: \u001b[1m\u001b[32m101329.31250\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 036 | loss: 101329.31250 - acc: 0.2496 -- iter: 352/497\n",
            "Training Step: 1143  | total loss: \u001b[1m\u001b[32m98732.65625\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 036 | loss: 98732.65625 - acc: 0.2496 -- iter: 368/497\n",
            "Training Step: 1144  | total loss: \u001b[1m\u001b[32m97252.29688\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 036 | loss: 97252.29688 - acc: 0.2497 -- iter: 384/497\n",
            "Training Step: 1145  | total loss: \u001b[1m\u001b[32m99815.37500\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 036 | loss: 99815.37500 - acc: 0.2497 -- iter: 400/497\n",
            "Training Step: 1146  | total loss: \u001b[1m\u001b[32m97371.39062\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 036 | loss: 97371.39062 - acc: 0.2435 -- iter: 416/497\n",
            "Training Step: 1147  | total loss: \u001b[1m\u001b[32m96718.16406\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 036 | loss: 96718.16406 - acc: 0.2441 -- iter: 432/497\n",
            "Training Step: 1148  | total loss: \u001b[1m\u001b[32m96132.08594\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 036 | loss: 96132.08594 - acc: 0.2385 -- iter: 448/497\n",
            "Training Step: 1149  | total loss: \u001b[1m\u001b[32m98517.57812\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 036 | loss: 98517.57812 - acc: 0.2459 -- iter: 464/497\n",
            "Training Step: 1150  | total loss: \u001b[1m\u001b[32m100928.67969\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 036 | loss: 100928.67969 - acc: 0.2400 -- iter: 480/497\n",
            "Training Step: 1151  | total loss: \u001b[1m\u001b[32m105350.63281\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 036 | loss: 105350.63281 - acc: 0.2348 -- iter: 496/497\n",
            "Training Step: 1152  | total loss: \u001b[1m\u001b[32m104731.67969\u001b[0m\u001b[0m | time: 1.137s\n",
            "| Adam | epoch: 036 | loss: 104731.67969 - acc: 0.2301 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1153  | total loss: \u001b[1m\u001b[32m104090.13281\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 037 | loss: 104090.13281 - acc: 0.2383 -- iter: 016/497\n",
            "Training Step: 1154  | total loss: \u001b[1m\u001b[32m103214.85938\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 037 | loss: 103214.85938 - acc: 0.2395 -- iter: 032/497\n",
            "Training Step: 1155  | total loss: \u001b[1m\u001b[32m98974.96094\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 037 | loss: 98974.96094 - acc: 0.2155 -- iter: 048/497\n",
            "Training Step: 1156  | total loss: \u001b[1m\u001b[32m95159.05469\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 037 | loss: 95159.05469 - acc: 0.1940 -- iter: 064/497\n",
            "Training Step: 1157  | total loss: \u001b[1m\u001b[32m93614.72656\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 037 | loss: 93614.72656 - acc: 0.1933 -- iter: 080/497\n",
            "Training Step: 1158  | total loss: \u001b[1m\u001b[32m94528.14062\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 037 | loss: 94528.14062 - acc: 0.1990 -- iter: 096/497\n",
            "Training Step: 1159  | total loss: \u001b[1m\u001b[32m94466.32031\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 037 | loss: 94466.32031 - acc: 0.2041 -- iter: 112/497\n",
            "Training Step: 1160  | total loss: \u001b[1m\u001b[32m95214.72656\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 037 | loss: 95214.72656 - acc: 0.2212 -- iter: 128/497\n",
            "Training Step: 1161  | total loss: \u001b[1m\u001b[32m98210.03125\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 037 | loss: 98210.03125 - acc: 0.2178 -- iter: 144/497\n",
            "Training Step: 1162  | total loss: \u001b[1m\u001b[32m99938.96875\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 037 | loss: 99938.96875 - acc: 0.2148 -- iter: 160/497\n",
            "Training Step: 1163  | total loss: \u001b[1m\u001b[32m102541.07812\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 037 | loss: 102541.07812 - acc: 0.2183 -- iter: 176/497\n",
            "Training Step: 1164  | total loss: \u001b[1m\u001b[32m101839.80469\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 037 | loss: 101839.80469 - acc: 0.2215 -- iter: 192/497\n",
            "Training Step: 1165  | total loss: \u001b[1m\u001b[32m102432.51562\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 037 | loss: 102432.51562 - acc: 0.2181 -- iter: 208/497\n",
            "Training Step: 1166  | total loss: \u001b[1m\u001b[32m101170.31250\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 037 | loss: 101170.31250 - acc: 0.2213 -- iter: 224/497\n",
            "Training Step: 1167  | total loss: \u001b[1m\u001b[32m102344.84375\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 037 | loss: 102344.84375 - acc: 0.2241 -- iter: 240/497\n",
            "Training Step: 1168  | total loss: \u001b[1m\u001b[32m103469.57031\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 037 | loss: 103469.57031 - acc: 0.2330 -- iter: 256/497\n",
            "Training Step: 1169  | total loss: \u001b[1m\u001b[32m106665.29688\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 037 | loss: 106665.29688 - acc: 0.2472 -- iter: 272/497\n",
            "Training Step: 1170  | total loss: \u001b[1m\u001b[32m104241.20312\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 037 | loss: 104241.20312 - acc: 0.2412 -- iter: 288/497\n",
            "Training Step: 1171  | total loss: \u001b[1m\u001b[32m103023.45312\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 037 | loss: 103023.45312 - acc: 0.2546 -- iter: 304/497\n",
            "Training Step: 1172  | total loss: \u001b[1m\u001b[32m101917.64844\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 037 | loss: 101917.64844 - acc: 0.2541 -- iter: 320/497\n",
            "Training Step: 1173  | total loss: \u001b[1m\u001b[32m102236.83594\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 037 | loss: 102236.83594 - acc: 0.2537 -- iter: 336/497\n",
            "Training Step: 1174  | total loss: \u001b[1m\u001b[32m103854.42969\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 037 | loss: 103854.42969 - acc: 0.2533 -- iter: 352/497\n",
            "Training Step: 1175  | total loss: \u001b[1m\u001b[32m106628.17188\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 037 | loss: 106628.17188 - acc: 0.2530 -- iter: 368/497\n",
            "Training Step: 1176  | total loss: \u001b[1m\u001b[32m105254.36719\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 037 | loss: 105254.36719 - acc: 0.2527 -- iter: 384/497\n",
            "Training Step: 1177  | total loss: \u001b[1m\u001b[32m107083.95312\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 037 | loss: 107083.95312 - acc: 0.2524 -- iter: 400/497\n",
            "Training Step: 1178  | total loss: \u001b[1m\u001b[32m103580.50000\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 037 | loss: 103580.50000 - acc: 0.2522 -- iter: 416/497\n",
            "Training Step: 1179  | total loss: \u001b[1m\u001b[32m106591.90625\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 037 | loss: 106591.90625 - acc: 0.2457 -- iter: 432/497\n",
            "Training Step: 1180  | total loss: \u001b[1m\u001b[32m108402.35156\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 037 | loss: 108402.35156 - acc: 0.2399 -- iter: 448/497\n",
            "Training Step: 1181  | total loss: \u001b[1m\u001b[32m108727.29688\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 037 | loss: 108727.29688 - acc: 0.2409 -- iter: 464/497\n",
            "Training Step: 1182  | total loss: \u001b[1m\u001b[32m107317.67188\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 037 | loss: 107317.67188 - acc: 0.2356 -- iter: 480/497\n",
            "Training Step: 1183  | total loss: \u001b[1m\u001b[32m103015.17969\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 037 | loss: 103015.17969 - acc: 0.2495 -- iter: 496/497\n",
            "Training Step: 1184  | total loss: \u001b[1m\u001b[32m101199.43750\u001b[0m\u001b[0m | time: 1.122s\n",
            "| Adam | epoch: 037 | loss: 101199.43750 - acc: 0.2433 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1185  | total loss: \u001b[1m\u001b[32m99615.07812\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 038 | loss: 99615.07812 - acc: 0.2377 -- iter: 016/497\n",
            "Training Step: 1186  | total loss: \u001b[1m\u001b[32m98169.14844\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 038 | loss: 98169.14844 - acc: 0.2202 -- iter: 032/497\n",
            "Training Step: 1187  | total loss: \u001b[1m\u001b[32m99577.96875\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 038 | loss: 99577.96875 - acc: 0.2294 -- iter: 048/497\n",
            "Training Step: 1188  | total loss: \u001b[1m\u001b[32m108134.79688\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 038 | loss: 108134.79688 - acc: 0.3065 -- iter: 064/497\n",
            "Training Step: 1189  | total loss: \u001b[1m\u001b[32m115835.94531\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 038 | loss: 115835.94531 - acc: 0.3758 -- iter: 080/497\n",
            "Training Step: 1190  | total loss: \u001b[1m\u001b[32m112886.14062\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 038 | loss: 112886.14062 - acc: 0.3570 -- iter: 096/497\n",
            "Training Step: 1191  | total loss: \u001b[1m\u001b[32m115979.75000\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 038 | loss: 115979.75000 - acc: 0.3401 -- iter: 112/497\n",
            "Training Step: 1192  | total loss: \u001b[1m\u001b[32m116426.06250\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 038 | loss: 116426.06250 - acc: 0.3123 -- iter: 128/497\n",
            "Training Step: 1193  | total loss: \u001b[1m\u001b[32m109656.94531\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 038 | loss: 109656.94531 - acc: 0.3061 -- iter: 144/497\n",
            "Training Step: 1194  | total loss: \u001b[1m\u001b[32m109134.04688\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 038 | loss: 109134.04688 - acc: 0.3005 -- iter: 160/497\n",
            "Training Step: 1195  | total loss: \u001b[1m\u001b[32m106211.50000\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 038 | loss: 106211.50000 - acc: 0.3142 -- iter: 176/497\n",
            "Training Step: 1196  | total loss: \u001b[1m\u001b[32m107550.25000\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 038 | loss: 107550.25000 - acc: 0.3203 -- iter: 192/497\n",
            "Training Step: 1197  | total loss: \u001b[1m\u001b[32m106976.27344\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 038 | loss: 106976.27344 - acc: 0.3195 -- iter: 208/497\n",
            "Training Step: 1198  | total loss: \u001b[1m\u001b[32m107978.89844\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 038 | loss: 107978.89844 - acc: 0.3063 -- iter: 224/497\n",
            "Training Step: 1199  | total loss: \u001b[1m\u001b[32m103277.25781\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 038 | loss: 103277.25781 - acc: 0.3069 -- iter: 240/497\n",
            "Training Step: 1200  | total loss: \u001b[1m\u001b[32m105445.87500\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 038 | loss: 105445.87500 - acc: 0.2887 -- iter: 256/497\n",
            "Training Step: 1201  | total loss: \u001b[1m\u001b[32m108033.32031\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 038 | loss: 108033.32031 - acc: 0.2786 -- iter: 272/497\n",
            "Training Step: 1202  | total loss: \u001b[1m\u001b[32m107013.03906\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 038 | loss: 107013.03906 - acc: 0.2570 -- iter: 288/497\n",
            "Training Step: 1203  | total loss: \u001b[1m\u001b[32m107239.67188\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 038 | loss: 107239.67188 - acc: 0.2688 -- iter: 304/497\n",
            "Training Step: 1204  | total loss: \u001b[1m\u001b[32m107631.56250\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 038 | loss: 107631.56250 - acc: 0.2732 -- iter: 320/497\n",
            "Training Step: 1205  | total loss: \u001b[1m\u001b[32m107853.35938\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 038 | loss: 107853.35938 - acc: 0.2521 -- iter: 336/497\n",
            "Training Step: 1206  | total loss: \u001b[1m\u001b[32m107896.71875\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 038 | loss: 107896.71875 - acc: 0.2644 -- iter: 352/497\n",
            "Training Step: 1207  | total loss: \u001b[1m\u001b[32m108400.64062\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 038 | loss: 108400.64062 - acc: 0.2692 -- iter: 368/497\n",
            "Training Step: 1208  | total loss: \u001b[1m\u001b[32m108854.45312\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 038 | loss: 108854.45312 - acc: 0.2610 -- iter: 384/497\n",
            "Training Step: 1209  | total loss: \u001b[1m\u001b[32m106665.60938\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 038 | loss: 106665.60938 - acc: 0.2599 -- iter: 400/497\n",
            "Training Step: 1210  | total loss: \u001b[1m\u001b[32m106664.40625\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 038 | loss: 106664.40625 - acc: 0.2527 -- iter: 416/497\n",
            "Training Step: 1211  | total loss: \u001b[1m\u001b[32m103617.89844\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 038 | loss: 103617.89844 - acc: 0.2462 -- iter: 432/497\n",
            "Training Step: 1212  | total loss: \u001b[1m\u001b[32m104287.59375\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 038 | loss: 104287.59375 - acc: 0.2778 -- iter: 448/497\n",
            "Training Step: 1213  | total loss: \u001b[1m\u001b[32m104438.45312\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 038 | loss: 104438.45312 - acc: 0.2688 -- iter: 464/497\n",
            "Training Step: 1214  | total loss: \u001b[1m\u001b[32m102212.39062\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 038 | loss: 102212.39062 - acc: 0.2731 -- iter: 480/497\n",
            "Training Step: 1215  | total loss: \u001b[1m\u001b[32m101763.14844\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 038 | loss: 101763.14844 - acc: 0.2583 -- iter: 496/497\n",
            "Training Step: 1216  | total loss: \u001b[1m\u001b[32m103091.09375\u001b[0m\u001b[0m | time: 1.135s\n",
            "| Adam | epoch: 038 | loss: 103091.09375 - acc: 0.2575 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1217  | total loss: \u001b[1m\u001b[32m105202.08594\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 039 | loss: 105202.08594 - acc: 0.2442 -- iter: 016/497\n",
            "Training Step: 1218  | total loss: \u001b[1m\u001b[32m106996.66406\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 039 | loss: 106996.66406 - acc: 0.2323 -- iter: 032/497\n",
            "Training Step: 1219  | total loss: \u001b[1m\u001b[32m104371.44531\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 039 | loss: 104371.44531 - acc: 0.2528 -- iter: 048/497\n",
            "Training Step: 1220  | total loss: \u001b[1m\u001b[32m104628.58594\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 039 | loss: 104628.58594 - acc: 0.2588 -- iter: 064/497\n",
            "Training Step: 1221  | total loss: \u001b[1m\u001b[32m95278.79688\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 039 | loss: 95278.79688 - acc: 0.2329 -- iter: 080/497\n",
            "Training Step: 1222  | total loss: \u001b[1m\u001b[32m86863.98438\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 039 | loss: 86863.98438 - acc: 0.2096 -- iter: 096/497\n",
            "Training Step: 1223  | total loss: \u001b[1m\u001b[32m85995.81250\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 039 | loss: 85995.81250 - acc: 0.2074 -- iter: 112/497\n",
            "Training Step: 1224  | total loss: \u001b[1m\u001b[32m85727.97656\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 039 | loss: 85727.97656 - acc: 0.2179 -- iter: 128/497\n",
            "Training Step: 1225  | total loss: \u001b[1m\u001b[32m87584.89844\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 039 | loss: 87584.89844 - acc: 0.2149 -- iter: 144/497\n",
            "Training Step: 1226  | total loss: \u001b[1m\u001b[32m90831.12500\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 039 | loss: 90831.12500 - acc: 0.2121 -- iter: 160/497\n",
            "Training Step: 1227  | total loss: \u001b[1m\u001b[32m90440.77344\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 039 | loss: 90440.77344 - acc: 0.2097 -- iter: 176/497\n",
            "Training Step: 1228  | total loss: \u001b[1m\u001b[32m91533.75781\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 039 | loss: 91533.75781 - acc: 0.2200 -- iter: 192/497\n",
            "Training Step: 1229  | total loss: \u001b[1m\u001b[32m93419.21875\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 039 | loss: 93419.21875 - acc: 0.2230 -- iter: 208/497\n",
            "Training Step: 1230  | total loss: \u001b[1m\u001b[32m97355.82812\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 039 | loss: 97355.82812 - acc: 0.2444 -- iter: 224/497\n",
            "Training Step: 1231  | total loss: \u001b[1m\u001b[32m98198.10156\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 039 | loss: 98198.10156 - acc: 0.2387 -- iter: 240/497\n",
            "Training Step: 1232  | total loss: \u001b[1m\u001b[32m96959.89844\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 039 | loss: 96959.89844 - acc: 0.2211 -- iter: 256/497\n",
            "Training Step: 1233  | total loss: \u001b[1m\u001b[32m99406.95312\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 039 | loss: 99406.95312 - acc: 0.2302 -- iter: 272/497\n",
            "Training Step: 1234  | total loss: \u001b[1m\u001b[32m99587.82812\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 039 | loss: 99587.82812 - acc: 0.2510 -- iter: 288/497\n",
            "Training Step: 1235  | total loss: \u001b[1m\u001b[32m100935.41406\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 039 | loss: 100935.41406 - acc: 0.2696 -- iter: 304/497\n",
            "Training Step: 1236  | total loss: \u001b[1m\u001b[32m103457.80469\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 039 | loss: 103457.80469 - acc: 0.2552 -- iter: 320/497\n",
            "Training Step: 1237  | total loss: \u001b[1m\u001b[32m102368.99219\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 039 | loss: 102368.99219 - acc: 0.2546 -- iter: 336/497\n",
            "Training Step: 1238  | total loss: \u001b[1m\u001b[32m103333.47656\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 039 | loss: 103333.47656 - acc: 0.2542 -- iter: 352/497\n",
            "Training Step: 1239  | total loss: \u001b[1m\u001b[32m99932.42188\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 039 | loss: 99932.42188 - acc: 0.2538 -- iter: 368/497\n",
            "Training Step: 1240  | total loss: \u001b[1m\u001b[32m100035.62500\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 039 | loss: 100035.62500 - acc: 0.2659 -- iter: 384/497\n",
            "Training Step: 1241  | total loss: \u001b[1m\u001b[32m103130.99219\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 039 | loss: 103130.99219 - acc: 0.2768 -- iter: 400/497\n",
            "Training Step: 1242  | total loss: \u001b[1m\u001b[32m103622.34375\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 039 | loss: 103622.34375 - acc: 0.2554 -- iter: 416/497\n",
            "Training Step: 1243  | total loss: \u001b[1m\u001b[32m104581.92188\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 039 | loss: 104581.92188 - acc: 0.2611 -- iter: 432/497\n",
            "Training Step: 1244  | total loss: \u001b[1m\u001b[32m106007.35938\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 039 | loss: 106007.35938 - acc: 0.2537 -- iter: 448/497\n",
            "Training Step: 1245  | total loss: \u001b[1m\u001b[32m105387.10938\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 039 | loss: 105387.10938 - acc: 0.2409 -- iter: 464/497\n",
            "Training Step: 1246  | total loss: \u001b[1m\u001b[32m103658.35938\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 039 | loss: 103658.35938 - acc: 0.2355 -- iter: 480/497\n",
            "Training Step: 1247  | total loss: \u001b[1m\u001b[32m101374.02344\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 039 | loss: 101374.02344 - acc: 0.2307 -- iter: 496/497\n",
            "Training Step: 1248  | total loss: \u001b[1m\u001b[32m101322.80469\u001b[0m\u001b[0m | time: 1.123s\n",
            "| Adam | epoch: 039 | loss: 101322.80469 - acc: 0.2264 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1249  | total loss: \u001b[1m\u001b[32m100876.52344\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 040 | loss: 100876.52344 - acc: 0.2350 -- iter: 016/497\n",
            "Training Step: 1250  | total loss: \u001b[1m\u001b[32m100516.24219\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 040 | loss: 100516.24219 - acc: 0.2303 -- iter: 032/497\n",
            "Training Step: 1251  | total loss: \u001b[1m\u001b[32m102957.81250\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 040 | loss: 102957.81250 - acc: 0.2260 -- iter: 048/497\n",
            "Training Step: 1252  | total loss: \u001b[1m\u001b[32m102966.12500\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 040 | loss: 102966.12500 - acc: 0.2346 -- iter: 064/497\n",
            "Training Step: 1253  | total loss: \u001b[1m\u001b[32m102487.70312\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 040 | loss: 102487.70312 - acc: 0.2299 -- iter: 080/497\n",
            "Training Step: 1254  | total loss: \u001b[1m\u001b[32m110808.59375\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 040 | loss: 110808.59375 - acc: 0.3069 -- iter: 096/497\n",
            "Training Step: 1255  | total loss: \u001b[1m\u001b[32m118297.39844\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 040 | loss: 118297.39844 - acc: 0.3762 -- iter: 112/497\n",
            "Training Step: 1256  | total loss: \u001b[1m\u001b[32m119827.34375\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 040 | loss: 119827.34375 - acc: 0.3699 -- iter: 128/497\n",
            "Training Step: 1257  | total loss: \u001b[1m\u001b[32m115501.72656\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 040 | loss: 115501.72656 - acc: 0.3579 -- iter: 144/497\n",
            "Training Step: 1258  | total loss: \u001b[1m\u001b[32m113549.14844\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 040 | loss: 113549.14844 - acc: 0.3408 -- iter: 160/497\n",
            "Training Step: 1259  | total loss: \u001b[1m\u001b[32m109959.32812\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 040 | loss: 109959.32812 - acc: 0.3193 -- iter: 176/497\n",
            "Training Step: 1260  | total loss: \u001b[1m\u001b[32m111501.32812\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 040 | loss: 111501.32812 - acc: 0.3373 -- iter: 192/497\n",
            "Training Step: 1261  | total loss: \u001b[1m\u001b[32m109234.46875\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 040 | loss: 109234.46875 - acc: 0.3223 -- iter: 208/497\n",
            "Training Step: 1262  | total loss: \u001b[1m\u001b[32m108422.79688\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 040 | loss: 108422.79688 - acc: 0.3089 -- iter: 224/497\n",
            "Training Step: 1263  | total loss: \u001b[1m\u001b[32m107348.08594\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 040 | loss: 107348.08594 - acc: 0.2967 -- iter: 240/497\n",
            "Training Step: 1264  | total loss: \u001b[1m\u001b[32m110104.21094\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 040 | loss: 110104.21094 - acc: 0.2733 -- iter: 256/497\n",
            "Training Step: 1265  | total loss: \u001b[1m\u001b[32m110473.40625\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 040 | loss: 110473.40625 - acc: 0.2585 -- iter: 272/497\n",
            "Training Step: 1266  | total loss: \u001b[1m\u001b[32m108307.26562\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 040 | loss: 108307.26562 - acc: 0.2514 -- iter: 288/497\n",
            "Training Step: 1267  | total loss: \u001b[1m\u001b[32m109462.54688\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 040 | loss: 109462.54688 - acc: 0.2387 -- iter: 304/497\n",
            "Training Step: 1268  | total loss: \u001b[1m\u001b[32m107422.92188\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 040 | loss: 107422.92188 - acc: 0.2336 -- iter: 320/497\n",
            "Training Step: 1269  | total loss: \u001b[1m\u001b[32m108578.87500\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 040 | loss: 108578.87500 - acc: 0.2353 -- iter: 336/497\n",
            "Training Step: 1270  | total loss: \u001b[1m\u001b[32m109637.00781\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 040 | loss: 109637.00781 - acc: 0.2492 -- iter: 352/497\n",
            "Training Step: 1271  | total loss: \u001b[1m\u001b[32m110353.57031\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 040 | loss: 110353.57031 - acc: 0.2618 -- iter: 368/497\n",
            "Training Step: 1272  | total loss: \u001b[1m\u001b[32m110727.49219\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 040 | loss: 110727.49219 - acc: 0.2606 -- iter: 384/497\n",
            "Training Step: 1273  | total loss: \u001b[1m\u001b[32m111406.39062\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 040 | loss: 111406.39062 - acc: 0.2658 -- iter: 400/497\n",
            "Training Step: 1274  | total loss: \u001b[1m\u001b[32m111606.53125\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 040 | loss: 111606.53125 - acc: 0.2580 -- iter: 416/497\n",
            "Training Step: 1275  | total loss: \u001b[1m\u001b[32m109294.17969\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 040 | loss: 109294.17969 - acc: 0.2509 -- iter: 432/497\n",
            "Training Step: 1276  | total loss: \u001b[1m\u001b[32m108815.46094\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 040 | loss: 108815.46094 - acc: 0.2571 -- iter: 448/497\n",
            "Training Step: 1277  | total loss: \u001b[1m\u001b[32m105233.12500\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 040 | loss: 105233.12500 - acc: 0.2689 -- iter: 464/497\n",
            "Training Step: 1278  | total loss: \u001b[1m\u001b[32m103940.35938\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 040 | loss: 103940.35938 - acc: 0.2732 -- iter: 480/497\n",
            "Training Step: 1279  | total loss: \u001b[1m\u001b[32m103927.39844\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 040 | loss: 103927.39844 - acc: 0.2709 -- iter: 496/497\n",
            "Training Step: 1280  | total loss: \u001b[1m\u001b[32m99437.67969\u001b[0m\u001b[0m | time: 1.124s\n",
            "| Adam | epoch: 040 | loss: 99437.67969 - acc: 0.2626 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1281  | total loss: \u001b[1m\u001b[32m98441.57031\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 041 | loss: 98441.57031 - acc: 0.2738 -- iter: 016/497\n",
            "Training Step: 1282  | total loss: \u001b[1m\u001b[32m99372.78906\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 041 | loss: 99372.78906 - acc: 0.2777 -- iter: 032/497\n",
            "Training Step: 1283  | total loss: \u001b[1m\u001b[32m99173.12500\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 041 | loss: 99173.12500 - acc: 0.2624 -- iter: 048/497\n",
            "Training Step: 1284  | total loss: \u001b[1m\u001b[32m102191.98438\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 041 | loss: 102191.98438 - acc: 0.2487 -- iter: 064/497\n",
            "Training Step: 1285  | total loss: \u001b[1m\u001b[32m102856.71875\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 041 | loss: 102856.71875 - acc: 0.2613 -- iter: 080/497\n",
            "Training Step: 1286  | total loss: \u001b[1m\u001b[32m103505.78125\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 041 | loss: 103505.78125 - acc: 0.2664 -- iter: 096/497\n",
            "Training Step: 1287  | total loss: \u001b[1m\u001b[32m111914.82812\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 041 | loss: 111914.82812 - acc: 0.2398 -- iter: 112/497\n",
            "Training Step: 1288  | total loss: \u001b[1m\u001b[32m119482.96875\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 041 | loss: 119482.96875 - acc: 0.2158 -- iter: 128/497\n",
            "Training Step: 1289  | total loss: \u001b[1m\u001b[32m117263.92969\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 041 | loss: 117263.92969 - acc: 0.2067 -- iter: 144/497\n",
            "Training Step: 1290  | total loss: \u001b[1m\u001b[32m119414.55469\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 041 | loss: 119414.55469 - acc: 0.2048 -- iter: 160/497\n",
            "Training Step: 1291  | total loss: \u001b[1m\u001b[32m121066.75781\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 041 | loss: 121066.75781 - acc: 0.2406 -- iter: 176/497\n",
            "Training Step: 1292  | total loss: \u001b[1m\u001b[32m118463.36719\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 041 | loss: 118463.36719 - acc: 0.2603 -- iter: 192/497\n",
            "Training Step: 1293  | total loss: \u001b[1m\u001b[32m115868.50000\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 041 | loss: 115868.50000 - acc: 0.2467 -- iter: 208/497\n",
            "Training Step: 1294  | total loss: \u001b[1m\u001b[32m115628.10156\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 041 | loss: 115628.10156 - acc: 0.2471 -- iter: 224/497\n",
            "Training Step: 1295  | total loss: \u001b[1m\u001b[32m115397.53125\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 041 | loss: 115397.53125 - acc: 0.2474 -- iter: 240/497\n",
            "Training Step: 1296  | total loss: \u001b[1m\u001b[32m112010.45312\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 041 | loss: 112010.45312 - acc: 0.2726 -- iter: 256/497\n",
            "Training Step: 1297  | total loss: \u001b[1m\u001b[32m111805.50000\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 041 | loss: 111805.50000 - acc: 0.2516 -- iter: 272/497\n",
            "Training Step: 1298  | total loss: \u001b[1m\u001b[32m110729.97656\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 041 | loss: 110729.97656 - acc: 0.2514 -- iter: 288/497\n",
            "Training Step: 1299  | total loss: \u001b[1m\u001b[32m106535.99219\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 041 | loss: 106535.99219 - acc: 0.2513 -- iter: 304/497\n",
            "Training Step: 1300  | total loss: \u001b[1m\u001b[32m107260.61719\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 041 | loss: 107260.61719 - acc: 0.2449 -- iter: 320/497\n",
            "Training Step: 1301  | total loss: \u001b[1m\u001b[32m106168.29688\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 041 | loss: 106168.29688 - acc: 0.2392 -- iter: 336/497\n",
            "Training Step: 1302  | total loss: \u001b[1m\u001b[32m103305.82812\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 041 | loss: 103305.82812 - acc: 0.2465 -- iter: 352/497\n",
            "Training Step: 1303  | total loss: \u001b[1m\u001b[32m102700.70312\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 041 | loss: 102700.70312 - acc: 0.2344 -- iter: 368/497\n",
            "Training Step: 1304  | total loss: \u001b[1m\u001b[32m103007.53125\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 041 | loss: 103007.53125 - acc: 0.2234 -- iter: 384/497\n",
            "Training Step: 1305  | total loss: \u001b[1m\u001b[32m102348.10156\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 041 | loss: 102348.10156 - acc: 0.2136 -- iter: 400/497\n",
            "Training Step: 1306  | total loss: \u001b[1m\u001b[32m100467.29688\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 041 | loss: 100467.29688 - acc: 0.1985 -- iter: 416/497\n",
            "Training Step: 1307  | total loss: \u001b[1m\u001b[32m100115.63281\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 041 | loss: 100115.63281 - acc: 0.2036 -- iter: 432/497\n",
            "Training Step: 1308  | total loss: \u001b[1m\u001b[32m97987.65625\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 041 | loss: 97987.65625 - acc: 0.2270 -- iter: 448/497\n",
            "Training Step: 1309  | total loss: \u001b[1m\u001b[32m97098.68750\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 041 | loss: 97098.68750 - acc: 0.2106 -- iter: 464/497\n",
            "Training Step: 1310  | total loss: \u001b[1m\u001b[32m98103.43750\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 041 | loss: 98103.43750 - acc: 0.2083 -- iter: 480/497\n",
            "Training Step: 1311  | total loss: \u001b[1m\u001b[32m98368.14062\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 041 | loss: 98368.14062 - acc: 0.1999 -- iter: 496/497\n",
            "Training Step: 1312  | total loss: \u001b[1m\u001b[32m100771.13281\u001b[0m\u001b[0m | time: 1.130s\n",
            "| Adam | epoch: 041 | loss: 100771.13281 - acc: 0.2112 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1313  | total loss: \u001b[1m\u001b[32m102926.11719\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 042 | loss: 102926.11719 - acc: 0.1901 -- iter: 016/497\n",
            "Training Step: 1314  | total loss: \u001b[1m\u001b[32m103609.24219\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 042 | loss: 103609.24219 - acc: 0.2211 -- iter: 032/497\n",
            "Training Step: 1315  | total loss: \u001b[1m\u001b[32m104664.28906\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 042 | loss: 104664.28906 - acc: 0.2490 -- iter: 048/497\n",
            "Training Step: 1316  | total loss: \u001b[1m\u001b[32m107954.36719\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 042 | loss: 107954.36719 - acc: 0.2553 -- iter: 064/497\n",
            "Training Step: 1317  | total loss: \u001b[1m\u001b[32m108282.77344\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 042 | loss: 108282.77344 - acc: 0.2485 -- iter: 080/497\n",
            "Training Step: 1318  | total loss: \u001b[1m\u001b[32m107838.83594\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 042 | loss: 107838.83594 - acc: 0.2549 -- iter: 096/497\n",
            "Training Step: 1319  | total loss: \u001b[1m\u001b[32m107433.69531\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 042 | loss: 107433.69531 - acc: 0.2607 -- iter: 112/497\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m99988.08594\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 042 | loss: 99988.08594 - acc: 0.2346 -- iter: 128/497\n",
            "Training Step: 1321  | total loss: \u001b[1m\u001b[32m93287.03906\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 042 | loss: 93287.03906 - acc: 0.2112 -- iter: 144/497\n",
            "Training Step: 1322  | total loss: \u001b[1m\u001b[32m94913.68750\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 042 | loss: 94913.68750 - acc: 0.2088 -- iter: 160/497\n",
            "Training Step: 1323  | total loss: \u001b[1m\u001b[32m97941.35938\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 042 | loss: 97941.35938 - acc: 0.2192 -- iter: 176/497\n",
            "Training Step: 1324  | total loss: \u001b[1m\u001b[32m99929.72656\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 042 | loss: 99929.72656 - acc: 0.2160 -- iter: 192/497\n",
            "Training Step: 1325  | total loss: \u001b[1m\u001b[32m95649.12500\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 042 | loss: 95649.12500 - acc: 0.2319 -- iter: 208/497\n",
            "Training Step: 1326  | total loss: \u001b[1m\u001b[32m96097.75000\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 042 | loss: 96097.75000 - acc: 0.2337 -- iter: 224/497\n",
            "Training Step: 1327  | total loss: \u001b[1m\u001b[32m97537.63281\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 042 | loss: 97537.63281 - acc: 0.2416 -- iter: 240/497\n",
            "Training Step: 1328  | total loss: \u001b[1m\u001b[32m100943.96094\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 042 | loss: 100943.96094 - acc: 0.2487 -- iter: 256/497\n",
            "Training Step: 1329  | total loss: \u001b[1m\u001b[32m100583.69531\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 042 | loss: 100583.69531 - acc: 0.2738 -- iter: 272/497\n",
            "Training Step: 1330  | total loss: \u001b[1m\u001b[32m99986.53125\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 042 | loss: 99986.53125 - acc: 0.2714 -- iter: 288/497\n",
            "Training Step: 1331  | total loss: \u001b[1m\u001b[32m96471.95312\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 042 | loss: 96471.95312 - acc: 0.2693 -- iter: 304/497\n",
            "Training Step: 1332  | total loss: \u001b[1m\u001b[32m99385.78906\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 042 | loss: 99385.78906 - acc: 0.2611 -- iter: 320/497\n",
            "Training Step: 1333  | total loss: \u001b[1m\u001b[32m99135.16406\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 042 | loss: 99135.16406 - acc: 0.2725 -- iter: 336/497\n",
            "Training Step: 1334  | total loss: \u001b[1m\u001b[32m95949.92969\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 042 | loss: 95949.92969 - acc: 0.2827 -- iter: 352/497\n",
            "Training Step: 1335  | total loss: \u001b[1m\u001b[32m95507.32031\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 042 | loss: 95507.32031 - acc: 0.2545 -- iter: 368/497\n",
            "Training Step: 1336  | total loss: \u001b[1m\u001b[32m95068.16406\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 042 | loss: 95068.16406 - acc: 0.2415 -- iter: 384/497\n",
            "Training Step: 1337  | total loss: \u001b[1m\u001b[32m98539.64062\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 042 | loss: 98539.64062 - acc: 0.2424 -- iter: 400/497\n",
            "Training Step: 1338  | total loss: \u001b[1m\u001b[32m103574.78125\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 042 | loss: 103574.78125 - acc: 0.2369 -- iter: 416/497\n",
            "Training Step: 1339  | total loss: \u001b[1m\u001b[32m102616.02344\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 042 | loss: 102616.02344 - acc: 0.2319 -- iter: 432/497\n",
            "Training Step: 1340  | total loss: \u001b[1m\u001b[32m102620.14062\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 042 | loss: 102620.14062 - acc: 0.2213 -- iter: 448/497\n",
            "Training Step: 1341  | total loss: \u001b[1m\u001b[32m106057.73438\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 042 | loss: 106057.73438 - acc: 0.2241 -- iter: 464/497\n",
            "Training Step: 1342  | total loss: \u001b[1m\u001b[32m107096.39062\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 042 | loss: 107096.39062 - acc: 0.2205 -- iter: 480/497\n",
            "Training Step: 1343  | total loss: \u001b[1m\u001b[32m107992.57031\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 042 | loss: 107992.57031 - acc: 0.2234 -- iter: 496/497\n",
            "Training Step: 1344  | total loss: \u001b[1m\u001b[32m104421.64844\u001b[0m\u001b[0m | time: 1.124s\n",
            "| Adam | epoch: 042 | loss: 104421.64844 - acc: 0.2386 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1345  | total loss: \u001b[1m\u001b[32m104580.58594\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 043 | loss: 104580.58594 - acc: 0.2397 -- iter: 016/497\n",
            "Training Step: 1346  | total loss: \u001b[1m\u001b[32m103964.09375\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 043 | loss: 103964.09375 - acc: 0.2407 -- iter: 032/497\n",
            "Training Step: 1347  | total loss: \u001b[1m\u001b[32m105393.61719\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 043 | loss: 105393.61719 - acc: 0.2479 -- iter: 048/497\n",
            "Training Step: 1348  | total loss: \u001b[1m\u001b[32m103062.03125\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 043 | loss: 103062.03125 - acc: 0.2356 -- iter: 064/497\n",
            "Training Step: 1349  | total loss: \u001b[1m\u001b[32m101446.63281\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 043 | loss: 101446.63281 - acc: 0.2308 -- iter: 080/497\n",
            "Training Step: 1350  | total loss: \u001b[1m\u001b[32m105238.45312\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 043 | loss: 105238.45312 - acc: 0.2327 -- iter: 096/497\n",
            "Training Step: 1351  | total loss: \u001b[1m\u001b[32m103004.30469\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 043 | loss: 103004.30469 - acc: 0.2157 -- iter: 112/497\n",
            "Training Step: 1352  | total loss: \u001b[1m\u001b[32m102242.63281\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 043 | loss: 102242.63281 - acc: 0.2316 -- iter: 128/497\n",
            "Training Step: 1353  | total loss: \u001b[1m\u001b[32m109076.38281\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 043 | loss: 109076.38281 - acc: 0.2085 -- iter: 144/497\n",
            "Training Step: 1354  | total loss: \u001b[1m\u001b[32m115226.75781\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 043 | loss: 115226.75781 - acc: 0.1876 -- iter: 160/497\n",
            "Training Step: 1355  | total loss: \u001b[1m\u001b[32m114677.75781\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 043 | loss: 114677.75781 - acc: 0.1876 -- iter: 176/497\n",
            "Training Step: 1356  | total loss: \u001b[1m\u001b[32m112864.86719\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 043 | loss: 112864.86719 - acc: 0.2064 -- iter: 192/497\n",
            "Training Step: 1357  | total loss: \u001b[1m\u001b[32m111870.32812\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 043 | loss: 111870.32812 - acc: 0.2107 -- iter: 208/497\n",
            "Training Step: 1358  | total loss: \u001b[1m\u001b[32m110531.96875\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 043 | loss: 110531.96875 - acc: 0.2209 -- iter: 224/497\n",
            "Training Step: 1359  | total loss: \u001b[1m\u001b[32m110557.87500\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 043 | loss: 110557.87500 - acc: 0.2113 -- iter: 240/497\n",
            "Training Step: 1360  | total loss: \u001b[1m\u001b[32m106267.45312\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 043 | loss: 106267.45312 - acc: 0.2152 -- iter: 256/497\n",
            "Training Step: 1361  | total loss: \u001b[1m\u001b[32m107111.57031\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 043 | loss: 107111.57031 - acc: 0.2249 -- iter: 272/497\n",
            "Training Step: 1362  | total loss: \u001b[1m\u001b[32m106152.83594\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 043 | loss: 106152.83594 - acc: 0.2274 -- iter: 288/497\n",
            "Training Step: 1363  | total loss: \u001b[1m\u001b[32m106967.16406\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 043 | loss: 106967.16406 - acc: 0.2109 -- iter: 304/497\n",
            "Training Step: 1364  | total loss: \u001b[1m\u001b[32m107318.52344\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 043 | loss: 107318.52344 - acc: 0.2086 -- iter: 320/497\n",
            "Training Step: 1365  | total loss: \u001b[1m\u001b[32m108011.52344\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 043 | loss: 108011.52344 - acc: 0.2127 -- iter: 336/497\n",
            "Training Step: 1366  | total loss: \u001b[1m\u001b[32m107612.84375\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 043 | loss: 107612.84375 - acc: 0.2227 -- iter: 352/497\n",
            "Training Step: 1367  | total loss: \u001b[1m\u001b[32m104941.61719\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 043 | loss: 104941.61719 - acc: 0.2254 -- iter: 368/497\n",
            "Training Step: 1368  | total loss: \u001b[1m\u001b[32m103808.78906\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 043 | loss: 103808.78906 - acc: 0.2341 -- iter: 384/497\n",
            "Training Step: 1369  | total loss: \u001b[1m\u001b[32m105562.97656\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 043 | loss: 105562.97656 - acc: 0.2357 -- iter: 400/497\n",
            "Training Step: 1370  | total loss: \u001b[1m\u001b[32m106107.77344\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 043 | loss: 106107.77344 - acc: 0.2497 -- iter: 416/497\n",
            "Training Step: 1371  | total loss: \u001b[1m\u001b[32m105935.33594\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 043 | loss: 105935.33594 - acc: 0.2497 -- iter: 432/497\n",
            "Training Step: 1372  | total loss: \u001b[1m\u001b[32m105547.76562\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 043 | loss: 105547.76562 - acc: 0.2372 -- iter: 448/497\n",
            "Training Step: 1373  | total loss: \u001b[1m\u001b[32m106305.78906\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 043 | loss: 106305.78906 - acc: 0.2447 -- iter: 464/497\n",
            "Training Step: 1374  | total loss: \u001b[1m\u001b[32m105660.78906\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 043 | loss: 105660.78906 - acc: 0.2515 -- iter: 480/497\n",
            "Training Step: 1375  | total loss: \u001b[1m\u001b[32m102360.94531\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 043 | loss: 102360.94531 - acc: 0.2389 -- iter: 496/497\n",
            "Training Step: 1376  | total loss: \u001b[1m\u001b[32m105977.03125\u001b[0m\u001b[0m | time: 1.132s\n",
            "| Adam | epoch: 043 | loss: 105977.03125 - acc: 0.2462 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1377  | total loss: \u001b[1m\u001b[32m108199.30469\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 044 | loss: 108199.30469 - acc: 0.2466 -- iter: 016/497\n",
            "Training Step: 1378  | total loss: \u001b[1m\u001b[32m106769.63281\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 044 | loss: 106769.63281 - acc: 0.2469 -- iter: 032/497\n",
            "Training Step: 1379  | total loss: \u001b[1m\u001b[32m105313.63281\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 044 | loss: 105313.63281 - acc: 0.2473 -- iter: 048/497\n",
            "Training Step: 1380  | total loss: \u001b[1m\u001b[32m104330.85938\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 044 | loss: 104330.85938 - acc: 0.2350 -- iter: 064/497\n",
            "Training Step: 1381  | total loss: \u001b[1m\u001b[32m105961.27344\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 044 | loss: 105961.27344 - acc: 0.2365 -- iter: 080/497\n",
            "Training Step: 1382  | total loss: \u001b[1m\u001b[32m107912.14844\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 044 | loss: 107912.14844 - acc: 0.2441 -- iter: 096/497\n",
            "Training Step: 1383  | total loss: \u001b[1m\u001b[32m105646.53125\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 044 | loss: 105646.53125 - acc: 0.2447 -- iter: 112/497\n",
            "Training Step: 1384  | total loss: \u001b[1m\u001b[32m105203.50781\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 044 | loss: 105203.50781 - acc: 0.2515 -- iter: 128/497\n",
            "Training Step: 1385  | total loss: \u001b[1m\u001b[32m103211.53125\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 044 | loss: 103211.53125 - acc: 0.2388 -- iter: 144/497\n",
            "Training Step: 1386  | total loss: \u001b[1m\u001b[32m111530.03906\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 044 | loss: 111530.03906 - acc: 0.2150 -- iter: 160/497\n",
            "Training Step: 1387  | total loss: \u001b[1m\u001b[32m119016.69531\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 044 | loss: 119016.69531 - acc: 0.1935 -- iter: 176/497\n",
            "Training Step: 1388  | total loss: \u001b[1m\u001b[32m115411.22656\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 044 | loss: 115411.22656 - acc: 0.1929 -- iter: 192/497\n",
            "Training Step: 1389  | total loss: \u001b[1m\u001b[32m112613.89844\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 044 | loss: 112613.89844 - acc: 0.1923 -- iter: 208/497\n",
            "Training Step: 1390  | total loss: \u001b[1m\u001b[32m114449.77344\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 044 | loss: 114449.77344 - acc: 0.1918 -- iter: 224/497\n",
            "Training Step: 1391  | total loss: \u001b[1m\u001b[32m114448.82812\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 044 | loss: 114448.82812 - acc: 0.1914 -- iter: 240/497\n",
            "Training Step: 1392  | total loss: \u001b[1m\u001b[32m115977.54688\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 044 | loss: 115977.54688 - acc: 0.1973 -- iter: 256/497\n",
            "Training Step: 1393  | total loss: \u001b[1m\u001b[32m110230.38281\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 044 | loss: 110230.38281 - acc: 0.2088 -- iter: 272/497\n",
            "Training Step: 1394  | total loss: \u001b[1m\u001b[32m107895.68750\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 044 | loss: 107895.68750 - acc: 0.2129 -- iter: 288/497\n",
            "Training Step: 1395  | total loss: \u001b[1m\u001b[32m105667.32031\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 044 | loss: 105667.32031 - acc: 0.2166 -- iter: 304/497\n",
            "Training Step: 1396  | total loss: \u001b[1m\u001b[32m103915.02344\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 044 | loss: 103915.02344 - acc: 0.2137 -- iter: 320/497\n",
            "Training Step: 1397  | total loss: \u001b[1m\u001b[32m104391.60938\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 044 | loss: 104391.60938 - acc: 0.2236 -- iter: 336/497\n",
            "Training Step: 1398  | total loss: \u001b[1m\u001b[32m103770.79688\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 044 | loss: 103770.79688 - acc: 0.2137 -- iter: 352/497\n",
            "Training Step: 1399  | total loss: \u001b[1m\u001b[32m104896.23438\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 044 | loss: 104896.23438 - acc: 0.2299 -- iter: 368/497\n",
            "Training Step: 1400  | total loss: \u001b[1m\u001b[32m108469.28906\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 044 | loss: 108469.28906 - acc: 0.2694 -- iter: 384/497\n",
            "Training Step: 1401  | total loss: \u001b[1m\u001b[32m107450.67188\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 044 | loss: 107450.67188 - acc: 0.2674 -- iter: 400/497\n",
            "Training Step: 1402  | total loss: \u001b[1m\u001b[32m108437.25781\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 044 | loss: 108437.25781 - acc: 0.2657 -- iter: 416/497\n",
            "Training Step: 1403  | total loss: \u001b[1m\u001b[32m106437.75781\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 044 | loss: 106437.75781 - acc: 0.2766 -- iter: 432/497\n",
            "Training Step: 1404  | total loss: \u001b[1m\u001b[32m104543.83594\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 044 | loss: 104543.83594 - acc: 0.2615 -- iter: 448/497\n",
            "Training Step: 1405  | total loss: \u001b[1m\u001b[32m105125.89844\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 044 | loss: 105125.89844 - acc: 0.2478 -- iter: 464/497\n",
            "Training Step: 1406  | total loss: \u001b[1m\u001b[32m103017.34375\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 044 | loss: 103017.34375 - acc: 0.2418 -- iter: 480/497\n",
            "Training Step: 1407  | total loss: \u001b[1m\u001b[32m107496.71094\u001b[0m\u001b[0m | time: 0.126s\n",
            "| Adam | epoch: 044 | loss: 107496.71094 - acc: 0.2364 -- iter: 496/497\n",
            "Training Step: 1408  | total loss: \u001b[1m\u001b[32m105883.53906\u001b[0m\u001b[0m | time: 1.135s\n",
            "| Adam | epoch: 044 | loss: 105883.53906 - acc: 0.2252 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1409  | total loss: \u001b[1m\u001b[32m106476.36719\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 045 | loss: 106476.36719 - acc: 0.2277 -- iter: 016/497\n",
            "Training Step: 1410  | total loss: \u001b[1m\u001b[32m108459.05469\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 045 | loss: 108459.05469 - acc: 0.2237 -- iter: 032/497\n",
            "Training Step: 1411  | total loss: \u001b[1m\u001b[32m107552.68750\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 045 | loss: 107552.68750 - acc: 0.2201 -- iter: 048/497\n",
            "Training Step: 1412  | total loss: \u001b[1m\u001b[32m104227.53125\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 045 | loss: 104227.53125 - acc: 0.2418 -- iter: 064/497\n",
            "Training Step: 1413  | total loss: \u001b[1m\u001b[32m104270.07031\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 045 | loss: 104270.07031 - acc: 0.2301 -- iter: 080/497\n",
            "Training Step: 1414  | total loss: \u001b[1m\u001b[32m102882.45312\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 045 | loss: 102882.45312 - acc: 0.2571 -- iter: 096/497\n",
            "Training Step: 1415  | total loss: \u001b[1m\u001b[32m103855.85156\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 045 | loss: 103855.85156 - acc: 0.2439 -- iter: 112/497\n",
            "Training Step: 1416  | total loss: \u001b[1m\u001b[32m103650.60156\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 045 | loss: 103650.60156 - acc: 0.2383 -- iter: 128/497\n",
            "Training Step: 1417  | total loss: \u001b[1m\u001b[32m105528.61719\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 045 | loss: 105528.61719 - acc: 0.2457 -- iter: 144/497\n",
            "Training Step: 1418  | total loss: \u001b[1m\u001b[32m106032.72656\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 045 | loss: 106032.72656 - acc: 0.2461 -- iter: 160/497\n",
            "Training Step: 1419  | total loss: \u001b[1m\u001b[32m113995.65625\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 045 | loss: 113995.65625 - acc: 0.2215 -- iter: 176/497\n",
            "Training Step: 1420  | total loss: \u001b[1m\u001b[32m102553.49219\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 045 | loss: 102553.49219 - acc: 0.1994 -- iter: 192/497\n",
            "Training Step: 1421  | total loss: \u001b[1m\u001b[32m101620.85156\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 045 | loss: 101620.85156 - acc: 0.1919 -- iter: 208/497\n",
            "Training Step: 1422  | total loss: \u001b[1m\u001b[32m104877.78906\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 045 | loss: 104877.78906 - acc: 0.2102 -- iter: 224/497\n",
            "Training Step: 1423  | total loss: \u001b[1m\u001b[32m104645.01562\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 045 | loss: 104645.01562 - acc: 0.2142 -- iter: 240/497\n",
            "Training Step: 1424  | total loss: \u001b[1m\u001b[32m101737.14062\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 045 | loss: 101737.14062 - acc: 0.2303 -- iter: 256/497\n",
            "Training Step: 1425  | total loss: \u001b[1m\u001b[32m102895.45312\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 045 | loss: 102895.45312 - acc: 0.2260 -- iter: 272/497\n",
            "Training Step: 1426  | total loss: \u001b[1m\u001b[32m103192.47656\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 045 | loss: 103192.47656 - acc: 0.2409 -- iter: 288/497\n",
            "Training Step: 1427  | total loss: \u001b[1m\u001b[32m105338.59375\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 045 | loss: 105338.59375 - acc: 0.2293 -- iter: 304/497\n",
            "Training Step: 1428  | total loss: \u001b[1m\u001b[32m101207.32812\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 045 | loss: 101207.32812 - acc: 0.2251 -- iter: 320/497\n",
            "Training Step: 1429  | total loss: \u001b[1m\u001b[32m98709.14062\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 045 | loss: 98709.14062 - acc: 0.2276 -- iter: 336/497\n",
            "Training Step: 1430  | total loss: \u001b[1m\u001b[32m100511.12500\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 045 | loss: 100511.12500 - acc: 0.2361 -- iter: 352/497\n",
            "Training Step: 1431  | total loss: \u001b[1m\u001b[32m101260.85938\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 045 | loss: 101260.85938 - acc: 0.2312 -- iter: 368/497\n",
            "Training Step: 1432  | total loss: \u001b[1m\u001b[32m99953.10938\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 045 | loss: 99953.10938 - acc: 0.2269 -- iter: 384/497\n",
            "Training Step: 1433  | total loss: \u001b[1m\u001b[32m101760.95312\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 045 | loss: 101760.95312 - acc: 0.2479 -- iter: 400/497\n",
            "Training Step: 1434  | total loss: \u001b[1m\u001b[32m101389.98438\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 045 | loss: 101389.98438 - acc: 0.2419 -- iter: 416/497\n",
            "Training Step: 1435  | total loss: \u001b[1m\u001b[32m99974.17188\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 045 | loss: 99974.17188 - acc: 0.2427 -- iter: 432/497\n",
            "Training Step: 1436  | total loss: \u001b[1m\u001b[32m99204.16406\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 045 | loss: 99204.16406 - acc: 0.2434 -- iter: 448/497\n",
            "Training Step: 1437  | total loss: \u001b[1m\u001b[32m99752.46875\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 045 | loss: 99752.46875 - acc: 0.2628 -- iter: 464/497\n",
            "Training Step: 1438  | total loss: \u001b[1m\u001b[32m101550.14062\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 045 | loss: 101550.14062 - acc: 0.2616 -- iter: 480/497\n",
            "Training Step: 1439  | total loss: \u001b[1m\u001b[32m102817.73438\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 045 | loss: 102817.73438 - acc: 0.2604 -- iter: 496/497\n",
            "Training Step: 1440  | total loss: \u001b[1m\u001b[32m104788.49219\u001b[0m\u001b[0m | time: 1.129s\n",
            "| Adam | epoch: 045 | loss: 104788.49219 - acc: 0.2531 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1441  | total loss: \u001b[1m\u001b[32m105122.72656\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 046 | loss: 105122.72656 - acc: 0.2590 -- iter: 016/497\n",
            "Training Step: 1442  | total loss: \u001b[1m\u001b[32m104763.96094\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 046 | loss: 104763.96094 - acc: 0.2456 -- iter: 032/497\n",
            "Training Step: 1443  | total loss: \u001b[1m\u001b[32m103549.48438\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 046 | loss: 103549.48438 - acc: 0.2523 -- iter: 048/497\n",
            "Training Step: 1444  | total loss: \u001b[1m\u001b[32m105276.15625\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 046 | loss: 105276.15625 - acc: 0.2333 -- iter: 064/497\n",
            "Training Step: 1445  | total loss: \u001b[1m\u001b[32m105576.70312\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 046 | loss: 105576.70312 - acc: 0.2538 -- iter: 080/497\n",
            "Training Step: 1446  | total loss: \u001b[1m\u001b[32m103926.07812\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 046 | loss: 103926.07812 - acc: 0.2409 -- iter: 096/497\n",
            "Training Step: 1447  | total loss: \u001b[1m\u001b[32m107751.67188\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 046 | loss: 107751.67188 - acc: 0.2355 -- iter: 112/497\n",
            "Training Step: 1448  | total loss: \u001b[1m\u001b[32m105907.14062\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 046 | loss: 105907.14062 - acc: 0.2370 -- iter: 128/497\n",
            "Training Step: 1449  | total loss: \u001b[1m\u001b[32m103261.65625\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 046 | loss: 103261.65625 - acc: 0.2383 -- iter: 144/497\n",
            "Training Step: 1450  | total loss: \u001b[1m\u001b[32m104264.69531\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 046 | loss: 104264.69531 - acc: 0.2395 -- iter: 160/497\n",
            "Training Step: 1451  | total loss: \u001b[1m\u001b[32m105463.56250\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 046 | loss: 105463.56250 - acc: 0.2280 -- iter: 176/497\n",
            "Training Step: 1452  | total loss: \u001b[1m\u001b[32m94874.60938\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 046 | loss: 94874.60938 - acc: 0.2052 -- iter: 192/497\n",
            "Training Step: 1453  | total loss: \u001b[1m\u001b[32m85344.54688\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 046 | loss: 85344.54688 - acc: 0.1847 -- iter: 208/497\n",
            "Training Step: 1454  | total loss: \u001b[1m\u001b[32m90895.72656\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 046 | loss: 90895.72656 - acc: 0.1912 -- iter: 224/497\n",
            "Training Step: 1455  | total loss: \u001b[1m\u001b[32m88247.69531\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 046 | loss: 88247.69531 - acc: 0.2034 -- iter: 240/497\n",
            "Training Step: 1456  | total loss: \u001b[1m\u001b[32m88594.89844\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 046 | loss: 88594.89844 - acc: 0.1893 -- iter: 256/497\n",
            "Training Step: 1457  | total loss: \u001b[1m\u001b[32m86345.75781\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 046 | loss: 86345.75781 - acc: 0.1891 -- iter: 272/497\n",
            "Training Step: 1458  | total loss: \u001b[1m\u001b[32m88622.31250\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 046 | loss: 88622.31250 - acc: 0.1889 -- iter: 288/497\n",
            "Training Step: 1459  | total loss: \u001b[1m\u001b[32m90114.80469\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 046 | loss: 90114.80469 - acc: 0.1950 -- iter: 304/497\n",
            "Training Step: 1460  | total loss: \u001b[1m\u001b[32m93906.20312\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 046 | loss: 93906.20312 - acc: 0.1880 -- iter: 320/497\n",
            "Training Step: 1461  | total loss: \u001b[1m\u001b[32m94510.48438\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 046 | loss: 94510.48438 - acc: 0.1880 -- iter: 336/497\n",
            "Training Step: 1462  | total loss: \u001b[1m\u001b[32m93751.04688\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 046 | loss: 93751.04688 - acc: 0.2129 -- iter: 352/497\n",
            "Training Step: 1463  | total loss: \u001b[1m\u001b[32m90406.57031\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 046 | loss: 90406.57031 - acc: 0.2041 -- iter: 368/497\n",
            "Training Step: 1464  | total loss: \u001b[1m\u001b[32m90483.61719\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 046 | loss: 90483.61719 - acc: 0.2087 -- iter: 384/497\n",
            "Training Step: 1465  | total loss: \u001b[1m\u001b[32m92654.19531\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 046 | loss: 92654.19531 - acc: 0.2129 -- iter: 400/497\n",
            "Training Step: 1466  | total loss: \u001b[1m\u001b[32m96235.47656\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 046 | loss: 96235.47656 - acc: 0.2228 -- iter: 416/497\n",
            "Training Step: 1467  | total loss: \u001b[1m\u001b[32m96022.71094\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 046 | loss: 96022.71094 - acc: 0.2443 -- iter: 432/497\n",
            "Training Step: 1468  | total loss: \u001b[1m\u001b[32m97586.66406\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 046 | loss: 97586.66406 - acc: 0.2511 -- iter: 448/497\n",
            "Training Step: 1469  | total loss: \u001b[1m\u001b[32m98502.55469\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 046 | loss: 98502.55469 - acc: 0.2510 -- iter: 464/497\n",
            "Training Step: 1470  | total loss: \u001b[1m\u001b[32m98515.40625\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 046 | loss: 98515.40625 - acc: 0.2509 -- iter: 480/497\n",
            "Training Step: 1471  | total loss: \u001b[1m\u001b[32m99867.32031\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 046 | loss: 99867.32031 - acc: 0.2571 -- iter: 496/497\n",
            "Training Step: 1472  | total loss: \u001b[1m\u001b[32m101392.50781\u001b[0m\u001b[0m | time: 1.125s\n",
            "| Adam | epoch: 046 | loss: 101392.50781 - acc: 0.2689 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1473  | total loss: \u001b[1m\u001b[32m102858.57031\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 047 | loss: 102858.57031 - acc: 0.2795 -- iter: 016/497\n",
            "Training Step: 1474  | total loss: \u001b[1m\u001b[32m104014.42969\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 047 | loss: 104014.42969 - acc: 0.2765 -- iter: 032/497\n",
            "Training Step: 1475  | total loss: \u001b[1m\u001b[32m103085.41406\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 047 | loss: 103085.41406 - acc: 0.2551 -- iter: 048/497\n",
            "Training Step: 1476  | total loss: \u001b[1m\u001b[32m103187.25000\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 047 | loss: 103187.25000 - acc: 0.2421 -- iter: 064/497\n",
            "Training Step: 1477  | total loss: \u001b[1m\u001b[32m104324.50781\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 047 | loss: 104324.50781 - acc: 0.2304 -- iter: 080/497\n",
            "Training Step: 1478  | total loss: \u001b[1m\u001b[32m103862.88281\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 047 | loss: 103862.88281 - acc: 0.2261 -- iter: 096/497\n",
            "Training Step: 1479  | total loss: \u001b[1m\u001b[32m105063.14844\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 047 | loss: 105063.14844 - acc: 0.2472 -- iter: 112/497\n",
            "Training Step: 1480  | total loss: \u001b[1m\u001b[32m104177.79688\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 047 | loss: 104177.79688 - acc: 0.2475 -- iter: 128/497\n",
            "Training Step: 1481  | total loss: \u001b[1m\u001b[32m103625.32812\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 047 | loss: 103625.32812 - acc: 0.2290 -- iter: 144/497\n",
            "Training Step: 1482  | total loss: \u001b[1m\u001b[32m103259.39844\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 047 | loss: 103259.39844 - acc: 0.2374 -- iter: 160/497\n",
            "Training Step: 1483  | total loss: \u001b[1m\u001b[32m106005.83594\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 047 | loss: 106005.83594 - acc: 0.2386 -- iter: 176/497\n",
            "Training Step: 1484  | total loss: \u001b[1m\u001b[32m106404.44531\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 047 | loss: 106404.44531 - acc: 0.2460 -- iter: 192/497\n",
            "Training Step: 1485  | total loss: \u001b[1m\u001b[32m99205.21094\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 047 | loss: 99205.21094 - acc: 0.2214 -- iter: 208/497\n",
            "Training Step: 1486  | total loss: \u001b[1m\u001b[32m92725.89844\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 047 | loss: 92725.89844 - acc: 0.1993 -- iter: 224/497\n",
            "Training Step: 1487  | total loss: \u001b[1m\u001b[32m90738.45312\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 047 | loss: 90738.45312 - acc: 0.1918 -- iter: 240/497\n",
            "Training Step: 1488  | total loss: \u001b[1m\u001b[32m94240.75000\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 047 | loss: 94240.75000 - acc: 0.2102 -- iter: 256/497\n",
            "Training Step: 1489  | total loss: \u001b[1m\u001b[32m94887.42188\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 047 | loss: 94887.42188 - acc: 0.2204 -- iter: 272/497\n",
            "Training Step: 1490  | total loss: \u001b[1m\u001b[32m94027.22656\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 047 | loss: 94027.22656 - acc: 0.2171 -- iter: 288/497\n",
            "Training Step: 1491  | total loss: \u001b[1m\u001b[32m93657.96094\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 047 | loss: 93657.96094 - acc: 0.1954 -- iter: 304/497\n",
            "Training Step: 1492  | total loss: \u001b[1m\u001b[32m91991.42188\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 047 | loss: 91991.42188 - acc: 0.1946 -- iter: 320/497\n",
            "Training Step: 1493  | total loss: \u001b[1m\u001b[32m92072.04688\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 047 | loss: 92072.04688 - acc: 0.1939 -- iter: 336/497\n",
            "Training Step: 1494  | total loss: \u001b[1m\u001b[32m93916.63281\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 047 | loss: 93916.63281 - acc: 0.1995 -- iter: 352/497\n",
            "Training Step: 1495  | total loss: \u001b[1m\u001b[32m97137.57031\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 047 | loss: 97137.57031 - acc: 0.2108 -- iter: 368/497\n",
            "Training Step: 1496  | total loss: \u001b[1m\u001b[32m96800.67188\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 047 | loss: 96800.67188 - acc: 0.2210 -- iter: 384/497\n",
            "Training Step: 1497  | total loss: \u001b[1m\u001b[32m98657.21875\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 047 | loss: 98657.21875 - acc: 0.2239 -- iter: 400/497\n",
            "Training Step: 1498  | total loss: \u001b[1m\u001b[32m100063.85938\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 047 | loss: 100063.85938 - acc: 0.2265 -- iter: 416/497\n",
            "Training Step: 1499  | total loss: \u001b[1m\u001b[32m99933.78125\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 047 | loss: 99933.78125 - acc: 0.2101 -- iter: 432/497\n",
            "Training Step: 1500  | total loss: \u001b[1m\u001b[32m99893.89844\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 047 | loss: 99893.89844 - acc: 0.2141 -- iter: 448/497\n",
            "Training Step: 1501  | total loss: \u001b[1m\u001b[32m102743.45312\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 047 | loss: 102743.45312 - acc: 0.2239 -- iter: 464/497\n",
            "Training Step: 1502  | total loss: \u001b[1m\u001b[32m107318.78125\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 047 | loss: 107318.78125 - acc: 0.2078 -- iter: 480/497\n",
            "Training Step: 1503  | total loss: \u001b[1m\u001b[32m107431.20312\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 047 | loss: 107431.20312 - acc: 0.2183 -- iter: 496/497\n",
            "Training Step: 1504  | total loss: \u001b[1m\u001b[32m105685.01562\u001b[0m\u001b[0m | time: 1.120s\n",
            "| Adam | epoch: 047 | loss: 105685.01562 - acc: 0.2152 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1505  | total loss: \u001b[1m\u001b[32m106240.67969\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 048 | loss: 106240.67969 - acc: 0.2062 -- iter: 016/497\n",
            "Training Step: 1506  | total loss: \u001b[1m\u001b[32m105728.75781\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 048 | loss: 105728.75781 - acc: 0.2105 -- iter: 032/497\n",
            "Training Step: 1507  | total loss: \u001b[1m\u001b[32m106291.58594\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 048 | loss: 106291.58594 - acc: 0.2207 -- iter: 048/497\n",
            "Training Step: 1508  | total loss: \u001b[1m\u001b[32m108441.51562\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 048 | loss: 108441.51562 - acc: 0.2112 -- iter: 064/497\n",
            "Training Step: 1509  | total loss: \u001b[1m\u001b[32m107132.24219\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 048 | loss: 107132.24219 - acc: 0.2213 -- iter: 080/497\n",
            "Training Step: 1510  | total loss: \u001b[1m\u001b[32m108734.16406\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 048 | loss: 108734.16406 - acc: 0.2492 -- iter: 096/497\n",
            "Training Step: 1511  | total loss: \u001b[1m\u001b[32m106896.93750\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 048 | loss: 106896.93750 - acc: 0.2555 -- iter: 112/497\n",
            "Training Step: 1512  | total loss: \u001b[1m\u001b[32m104848.81250\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 048 | loss: 104848.81250 - acc: 0.2550 -- iter: 128/497\n",
            "Training Step: 1513  | total loss: \u001b[1m\u001b[32m101014.02344\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 048 | loss: 101014.02344 - acc: 0.2420 -- iter: 144/497\n",
            "Training Step: 1514  | total loss: \u001b[1m\u001b[32m102757.75000\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 048 | loss: 102757.75000 - acc: 0.2553 -- iter: 160/497\n",
            "Training Step: 1515  | total loss: \u001b[1m\u001b[32m103292.23438\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 048 | loss: 103292.23438 - acc: 0.2610 -- iter: 176/497\n",
            "Training Step: 1516  | total loss: \u001b[1m\u001b[32m103512.82031\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 048 | loss: 103512.82031 - acc: 0.2786 -- iter: 192/497\n",
            "Training Step: 1517  | total loss: \u001b[1m\u001b[32m102364.09375\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 048 | loss: 102364.09375 - acc: 0.2758 -- iter: 208/497\n",
            "Training Step: 1518  | total loss: \u001b[1m\u001b[32m96956.43750\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 048 | loss: 96956.43750 - acc: 0.2482 -- iter: 224/497\n",
            "Training Step: 1519  | total loss: \u001b[1m\u001b[32m92089.54688\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 048 | loss: 92089.54688 - acc: 0.2234 -- iter: 240/497\n",
            "Training Step: 1520  | total loss: \u001b[1m\u001b[32m95631.07031\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 048 | loss: 95631.07031 - acc: 0.2260 -- iter: 256/497\n",
            "Training Step: 1521  | total loss: \u001b[1m\u001b[32m92769.92188\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 048 | loss: 92769.92188 - acc: 0.2347 -- iter: 272/497\n",
            "Training Step: 1522  | total loss: \u001b[1m\u001b[32m94215.01562\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 048 | loss: 94215.01562 - acc: 0.2300 -- iter: 288/497\n",
            "Training Step: 1523  | total loss: \u001b[1m\u001b[32m95466.44531\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 048 | loss: 95466.44531 - acc: 0.2320 -- iter: 304/497\n",
            "Training Step: 1524  | total loss: \u001b[1m\u001b[32m97655.64844\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 048 | loss: 97655.64844 - acc: 0.2525 -- iter: 320/497\n",
            "Training Step: 1525  | total loss: \u001b[1m\u001b[32m96264.82812\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 048 | loss: 96264.82812 - acc: 0.2398 -- iter: 336/497\n",
            "Training Step: 1526  | total loss: \u001b[1m\u001b[32m97700.36719\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 048 | loss: 97700.36719 - acc: 0.2220 -- iter: 352/497\n",
            "Training Step: 1527  | total loss: \u001b[1m\u001b[32m102569.73438\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 048 | loss: 102569.73438 - acc: 0.2373 -- iter: 368/497\n",
            "Training Step: 1528  | total loss: \u001b[1m\u001b[32m104748.50781\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 048 | loss: 104748.50781 - acc: 0.2386 -- iter: 384/497\n",
            "Training Step: 1529  | total loss: \u001b[1m\u001b[32m102488.68750\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 048 | loss: 102488.68750 - acc: 0.2397 -- iter: 400/497\n",
            "Training Step: 1530  | total loss: \u001b[1m\u001b[32m104053.28906\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 048 | loss: 104053.28906 - acc: 0.2658 -- iter: 416/497\n",
            "Training Step: 1531  | total loss: \u001b[1m\u001b[32m101683.50781\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 048 | loss: 101683.50781 - acc: 0.2642 -- iter: 432/497\n",
            "Training Step: 1532  | total loss: \u001b[1m\u001b[32m100795.16406\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 048 | loss: 100795.16406 - acc: 0.2628 -- iter: 448/497\n",
            "Training Step: 1533  | total loss: \u001b[1m\u001b[32m101887.70312\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 048 | loss: 101887.70312 - acc: 0.2615 -- iter: 464/497\n",
            "Training Step: 1534  | total loss: \u001b[1m\u001b[32m105582.54688\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 048 | loss: 105582.54688 - acc: 0.2603 -- iter: 480/497\n",
            "Training Step: 1535  | total loss: \u001b[1m\u001b[32m102763.64062\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 048 | loss: 102763.64062 - acc: 0.2531 -- iter: 496/497\n",
            "Training Step: 1536  | total loss: \u001b[1m\u001b[32m100655.96875\u001b[0m\u001b[0m | time: 1.132s\n",
            "| Adam | epoch: 048 | loss: 100655.96875 - acc: 0.2340 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1537  | total loss: \u001b[1m\u001b[32m100406.64062\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 049 | loss: 100406.64062 - acc: 0.2481 -- iter: 016/497\n",
            "Training Step: 1538  | total loss: \u001b[1m\u001b[32m99658.03125\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 049 | loss: 99658.03125 - acc: 0.2295 -- iter: 032/497\n",
            "Training Step: 1539  | total loss: \u001b[1m\u001b[32m100424.73438\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 049 | loss: 100424.73438 - acc: 0.2253 -- iter: 048/497\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m101362.14844\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 049 | loss: 101362.14844 - acc: 0.2216 -- iter: 064/497\n",
            "Training Step: 1541  | total loss: \u001b[1m\u001b[32m100208.70312\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 049 | loss: 100208.70312 - acc: 0.2244 -- iter: 080/497\n",
            "Training Step: 1542  | total loss: \u001b[1m\u001b[32m98848.73438\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 049 | loss: 98848.73438 - acc: 0.2395 -- iter: 096/497\n",
            "Training Step: 1543  | total loss: \u001b[1m\u001b[32m99544.53906\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 049 | loss: 99544.53906 - acc: 0.2343 -- iter: 112/497\n",
            "Training Step: 1544  | total loss: \u001b[1m\u001b[32m100083.60156\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 049 | loss: 100083.60156 - acc: 0.2233 -- iter: 128/497\n",
            "Training Step: 1545  | total loss: \u001b[1m\u001b[32m100583.88281\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 049 | loss: 100583.88281 - acc: 0.2260 -- iter: 144/497\n",
            "Training Step: 1546  | total loss: \u001b[1m\u001b[32m98457.64062\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 049 | loss: 98457.64062 - acc: 0.2222 -- iter: 160/497\n",
            "Training Step: 1547  | total loss: \u001b[1m\u001b[32m99251.17188\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 049 | loss: 99251.17188 - acc: 0.2374 -- iter: 176/497\n",
            "Training Step: 1548  | total loss: \u001b[1m\u001b[32m102237.28125\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 049 | loss: 102237.28125 - acc: 0.2512 -- iter: 192/497\n",
            "Training Step: 1549  | total loss: \u001b[1m\u001b[32m102150.25000\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 049 | loss: 102150.25000 - acc: 0.2511 -- iter: 208/497\n",
            "Training Step: 1550  | total loss: \u001b[1m\u001b[32m103526.59375\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 049 | loss: 103526.59375 - acc: 0.2447 -- iter: 224/497\n",
            "Training Step: 1551  | total loss: \u001b[1m\u001b[32m111742.44531\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 049 | loss: 111742.44531 - acc: 0.2202 -- iter: 240/497\n",
            "Training Step: 1552  | total loss: \u001b[1m\u001b[32m119136.71094\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 049 | loss: 119136.71094 - acc: 0.1982 -- iter: 256/497\n",
            "Training Step: 1553  | total loss: \u001b[1m\u001b[32m115289.03906\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 049 | loss: 115289.03906 - acc: 0.1971 -- iter: 272/497\n",
            "Training Step: 1554  | total loss: \u001b[1m\u001b[32m114565.35938\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 049 | loss: 114565.35938 - acc: 0.2024 -- iter: 288/497\n",
            "Training Step: 1555  | total loss: \u001b[1m\u001b[32m114599.70312\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 049 | loss: 114599.70312 - acc: 0.2072 -- iter: 304/497\n",
            "Training Step: 1556  | total loss: \u001b[1m\u001b[32m110843.86719\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 049 | loss: 110843.86719 - acc: 0.2115 -- iter: 320/497\n",
            "Training Step: 1557  | total loss: \u001b[1m\u001b[32m111214.67188\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 049 | loss: 111214.67188 - acc: 0.2091 -- iter: 336/497\n",
            "Training Step: 1558  | total loss: \u001b[1m\u001b[32m108245.96875\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 049 | loss: 108245.96875 - acc: 0.2319 -- iter: 352/497\n",
            "Training Step: 1559  | total loss: \u001b[1m\u001b[32m106775.35156\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 049 | loss: 106775.35156 - acc: 0.2462 -- iter: 368/497\n",
            "Training Step: 1560  | total loss: \u001b[1m\u001b[32m109105.35156\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 049 | loss: 109105.35156 - acc: 0.2404 -- iter: 384/497\n",
            "Training Step: 1561  | total loss: \u001b[1m\u001b[32m107004.78125\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 049 | loss: 107004.78125 - acc: 0.2476 -- iter: 400/497\n",
            "Training Step: 1562  | total loss: \u001b[1m\u001b[32m104853.10938\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 049 | loss: 104853.10938 - acc: 0.2478 -- iter: 416/497\n",
            "Training Step: 1563  | total loss: \u001b[1m\u001b[32m106195.61719\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 049 | loss: 106195.61719 - acc: 0.2293 -- iter: 432/497\n",
            "Training Step: 1564  | total loss: \u001b[1m\u001b[32m104736.34375\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 049 | loss: 104736.34375 - acc: 0.2314 -- iter: 448/497\n",
            "Training Step: 1565  | total loss: \u001b[1m\u001b[32m106340.38281\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 049 | loss: 106340.38281 - acc: 0.2207 -- iter: 464/497\n",
            "Training Step: 1566  | total loss: \u001b[1m\u001b[32m107310.53906\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 049 | loss: 107310.53906 - acc: 0.2424 -- iter: 480/497\n",
            "Training Step: 1567  | total loss: \u001b[1m\u001b[32m105101.58594\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 049 | loss: 105101.58594 - acc: 0.2369 -- iter: 496/497\n",
            "Training Step: 1568  | total loss: \u001b[1m\u001b[32m107531.57812\u001b[0m\u001b[0m | time: 1.131s\n",
            "| Adam | epoch: 049 | loss: 107531.57812 - acc: 0.2382 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n",
            "Training Step: 1569  | total loss: \u001b[1m\u001b[32m107802.35156\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 050 | loss: 107802.35156 - acc: 0.2644 -- iter: 016/497\n",
            "Training Step: 1570  | total loss: \u001b[1m\u001b[32m105553.78125\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 050 | loss: 105553.78125 - acc: 0.2567 -- iter: 032/497\n",
            "Training Step: 1571  | total loss: \u001b[1m\u001b[32m104098.47656\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 050 | loss: 104098.47656 - acc: 0.2560 -- iter: 048/497\n",
            "Training Step: 1572  | total loss: \u001b[1m\u001b[32m103511.80469\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 050 | loss: 103511.80469 - acc: 0.2617 -- iter: 064/497\n",
            "Training Step: 1573  | total loss: \u001b[1m\u001b[32m105110.49219\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 050 | loss: 105110.49219 - acc: 0.2730 -- iter: 080/497\n",
            "Training Step: 1574  | total loss: \u001b[1m\u001b[32m104416.17188\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 050 | loss: 104416.17188 - acc: 0.2770 -- iter: 096/497\n",
            "Training Step: 1575  | total loss: \u001b[1m\u001b[32m107198.97656\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 050 | loss: 107198.97656 - acc: 0.2680 -- iter: 112/497\n",
            "Training Step: 1576  | total loss: \u001b[1m\u001b[32m105337.19531\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 050 | loss: 105337.19531 - acc: 0.2662 -- iter: 128/497\n",
            "Training Step: 1577  | total loss: \u001b[1m\u001b[32m105471.12500\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 050 | loss: 105471.12500 - acc: 0.2458 -- iter: 144/497\n",
            "Training Step: 1578  | total loss: \u001b[1m\u001b[32m105979.46094\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 050 | loss: 105979.46094 - acc: 0.2400 -- iter: 160/497\n",
            "Training Step: 1579  | total loss: \u001b[1m\u001b[32m106573.30469\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 050 | loss: 106573.30469 - acc: 0.2473 -- iter: 176/497\n",
            "Training Step: 1580  | total loss: \u001b[1m\u001b[32m104824.08594\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 050 | loss: 104824.08594 - acc: 0.2288 -- iter: 192/497\n",
            "Training Step: 1581  | total loss: \u001b[1m\u001b[32m103459.89844\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 050 | loss: 103459.89844 - acc: 0.2309 -- iter: 208/497\n",
            "Training Step: 1582  | total loss: \u001b[1m\u001b[32m106016.67188\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 050 | loss: 106016.67188 - acc: 0.2203 -- iter: 224/497\n",
            "Training Step: 1583  | total loss: \u001b[1m\u001b[32m107458.14062\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 050 | loss: 107458.14062 - acc: 0.2170 -- iter: 240/497\n",
            "Training Step: 1584  | total loss: \u001b[1m\u001b[32m97987.26562\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 050 | loss: 97987.26562 - acc: 0.2953 -- iter: 256/497\n",
            "Training Step: 1585  | total loss: \u001b[1m\u001b[32m89463.47656\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 050 | loss: 89463.47656 - acc: 0.3658 -- iter: 272/497\n",
            "Training Step: 1586  | total loss: \u001b[1m\u001b[32m87555.12500\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 050 | loss: 87555.12500 - acc: 0.3542 -- iter: 288/497\n",
            "Training Step: 1587  | total loss: \u001b[1m\u001b[32m90019.70312\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 050 | loss: 90019.70312 - acc: 0.3500 -- iter: 304/497\n",
            "Training Step: 1588  | total loss: \u001b[1m\u001b[32m89599.92969\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 050 | loss: 89599.92969 - acc: 0.3338 -- iter: 320/497\n",
            "Training Step: 1589  | total loss: \u001b[1m\u001b[32m92912.00000\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 050 | loss: 92912.00000 - acc: 0.3192 -- iter: 336/497\n",
            "Training Step: 1590  | total loss: \u001b[1m\u001b[32m91317.62500\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 050 | loss: 91317.62500 - acc: 0.2997 -- iter: 352/497\n",
            "Training Step: 1591  | total loss: \u001b[1m\u001b[32m92515.04688\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 050 | loss: 92515.04688 - acc: 0.2760 -- iter: 368/497\n",
            "Training Step: 1592  | total loss: \u001b[1m\u001b[32m91726.17969\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 050 | loss: 91726.17969 - acc: 0.2984 -- iter: 384/497\n",
            "Training Step: 1593  | total loss: \u001b[1m\u001b[32m90487.07812\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 050 | loss: 90487.07812 - acc: 0.2998 -- iter: 400/497\n",
            "Training Step: 1594  | total loss: \u001b[1m\u001b[32m89083.72656\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 050 | loss: 89083.72656 - acc: 0.3073 -- iter: 416/497\n",
            "Training Step: 1595  | total loss: \u001b[1m\u001b[32m91805.49219\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 050 | loss: 91805.49219 - acc: 0.3016 -- iter: 432/497\n",
            "Training Step: 1596  | total loss: \u001b[1m\u001b[32m94086.35938\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 050 | loss: 94086.35938 - acc: 0.2964 -- iter: 448/497\n",
            "Training Step: 1597  | total loss: \u001b[1m\u001b[32m94585.80469\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 050 | loss: 94585.80469 - acc: 0.2856 -- iter: 464/497\n",
            "Training Step: 1598  | total loss: \u001b[1m\u001b[32m91144.87500\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 050 | loss: 91144.87500 - acc: 0.2757 -- iter: 480/497\n",
            "Training Step: 1599  | total loss: \u001b[1m\u001b[32m96106.13281\u001b[0m\u001b[0m | time: 0.127s\n",
            "| Adam | epoch: 050 | loss: 96106.13281 - acc: 0.2732 -- iter: 496/497\n",
            "Training Step: 1600  | total loss: \u001b[1m\u001b[32m95855.65625\u001b[0m\u001b[0m | time: 1.136s\n",
            "| Adam | epoch: 050 | loss: 95855.65625 - acc: 0.2521 | val_loss: 112203.12500 - val_acc: 0.4000 -- iter: 497/497\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TELPsQHEn9IH"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpiCKPtrmpDH",
        "outputId": "c4a67092-980f-4c8a-eb0c-ab88ef71efaf"
      },
      "source": [
        "!python real_test.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "\n",
            "Enter WSpeed: \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "2021-10-13 13:15:02.190166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.199950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.200944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.202401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.203368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.204314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.855040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.856014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.856972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:02.857902: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-10-13 13:15:02.857979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-10-13 13:15:03.175052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.176080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.176957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.177993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.178860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.179765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-10-13 13:15:03.209028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.209915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.210746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.211816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.212951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.213945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "2021-10-13 13:15:03.279027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.280228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.281235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.282151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.283062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-13 13:15:03.283850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15435 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
            "\n",
            "The prediction power is [[ 61.35804     6.738439  -51.245502  -33.530956 ]\n",
            " [ 62.85593     6.9056554 -52.499332  -34.351894 ]\n",
            " [ 56.36458     6.189026  -47.069267  -30.797846 ]\n",
            " [ 53.83426     5.909943  -44.952316  -29.41253  ]\n",
            " [ 64.219444    7.0548463 -53.639416  -35.097775 ]\n",
            " [ 54.46541     5.9802337 -45.480694  -29.758438 ]\n",
            " [ 57.79769     6.3463464 -48.26737   -31.582077 ]\n",
            " [ 61.44551     6.7489457 -51.318752  -33.579113 ]\n",
            " [ 64.91803     7.1310306 -54.223244  -35.47983  ]\n",
            " [ 65.656494    7.2122884 -54.8409    -35.88401  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdF9o9Qet2PC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}